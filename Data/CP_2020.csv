,Title,email,KeyWords,Abstract,domain,Institution,Country,Latitude,Longitude
0,BLENDED PROJECT MANAGEMENT FOR CONSTRUCTION,@best-solutions.org,"Construction, Payouts, Strategic Planning Process, Strategic Management. ","The Construction industry is divided into two distinct functional groups: an administrative segment and a vocational  segment. The administrative segment  needs to be very structured to avoid scope creep, to complete the project on  time, and within budget constraints. The vocational segment needs to be flexible enough to adapt and overcome any  obstacle. Their focus is on creating a quality product, value for the customer, and meeting project constraints such as  building codes, zoning regulations, inspections, and customer expectations. Their triangle is value, quality, and  constraints.  This research offers a methodology to resolve the division between the management styles of the traditional  construction project manager and the iterative and incremental management style used by the site superintendent. This  methodology is important because the lack of management continuity between the administrative segment and the  vocational segment is problematic and widespread in the construction industry; it increases construction cost and  creates logistical issues in several areas, such as scheduling, estimating of both materials and man-hours, and  accurately tracking the status of the project.  The research objectives defined the pertinent components of the two current management styles, to present a  methodology to blend the existing styles while fostering understanding of the required functions to bridge the gap  between the administrative and vocational segments and finally to examine potential outcomes resulting from the  implementation of blended project management such as the ability to accomplish payouts for percent of project  completion with accuracy.  Keywords  Construction, Payouts, Strategic Planning Process, Strategic Management.  Introduction  The purpose of this project is to offer a methodology to resolve the division between the management styles of the  traditional construction project manager and the iterative and incremental management style used by the site  superintendent. This methodology is important because the lack of management continuity between the administrative  segment and the vocational segment is problematic and widespread in the construction industry; it increases  construction cost, and creates logistical issues in several areas, such as scheduling, estimating of both materials and  man-hours, and accurately tracking the status of the project.",best-solutions.org,,,46.3144754,11.0480288
1,COST-EFFICIENCY OF NITROGEN REMOVAL IN STORMWATERDEVICES,@astate.edu,"Infrastructure Sustainability, Infrastructure Risk Management, Stormwater Management ","Water quality is of growing concern to many jurisdictional authorities. One area of concern is with development.  Increased development contributes to excess nutrients flowing to local waterways.  These excess nutrients can be  harmful to animals and humans.  Therefore, it becomes the responsibility of the delegations to ensure harmful nutrients  such as nitrogen are not entering the waterways.  This research analyzed residential developments to determine cost - effective methods for meeting nitrogen requirements.  By determining cost -efficient methods, development can still  occur while meeting jurisdictional regulations for water quality.  These methods help maintain the quality of water  while at the same time improving the quality of life.   Keywords  Infrastructure Sustainability, Infrastructure Risk Management, Stormwater Management  Introduction  The mission of the City of Raleigh Stormwater Management Division is, “To preserve and protect life, support healthy  natural resources, and complement sustainable growth within the Raleigh community.” (Raleigh Stormwater  Management, 2019) One way of achieving this is through regulating development.    When new developments are being constructed, they are typically built on pervious surfaces.  Pervious  surfaces include surfaces such as grass, shrubs, trees, and wooded areas.  These are natural surfaces that allow  rainwater to filtrate through the ground.  Filtration allows two things to happen.  The first, is a reduction of excess  runoff.  The second function of pervious surfaces is reducing the amount of nutrients that enter the waterways.  Excess  nutrients such as nitrogen can lead to the deaths of aquatic life. (What is Nutrient Pollution, 2018)  The City of Raleigh lies entirely within the Neuse River basin.  The Neuse River is subject to nutrient  regulations as regulated by the North Carolina Department of Environmental Quality.  One nutrie nt that is regulated  is nitrogen.  Therefore, the City of Raleigh is required to regulate nitrogen levels within its city limits to the State of  North Carolina standards.   Development within Raleigh is growing at a fast rate. Is the 5th fastest growing city in the southeast United  States. (America’s Fastest Growing Cities, 2019) As the city continues to grow and expand, people need a place to  live.  This growth creates a demand for new residential developments.  However, these developments are typically  built on pervious surfaces that contribute excess nitrogen loadings draining into the Neuse River.    Stormwater devices are used to reduce nitrogen impacts into the waterways.   Devices include but are not  limited to, wet ponds, dry ponds, and sand filters.  Developers will hire consulting engineers whose job it is to design",astate.edu,Arkansas State University,United States,35.8447472,-90.67262556246513
2,ANALYSIS OF U.S GOVERNMENT CONSTRUCTION PROJECTS INAFGHANISTAN USING VALUE ENGINEERING PRINCIPLES,@gmail.com,"Value, Engineering, Best Value, Function, Cost, Quantitative, Qualitative, Design ","Value Engineering is a target and requirement for US construction projects abroad. According to the 2015 United  States Government Accountability Office (GAO), “The U.S. Office of Management and Budget (OMB) require value  engineering for all construction projects”. (Spero, 2016, p.17) Value engineering is an important aspect for projects in  Afghanistan. The lack of value engineering on projects in Afghanistan causes delays and raises cost on these projects.  The construction projects in Kabul, Afghanistan were lacking value engineering. The value engineering management  effort is governed by the Office of Overseas Building Operations (OBO). It is their duty to ensure value engineering  is implemented during all stages of the construction: planning, design, and post construction. Cost savings can be  ensured only if more focus is given to value engineering during the planning, design, and post construction phases .  This paper discussed how value engineering is lacked on US government construction projects in Afghanistan. The  paper underlined that the US government should focus on the steps to value engineering and its proper implementation.  After white papers, interviews, and official US government reports were analyzed to thoroughly assess the value  engineering practices on the construction projects in Afghanistan, quantified findings were provided along with the  recommendations to mitigate the lack of value engineering practices.  Keywords  Value, Engineering, Best Value, Function, Cost, Quantitative, Qualitative, Design  Introduction  An abundance of shortages in materials and finished products existed during the Second World War. Lawrence Miles,  a former General Electric Company recognized that manufacturing was running at maximum capacity and needed  innovative ideas to further expand production and improve the value of goods. So, Miles pioneered the value  engineering (VE) methodology (Miles, 1973). Since its inception, VE made an impact not only to the United States  Government (USG) but also across the world as a team-based approach and cost reduction technique. VE is a complex  technique that is directed toward analyzing functions to determine the best value. “Best value is synonymous with an  item or process that consistently performs the required basic function and has the lowest life cycle cost” (Parakhiya,  2017).  VE is the cornerstone of the fast-paced construction field. More importantly, VE within the US Government  construction projects is vit especially in terms of sustainability and spreading diplomacy abroad. Although VE is a",gmail.com,,,46.3144754,11.0480288
3,INCREASING PROFIT AND TIME VALUE FOR HOME BASED BAKERYUSING PROCESS IMPROVEMENT AND QUALITY TOOLS,@mocs.utc.edu,"Process Improvement, Inventory Management, Supply Chain, DMAIC, Six-Sigma, Lean ","TG Incredible Cakes is a home- based bakery that suffers from low net profit, especially when evaluated against the  amount of effort.  This study confirms that the owner’s hourly wage is low.  The competitive analysis revealed that  this bakery  is a high priced, but high -quality baker that offers excellent customization options.  This problem is  approached with the DMIAC method and several other quality tools.  Underlying process problems were evaluated  and solved with a combination of basic engineering knowledge and design of experimentation.  The final  recommendations from this study consisted primarily of suggestions that would decrease effort and/or costs.  Although  it was identified that a cost increase was recommended for short notice orders, which typically carry higher costs, due  to unplanned purchases.  It is recommended two process changes and one tax deduction change that, when combined,  should yield more than 12% improvement in net profit.  Other actions are presented to improve time management and  planning, which should also result in an increase net profit/hour.  Keywords  Process Improvement, Inventory Management, Supply Chain, DMAIC, Six-Sigma, Lean  Introduction  The company of focus for this project is TG Incredible Cakes  (TG hereafter), located in Bristol , Tennessee. This  business is a home -based bakery that makes and deliv ers custom cakes for all occasions: birthdays, weddings,  anniversaries, graduations, etc.  The owner is the only employee of the company, so the owner actively performs all  functions of the business including quoting, baking, decorating, delivery, cleanup, etc. The business is a sole  proprietorship and has been licensed with the state of Tennessee since 2014.  TG primarily serves customers that are  in Tennessee and collects sales tax on those sales, but there is  typically a small percentage of sales deliver ed to  customers in Virginia and there have been a very small number of sales into North Carolina.  TG does not collect sales  tax for sales that are outside of Tennessee.  One recurring problem noted by the owner was found that needed to be addressed  was the fondant icing  cracking. “This happens when your fondant begins to dry, either because you've overworked it or haven't protected it  from the elements” (""Fondant Fails""). Fondant is a type of icing that is in a sheet form and typically used to cover  cakes and make decorations.  When done properly, fondant can provide a very smooth and elegant look to the cake",mocs.utc.edu,,,46.3144754,11.0480288
4,IMPROVING THE FABRICATION PROCESS AT TOBYHA NNA ARMYDEPOT,@westpoint.edu,"Lean Six Sigma, Process Improvement ","Tobyhanna Army Depot (TYAD) is a logistics center for the Department of Defense,  specializing in electronic  systems, and employs 3,388 people as a primary resourcing center for the military. The Tobyhanna Army Depot’s  Equipage Branch is responsibility for maintenance and fabrication on contracted work orders. TYAD partnered with  a team from the US Military Academy to apply the Lean Six Sigma (LSS) DMAIC process on fabrication backlog.  LSS draws upon the literature of both six-sigma, focusing on defects and quality, as well as the lean literature, which  focuses on eliminating waste to improve organizational processes. This paper describes the unique aspects of this  project and the application of LSS tools and methods to improve the performance of the process. The delay in services  to different work centers impacts T YAD's ability to compl ete contracted work within the agreed upon time frames.  From June to August 2019, the average existing backlog per week within the Equipage Branch was 922 hours resulting  in excess costs of overtime as well as increased lead times causing late deliveries across the depot. The branch is  responsible for several jobs at different locations within TYAD from fabrication to equipment management. This  creates unique challenges, compared to static assembly-line process, as workers perform operations at different areas  of the facility then return to a starting point.  This paper summarizes the team's application of LSS to analyze ways to  make the process more efficient, improve the process, and ensure the process remains in control and stable over time.  Keywords  Lean Six Sigma, Process Improvement  Introduction  Tobyhanna Army Depot (TYADis a logistics center for the United States Department of Defense, specializing in  primarily electronic systems. It employs 3,388 people and is a primary resourcing center for the United States Military.  The mission statement of TYAD is “provide full life cycle support for all C5ISR weapon systems” (Tobyhanna Army  Depot, 2017). C5ISR stands for Command, Control, Communications, Computers, Combat Systems , Intelligence,  Surveillance, and Reconnaissance. The TYAD’s Equipage Branch’s Fabric Cost Center (FABAPP01)  is responsible  for maintenance on contracted work orders .. Tobyhanna Army Depot partnered with an L ean Six Sigma team from  the United States Military Academy to apply the D efine, Measure, Analyze, Implement, Control (D MAIC) process  on the backlog within the FABAPP01. Because there is a significant backlog in work hours, an average of 922 hours  per week, many leaders within the Equipage Branch have allocated their time to solving this problem.   The process within FABAPP01 begins with Tobyhanna’s Process Controller reviewing and updating the Logistics  Management Program (LMP). Additionally, the Process Controller formulates the work order schedule and task list  for the upcoming week’s work. After the Process Controller compiles the list he passes the information to the Equipage  Integration Support Division Chief (ISDC). The ISDC then reviews the work orders and passes the information to the  FABAPP01 Brach Chief or returns the work orders to the production controller due to errors in the information. If the  work is returned, the Branch Engineers review the work orders and identify discrepancies. As well, the Branch  Engineers recommend necessary changes to the Process Controller. The Process Controller then prints the new work",westpoint.edu,,,46.3144754,11.0480288
5,MACHINE LEARNING APPROACH TO COUNTERACT THEBULLWHIP EFFECT IN SUPPLY CHAIN,@northeastern.edu,"Bullwhip Effect, Supply Chain, Machine Learning, Customer Categorization, PCA, k-Means Clustering, ","The Bullwhip effect is a phenomenon that tends to increase the volatility in the demand distribution when moving  upstream in the supply chain. Because of this, most of the industries are failing to accurately forecast the demand.  This leads to increased manufacturing cost, higher inventory level, longer  replenishment lead times, low product  availability, higher transportation cost and overall, it reduces the supply chain profitability. Previous studies have  proposed different methods to overcome the Bullwhip effect such as  improving demand forecasting, reducing order  batching, reducing the incentives of forward buying, better sharing of information, shorten lead and review period  times and designing single -stage replenishment control. This study focuses on the customer-centric approach of  categorizing the customers by incorporating Machine Learning techniques for better understanding the customers ’  purchasing behavior. Specifically, this study analyzes the hidden patterns and similarities that exist between different  customers and the products that they purchase. For the analysis,  Principal Component Analysis ( PCA) is used for  dimensionality reduction and k -Means Clustering, an unsupervised learning technique  is used for customer  categorization. By this, the end customer demands are tracked downstream from upstream in the supply chain, which  may help to reduce the demand variation.  Keywords  Bullwhip Effect, Supply Chain, Machine Learning, Customer Categorization, PCA, k-Means Clustering,  Unsupervised Learning.  Introduction  Forecasting the demand is a big challenge for most of the Supply chain industries (Wieland & Wallenburg, 2011) .  The traditional demand forecasting methods include Cumulative forecasting, Naïve forecasting and Exponential  smoothing technique (Holt, 2004). These forecasting techniques may also include the level, seasonality and trend  in  the demand distribution for better prediction of the results (Gamberini, Lolli, Rimini, & Sgarbossa, 2010). In previous  studies, Regression technique has also been widely used for demand forecasting (Brazdil, Carrier, Soares, & Vilalta,  2008; Freedman, 2009; Levis & Papageorgiou, 2005). However, volatility in demand distribution makes it harder to  predict the demand pattern. The volatility starts to increase when moving upstream in the supply chain i.e. from the  end customers to the retailers, retailers to the distributers, distributers to the manufacturer and from the manufacturer  to the suppliers. This effect is called the Bullwhip effect in the supply chain. This concept was initially discussed by  Forrester (1997), thus it is also known as Forrester effect, which makes the demand pattern more volatile while moving  upstream in supply chain. Although there are many techniques proposed to overcome the Bullwhip effect, it is still a  big challenge to accurately predict the demand as the variability of the end customer demand starts to pile up when  moving upstream in the supply chain. One way to overcome this problem would be reducing the variability in the  demand distribution, which could ultimately produce a better forecasting result. In the supply chain, eliminating the  multiple forecasts, that only use the immediate partner order data in implementing the collaborative forecasting  (McCarthy & Golicic, 2002), will reduce the Bullwhip effect. A customer-centric approach of predicting, “what the  end customers need” and “when they need it” would avoid multiple forecasting and thus reduces the variability in the  demand distribution.",northeastern.edu,,,46.3144754,11.0480288
6,DYNAMICS OF MANAGING A SPECIALIZED TALENT POOL OFEMPLOYEES,@westpoint.edu,"Workforce management, system dynamics, labor shortage ","Organizational policy that governs promotion, retention, and hiring practices is critical to a firm’s human resource  management strategy. The policies implemented by firms will vary based on the available supply of qualified  employees. As organizations become increasingly reliant on technology and data, their need for specialized and skilled  employees also increases. Further, the market for technologically skilled workers continues to become more  competitive and in turn, hiring and retaining qualified, skilled, and experienced employees becomes increasingly  difficult. Organizations will likely have to alter their human resources management strategy to maintain their required  talent pool. Analyzing the hiring, retention, and promotion structures within an organization, this research investigates  the effects of limiting the population of low -density, skilled employees available for an organization to hire.  Representing a more competitive talent marketplace and talent pool, the goal of this research is to understand how  employee retention strategies, incentives, and internal promotion policy can be altered to achieve desired manning  levels within these highly skilled disciplines. Leveraging a system dynamics model, this study builds an interactive  simulation tool allowing policy makers to view the potential outcomes of their policy decisions. System dynamics  models the underlying system structure providing insight into variable interaction, causal relationships, and system  behavior. The outcomes of this simulation will allow policy makers to identify differences in policy outcomes, reveal  potential policy leverage points, and make more informed policy decisions.  Keywords  Workforce management, system dynamics, labor shortage  Introduction  According the U.S. Bureau of Labor Statistics (2019), jobs within the United States involving tech sector are projected  to grow by over 30 percent over the next 10  years. Coupled with an already present labor shortage in highly skilled  workers, The Wall Street Journal (2019) suggests the tech sector labor gap will soon crest 1 million jobs. Firms have  been required to implement new strategies to recruit, employ, and retain their skilled workforce. The gap between a  firm’s demand for highly skilled workers and workers in the available labor pool is further exasperated when additional  requirements are added by prospective employers. Increased education or experience levels, specific certifications or  skill requirements, citizenship requirements, security clearance eligibility, and several others  all seem to drive the  chances of landing a new hire down.    Jobs involving big data, data science, and data analysis have grown over 14 percent in the past 4 years. These  career fields are expected to sustain a 31 percent growth rate for the next 8 years and have the potential to grow at an  even faster rate. Similar statements can be made about information technology (IT), cyber security professionals, and  software developers (estimated 32 percent growth rate) (U.S. Bureau of Labor Statistics, 2019). Large companies and  the tech sector rely on high starting salaries, large stock options, flexible work schedules, and attractive work- life  balance to recruit and retain top talent. Further, companies like Amazon and AT&T have pledged they are committing  to spend upwards of $1 Billion Dollars to invest in their existing workforce. Facebook and Google have committed to  allowing much of its workforce to work more regularly from home  (Kerr & Ng, 2020) . The market for qualified  employees either is gro wing ever more competitive both a the entry -level and in more experienced positions. How  then, are firms without the means or resources to compete with the major players in the tech industry to recruit, hire,  and retain their skilled workforce? Further, how do organizations within the U. S. government recruit, hire, and retain",westpoint.edu,,,46.3144754,11.0480288
7,ENTREPRENEURIAL SKILLS FOR ENGINEERS – INSIGHTS FROM THEDEVELOPMENT OF AN ONLINE COURSE,@lsbu.ac.uk,"Entrepreneurial and innovation related skills, engineering education, online course. ","Engineers need to have a solid technical foundation for their education and practice but there is also a requirement  to  have a wider set of professional and enterprise related knowledge and skills in order to be successful in the workplace.  This includes understanding how to develop innovative ideas through to new products and businesses as part of the  entrepreneurial journey – including both new venture creation as well as developing a new business area within an existing  organization (i.e. intrapreneurial). Traditionally, engineering education has not included the provision of teaching in this  area and consequently a new online course has been developed called ‘Entrepreneurial Skills for Engineers’. The course  provides a comprehensive and integrated view of the different aspects to support an engineer to become more  innovative and entrepreneurial. This paper provides insights from the design and delivery of the online program, including  discussion of how the technical material was assembled as well as the learning outcomes for the course and wider  pedagogical aspects. A review of the requirements for enterprise education is provided through analysis of literature in the  area. The paper also provides guidance to engineering faculty and others who are seeking to develop enterprise related  programs for engineers.  Keywords  Entrepreneurial and innovation related skills, engineering education, online course.  Introduction  Engineers should have a solid appreciation and supporting knowledge base in the engineering fundamentals and this  includes, for instance, engineering mathematics, design, materials and material failure, thermodynamics and cycles, fluid  mechanics and engineering processes. However, many studies have highlighted that there is a further need for engineers  to have a wider set of professional related skills (Litchfield et al., 2016; Mitchell et al., 2019), including communication,  team working, problem-solving, project management as well as innovation and entrepreneurship. This is because a  professional engineer is unlikely to spend 100% of their time on purely technical engineering tasks.  In the case where an  engineer moves into another functional area, such as commercial management, or sales & marketing, they will of course  need to acquire specialist required skills and knowledge to operate successfully in these fields.  However, even when they  remain in technical engineering roles, they will likely work on projects and in engineering teams; consequently, there is a  need for problem solving, team working and other professional skills (Philbin et al., 2019).   Engineers involved in developing new projects, products and engineering areas need to understand how to be  innovative (MacLeod, 2010) – how to take an idea forward as part of new product development and how to plan for  implementation of a new manufacturing process, or deployment of the new technology within an industrial setting. This  set of skills and knowledge required by engineers can be positioned in regard to being enterprising. In this context,  enterprise refers to innovation as well as entrepreneurship (and intrapreneurship). This is because being enterprising  is not limited to new venture creation but also includes the ability to understand how to develop an idea into a new  product or a new business area within an existing company. It a lso refers to understanding how to commercialize   research and technology towards new product development as well as the management of research, technology and  engineering projects. Consequently, the question arises how can engineers build on their technica l engineering  foundation and numerical skills by becoming more enterprising? This question can be partly addressed  through  specialist education that has been designed and tailored to the needs of engineers and is focused on the provision of  innovation and entrepreneurial skills and knowledge for engineers as well as  other STEM ( science, technology,  engineering, and mathematics) specialists.  The structure of this article is as follows. After the introduction is the literature review on the requirements for enterprise  education. This is followed by the case study investigation, which includes the following sections: background on the case,",lsbu.ac.uk,,,46.3144754,11.0480288
8,ANALYZING THE CALIFORNIA FINISHED MOTOR GASOLINESUPPLY AND DISTRIBUTION,@calpoly.edu,"Petroleum Industry, Supply Chain Management, Motor Gasoline, California. ","Gasoline is necessary for the lives of many Americans in carrying out many of their daily tasks. In 2018, according to  the U.S. Energy Information Administration (EIA), Americans consumed 142.86 billion gallons or 3.4 billion barrels  of finished motor gasoline. For the most populous state in the U.S., California, the financial burden of purchasing  gasoline per gallon compared to the national average is mind-numbing. As of 2019, the average California price per  gallon of all grades of gasoline was $3.677, while the national average was $2.691. This research intends to get to the  root cause of the discrepancy between the gasoline prices in California and the national average through analyzing the  supply chain distribution of fuel in California. Since 2012, the United States has produced almost double the gas than  any other country in the world, yet the prices for California continue to rise. Discounting the fact that taxes influence  the price, the analysis will be adjusted based on a tax -free system to compare the prices of gasoline fully. By use of  Supply Chain Analysis, we will pinpoint the demand and where to optimally deliver the gasoline tankers to effectively  lower the price of gas in California to align more with the national average.  Keywords  Petroleum Industry, Supply Chain Management, Motor Gasoline, California.  Introduction  With the growing demand placed on the petroleum industry, the gasoline supply chain has developed into a complex  and challenging environment. The area of interest currently has everything to do with the supply chain, specifically in  the California gasoline and oil market. Over the years , gasoline prices have fluctuated significan tly. Ever since the  early days of 2012, when fracking was introduced into the United States, the country has become a net exporter of oil.  With all these new developments, one could be confused as to why the gas oline prices have remained statistically  higher in California when compared to the rest of the United States. California itself has the second -highest gasoline  tax rate in the United States. At the time of this research,  the per-gallon California average gasoline price is $4.20.  The average US-wide gas price is $2.65. It would make sense if the highest gas pressed price state of Pennsylvania  had the highest gas price. But even though Pennsylvania has the highest tax rate of gasoline, California is still more  expensive by roughly $1.20 per gallon. Additionally, seeing the average gasoline price in Hawaii be more than $0.10  less than the average gas price in California. At the same time, Hawaii is a group of islands in the Pacific rather than  connected to the lower 48 continental United States is quite puzzling. For all these reasons, the authors decided to dive  deeper into the supply chain of gasoline, considering that now tax is not the main issue of what most people think is  increasing the gas oline prices. Specifically, we will investigate the supply chain network design, complications , as  well as environmental regulations, and then, will take us into our solution of how to fix this problem.  Supply Chain Network  For the purpose of this paper, the supply chain network for the petroleum industry can be broken into two separate  sections. The first section will deal with the supply chain network design and each component of the network. This  will provide an understanding of the different levels of production that are required to provide tangible finished  products to the end-users. The next portion of the supply chain network will address complications and limitations  that have been identified as inhibitors to the overall success of the supply chain. These complications are some of the",calpoly.edu,"California Polytechnic State University, San Luis Obispo",United States,35.3074623,-120.66464833678835
9,THE IMPACT OF AUTOMATION ON SHIPPING AND RECEIVINGOPERATIONS,@calpoly.edu,"Automation, Warehouse Operations, Supply Chain. ","Within the field of supply chain and logistics, automation is at the forefront of research and development. While many  warehouses and distribution centers have automated many of their operations, shipping and receiving operations,  specifically loading and unloading products, is an undeveloped aspect of the automated supply chain. Currentl y,  shipping and receiving functions are experiencing a high employee turnover rate due to harsh conditions and strenuous  movements for the body, which is leading to additional expenses for companies. Automation could open up the  opportunity for improvement of worker satisfaction and retention rates. Additionally, automating shipping and  receiving will improve efficiency and accuracy and reduce injuries on the job. Various companies have developed  robots to automate loading and unloading both individual parcels and pallets. With these new technologies arising and  being implemented in warehouses, it is imperative to standardize these new processes within a highly complex supply  chain by creating standard operating procedures and effectively communicating them to management and the  workforce.  Keywords  Automation, Warehouse Operations, Supply Chain.  Introduction  Logistics is one of the largest industries in the United States, with millions of shipments nationwide taking place each  year. However, the technology behind the industry has not caught up to the times with regard to one critical aspect:  the loading and unloading of trucks.  Background   In a warehouse or a distribution center (D .C.), shipping is the process where goods are packaged and prepared  to be  sent out to a new location, be it another warehouse, retail location, or an end consumer. Additionally, the same location  in the facility is where inbound goods are received to be routed into the facility’ s processes. Material handlers are  typically the designated employees to handle those two areas of operations. While automation has begun to take over  the supply chain industry, the shipping and receiving process es are  still largely manual. Material handlers must  load/unload trucks using forklifts, pallet jacks, or even by hand. This unloading/loading process can take  approximately 30 minutes per truckload, and that only includes the physical movement of the goods (Smylie, 2019).  If a truck must be loaded and unloaded, then it will take at a minimum an hour of just moving goods, which does not  consider signing and communication during the carrier and shipper transaction or the queue the truck driver has to  wait in potentially.   Other logistical duties of material handlers include recording the movement of parts, supplies, materials,  equipment, and stock to and from an establishment ( CCOHS, 2019). Besides, there may be a need to document  inspection of the incoming/outgoing goods, report damaged items, and compile this information into some sort of  report sent out to others in the company (QStock Inventory, 2016). This breakdown of the material handler duties, and  the larger shipping and receiving operation, shows how the process is one that may be difficult to automate fully. The",calpoly.edu,"California Polytechnic State University, San Luis Obispo",United States,35.3074623,-120.66464833678835
10,SUCCESS FACTORS FOR KAIZEN EVENTS IN HOSPITALS:SYSTEMATIC LITERATURE REVIEW,@vt.edu,"Kaizen Events, Hospitals, Continuous Improvement, Success Factors ","The complexity of hospital operations and the ongoing need to ensure safe and high-quality patient care continues to  present challenges for hospital managers and administrators. Kaizen events (KEs) offer opportunities to address some  of the ongoing challenges in managing performance within a complex hospital environment. Kaizen events - a type  of improvement project - have been utilized in healthcare as a mechanism to apply improvement tools to reduce waste,  reduce non-value-added activity, and improve quality of patient care . A Kaizen event is a focused and structured  improvement project utilizing a dedicated cross -functional team to address a targeted process or work area with  specific improvement objectives in an accelerated timeframe. A critical issue for hospital is understanding the key  factors that support and ensure the effective use of KEs in hospital settings to achieve the outcomes and benefits that  impact both operations and the stakeholders of care. A preliminary assessment of a systematic literature review - a  rigorous research method that evaluates and synthesizes the literature - was used to answer the primary research  question: What are the main success factors of Kaizen events in hospital settings? Qualitative analysis was performed  to extract, synthesize, and characterize the most frequently-reported success factors for KEs. Additional analyses were  conducted to assess the strength of association between factors. This synthesis of knowledge is useful for practitioners  for increasing understanding and improving the effectiveness and success of KEs in hospitals.   Keywords  Kaizen Events, Hospitals, Continuous Improvement, Success Factors  Introduction  Healthcare operations can be adversely affected by a multitude of problems that can undermine patient care delivery.  The need to address ongoing challenges in a timely and efficient manner remains crucial. Hospital leaders must be  proactive and responsive to mitigate the burden of complexity and compartmentalization. Management  must  continuously pursue Healthcare Continuous Improvement (HCI) to minimize problems  or deficiencies that can  negatively impact operations and the quality of care that patients receive. Inspired by the application of Kaizen events  (KEs) in other sectors , such as manufacturing and se rvice industries, KEs have been more recently utilized within  healthcare to bring about meaningful changes (Holden & Hackbart, 2012).  Kaizen events have been successfully  applied to healthcare operations as a continuous improvement mechanism ( Sankoff et al., 2013; Cowen & Joseph,  2016). Due to the potential impact of this type of improvement initiative within hospitals, it remains advantageous to  be examined. A KE is defined as a “focused and structured improvement project utilizing a dedicated cross-functional  team to address a targeted process or work area with specific improvement objectives in an accelerated timeframe”   (Farris et al., 2009, p.1). KEs have great potential to achieve breakthrough improvements and can support hospitals’  continued efforts to improve processes and bring about critical changes that ensure efficient care processes and high- quality patient care (Skeldon et al., 2014; Biffl et al., 2011). Understanding what contributes to success for KEs in  hospitals will help leadership to implement key components  that accrue high benefit with minimal resistance and  waste in the KE effort. Hence, this paper seeks primarily to identify the main success factors for KEs in hospitals. The  following sections discuss additional background information about KEs, describe the SLR methodology used along  with justification for further analysis on KE success factors, present and discuss research findings, and summarize  insights from this work along with recommendations for KEs within hospitals and future work.",vt.edu,Virginia Tech,United States,37.22192675,-80.42728184013652
11,"4.0 VUCA (VOLATILITY, UNCERTAINTY, COMPLEXITY ANDAMBIGUITY) LEADERSHIP",@adelaide.edu.au,"Industry 4.0, VUCA, leadership, management, decision-making. ","Industry 4.0 implementation is growing fast due to the rapidly evolving nature of business which focuses on the  combination of automation and data exchange to establish new communication protocols between humans and  machines. In other words, all components of a system, including machines, workers, computers and customers, are  automatically interconnected to generate the ‘big data’ which is used for making decisions in the complex Industry  4.0 environment. Such new communication protocols will redefine the business game and will also bring elements of  volatility, uncertainty, complexity and ambiguity (VUCA) to a firm. This large -scale technological transformation  will require different approaches to management and leadership. As a result, there is an urgent need to address new  leadership skills which can enhance capabilities for complex decision-making. In this study we conduct a review of  literature from 2010 to 2020 focusing upon the main challenges for leaders in this transition and the critical factors  which leaders must address in order to enhance their decision-making quality. Major decision-making factors arising  from literature are data overload, management style, risk acceptance, power, leadership qualities and competencies,  availability of resources and organisational culture.  Keywords  Industry 4.0, VUCA, leadership, management, decision-making.  Introduction  Today’s business environment has been irreversibly impacted by huge and rapid advances in technology. Not only do  leaders have to cope with a vast amount of information and the growing involvement of stakeholders, the  interconnectedness of systems and people as evidenced by the widespread move towards Industry 4.0, with the  ongoing transformation of traditional practices through smart technology and the Internet of things, will require radical  shifts in how to approach business decision -making. When machines, workers, computers, customers and various  other stakeholders are all connected, the amount of data generated is greatly increased, as is the likelihood of  unforeseen consequences. When there is real-time access to enormous amounts of data, this offers great opportunities  to change the way businesses are run and positioned; but all this digital information from so many different sources  will be very difficult for leaders to integrate, in order to take advantage and increase competitiveness. Leaders are also  under pressure to make decisions rapidly because the environment is changing so fast. All of this has a profound effect  on leadership and how it should be carried out. Leadership practices need to be enhanced to juggle all the options and  create innovative environments which can move with the times.  Given all this pressure, leadership decision-making has never been more challenging. How decisions were made  in the past, the old way of doing things, will need a significant ramp up to be able to cope with the complex conditions",adelaide.edu.au,University of Adelaide,Australia,-34.9189226,138.60423667410745
12,M,Missing,,,Missing,,,46.3144754,11.0480288
13,IN-SITU INNOVATION DIFFUSION RATE FORECASTING,@swri.org,"Technological Innovation Diffusion Rate, Diffusion Rate Forecasting. ","The rate at which an innovation diffuses is an essential factor in determining, impacting, and dictating the speed at  which decisions are made within an organization, affecting an organization’s success. It is easy for an organization to  determine a technological innovation’s diffusion rate post -situ (e.g., in hindsight).  This, post-situ based, mode of  decision making often leads to reactive rather than proactive actions. Although, post - situ examinations are, and can  be, useful; it leaves a blind spot to how decision- makers manage resources (labor, equipment, materials, time, and  capital). A framework for gaining in-situ knowledge on a technological innovation’s diffusion rate would assist and  benefit an organization’s decision-makers in proactively setting strategy, policy, and resource management rather than  reactively. The primary objective of this research endeavor is to outline the components required to develop a  framework towards determining how accurately a technological innovation’s diffusion rate can be predicted with  partial diffusion data. As a prime component of this effort, a review of partial innovation diffusion models is presented.  Keywords  Technological Innovation Diffusion Rate, Diffusion Rate Forecasting.  Introduction  In a proactive organizational environment, an organization will strive to, and has a higher potential to, maximize  profits/growth and minimize losses through the management of its limited resources; for proactive organizations have  a higher potential to outpace those of reactive organizations (LaPorte & Consolini, 1991; Lin & Carley, 1993; Parvin  Jr & Beruvides, 2018; Pauchant, Mitroff, Weldon, & Ventolo, 1990) .  In the research literature, there exists a robust  body of knowledge on technological innovation adoption, but the arena of technological innovation abandonment is  significantly less developed and poses a common problem for practitioners (Greenwood, Agarwal, Agarwal, & Gopal,  2016; Parvin Jr & Beruvides, 2017).  That said, many organizations view technological innovation abandonment as a  reactive life cycle event (i.e., obsolescence), commonly waiting and allowing a new innovation or technology to  displace an older innovation or technology (Barreca, 2000).  Conversely, abandonment optimization of technological  innovation management (e.g., technological innovation abandonment optimization, when an inno vation is  purposefully abandoned (“phased-out”) to shift resources to new investment areas) is a proactive occurrence.   A key  component of technological innovation abandonment management is the forecasting of its diffusion rate , for it is an  essential factor in determining the economic impact of an organization’s decision speed (i.e., the speed at which an  organization makes resource decisions) (Parvin Jr & Beruvides, 2019).  An operational definition of the term technological innovation is warranted to start , for there is much  discourse and confusion in the research literature on what is a technological innovation.  For this effort, the process  of technological innovation will not be a focus, but rather the exploration of both new or improve product s and  processes.  Accordingly, for this endeavor and i n this connection, a technological innovation is defined as a new or  (minor or major) improved product, process, or device, that results in the societal distribution (commercialization) of  new or improved goods (products or services) or a new or improved (products or services) production pr ocess  (Diaconu, 2011; Kelly & Kranzberg, 1975; OECD, 2005).  As a result, commercialize products and processes such as  the Microwave Oven and Blast Ox ygen Furnace are examples of technological innovations. Per this operational  definition, the diffusion of technological innovation, can be tracked via market sales data, allowing the study of  product and process innovation diffusion from a generalized (macro) perspective.  Thus, the term technological",swri.org,,,46.3144754,11.0480288
14,HISTORICAL OVERVIEW OF THE RELATIONSHIP BETWEENRENEWABLE ENERGY INTENSITY AND FUEL PRICES: SOUTHKOREA AND THE U.S.,@ttu.edu,"Renewable energy intensity, fossil fuels, population base, LNG fuel price, energy markets, systems thinking ","This paper examines the historical relationship between renewable energy intensity in South Korea (Korea) and major  fuel prices with macroeconomic variables , and compares the results with the U.S. counterpart. Renewable energy  initiatives have been more active in the world energy market since the Arab oil embargo in the early 1970s. South  Korea, the 12th largest economy in  2018 and the 9th largest energy consumer in 2017 in the world, possesses little  domestic natural resources and had to meet 98% of its energy needs from international sources. As renewable energy  is a viable substitute for electricity generation, with biof uel for transportation energy use, renewable energy sources  have well served South Korea and the U.S. The benefits have been two- fold: reduced dependence on foreign energy  imports and the promotion of a cleaner environment. With renewable energy competing with fossil fuels for price  advantages in energy markets, this paper analyzes annual data, from 1990 to 2017, to examine and identify the  multifaceted relationship between fuel prices and renewable energy intensity, under the developing market dynamics  in these countries. The implications will help engineering managers better understand sustainability issues under the  dynamics of energy market systems.  Keywords  Renewable energy intensity, fossil fuels, population base, LNG fuel price, energy markets, systems thinking  Introduction  U.S. Department of Energy (DOE) defines energy intensity in two ways: energy efficiency in Gross Domestic Product  per Mega-Watt-hours (GDP/MWh) or Gross Domestic Product per Million British Thermal units (GDP/MMBtu) and  energy intensity in MWh/GDP  or MMBtu/GDP.  Energy efficiency refers to the activity or product that may be  produced given an amount of energy, while energy intensity  is simply inverse of energy efficiency, with its decline  viewed as a proxy for efficiency improvement (U.S. Department of Energy, n.d.). In a slightly different perspective,  U.S. Energy Information Administration also defines energy intensity two ways: MMBtu/GDP, the same as  energy  intensity of the DOE, and MMBtu/person, or per capita energy consumption in a nation (U.S. Energy Information  Administration (EIA), 2018).  Energy efficiency has played an important part in the U.S. energy strategy since the Arab Oil Embargo in 1973  (U.S. Department of Energy, n.d.). The Arab oil embargo exerted dramatic impact on U.S. energy markets, with a  sharp increase in electricity prices, and as a result, the U.S. designed and implemented policies to reduce dependence  on imported fuels and promote renewable source s. The efforts included the Public Utilities Regulatory Policies Act  (PURPA) of 1978 to mandate more use of renewable energy including wind and solar (Perkins, 1998).  The term “green energy intensity” appeared in a study which attempted to identify relationship between the  “green” economic development, or renewable energy growth and economic growth (Yang, 2014). Equivalent to the  “green energy intensity”, we use  a term “renewable energy intensity  (REI-Gen)”, a measure as electricity  consumption, produced from all renewable sources, divided by population in a nation. A conversion ratio of MWh to  MMBtu is 1 to 3.412142.  South Korea (Korea) was the 12th largest economy in the world in 2018 (Investopedia, 2020), and the 9th largest  energy consumer in the world in 2017, according to BP Statistical Review of World Energy 2018. The nation imported  98% of total fossil fuel consumption, with top five fuels being LNG, coal, crude oil, and refined products (U.S. Energy  Information Administration (EIA), 2018). Due to its heavy dependence on imported energy, Korea has been aggressive",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
15,"INTRODUCTION OF THE PROPER MODEL, AN OPTIMIZATIONMETHODOLOGY TO REDUCE COST AND LEAD-TIME FOR EXISTINGDESIGNS OF MACHINED GOODS",@outlook.com,"Proper Model, Manufacturing Optimization, Cost Reduction, Lead Time Reduction, Existing Design Improvement , ","Manufacturers in the USA are facing increasing pressure to offer products capable of competing with low-cost-country  vendors, forcing them to continuously lower prices and shorten delivery times. Simultaneously, shop employees are  discovering inefficiencies in released designs after they have already gone into production, leading to an inflated  manufacturing cost, affecting the manufacturers’ margins in every sale. This paper explores a viable solution to this  problem by introducing the PROPER (Product Redesign, Optimization of Processes, and Effectiveness Review)  Model, a new methodology created to simplify existing designs and le an-out the manufacturing processes. Unlike  existing optimization methodologies, such as Lean or Six-sigma, which focus heavily on improving the manufacturing  process only, the PROPER Model encompasses optimizations to any existing design, provides guidelines for multiple  optimization approach to the manufacturing process es, and increases employees’ ownership for quality verification  with the goal of reducing cost and production lead-times. A practical application of this new model was implemented  to Safety Relieve Valve components with the purpose of demonstrating that cost and lead -time reduction can be  accomplished using the steps outlined by this methodology. The full implementation process is summarized and  presented in this paper, including the outcome of cost reduction of up to 25% and an average lead -time decrease of  16%. The PROPER Model was initially developed to reduce cost and lead- time for Primary Pressure -Retaining  Components in Safety Relief Valves, but it could be applied in other existing products and industries as a mean to  identify and reduce waste on existing component designs.  Keywords  Proper Model, Manufacturing Optimization, Cost Reduction, Lead Time Reduction, Existing Design Improvement ,  Market Competitiveness, Machined Goods, Redesigning.  Introduction  Cost and lead -time (LT) are two of the main measurements  in manufacturing activities as a gauge for production  efficiency. Cost, often referred to as Cost of Goods Sold (COGS), is the direct expense attributable to the  manufacturing of goods sold in a company. The sum of COGS includes the materials consumed in creating the good  and the direct labor used to make the good. It excludes administration and other overhead expenses such as utilities,  sales, and distribution costs (Law, n.d.). LT is the amount of time it takes to execute a process from start to finish. In  manufacturing, LT is seen as the time length when an order is placed until it is shipped. LT is a critical measurement  for calculating production schedules, procurement of raw materials, and inventory safety-stock levels (Bradley, 2015).  It also plays a big role in determining whether an order-bid is won or lost to a competitor.  Outdated manufacturing processes, as well as lack of willingness to change existing  designs fueled by the  mentality of “don’t fix what is not broken,” have led to  the constantly increasing cost and lead- times of machined  goods in an ever more competitive market, thus forcing manufacturer to lower contribution margins or facing loss of  business.  Developing an optimization model with redesigning guidelines, process streamlining, and quality assurance  simplification for existing machined goods will generate enhanced components with improved manufacturability,  reducing non-value-added processes, such as over -machining and full -inspection steps; hence, making such goods  cheaper and faster to produce without a negative impact to quality, fit, form, or function.  This paper introduces the PROPER (Product Redesign, Optimization of Processes, and Effectiveness Review)  Model, a new 9 -step methodology created to simplify existin g product designs and lean -out the manufacturing  process. Typical lean/six -sigma methodologies focuses heavily on improving the manufacturing process only. In",outlook.com,,,46.3144754,11.0480288
16,B2B DIGITAL TRANSFORMATION: AN INDUSTRIALMANUFACTURER’S STRATEGY TO DRIVE DIGITAL ENGAGEMENTWITH CHANNEL PARTNERS,@tamu.edu,,"Customer service and customer experience in Business-to-Consumer (B2C) industries have been shaped by the next  generation of influential consumers, and now those expectations are being seen in Business-to-Business ( B2B)  markets as well (Solomon, 2016). These trends provide evidence of an emerging trend that industrial manufacturing  organizations and their distribution par tners must follow suit. The industry has been recognizing the need for  digitization in a distributor’s offerings for the past several years, and now the time has come for the manufacturers to  push for digitization as well. A global manufacturer of industrial products embarked on a digital transformation of  their business. The manufacturer’s success at becoming a fully digital company is challenged by the limited  engagement of distributor partners, who constitute approximately 70% of the manufacturer’s business in the U.S. This  research project focuses on understanding current digital engagement between the manufacturer and its distributors,  quantifying the value of digital engagement to both, and the developing recommendations improvements needed to  the manufacturer’s digital strategy to improve all parties’ digital footprints. The key findings of this project are focused  on the feedback from distributor interviews, and the importance of each digital strategy with regards to each type of  distributor. The data collected was utilized to create a stratification matrix that allow the manufacturer to prioritize  digital engagement initiatives. A digital strategy framework was developed to identify core strategy components of a  digital strategy implementation for both the manufacturer and their channel partners. This project also helped to  demonstrate the returns of digital engagement and the risks of non-engagement to show proof of value to distributors.   Introduction  Manufactuer S is a global manufacturing leader with sales of $27B , operates in over 100 countries worldwide, and  employs over 140,000 people globally. This manufacturer is focused on being a global leader in energy management,  and in support of this goal has utilized an ongoing digital transformation strategy to meet the changing needs of  customers and the electrical industry as a whole. This strategy focuses heavily around engaging with distribution  partners in new ways, specifically utilizing technology as a connecting tool across the supply chain. The manufacturer  currently has limited visibility regarding the digital strategies of its channel partners, and there is a need to understand  where distributors have implemented a digital strategy profitably and how digital engagement leads to revenue growth  and stronger value propositions. This project will develop a digital strategy framework to identify core strategy  components and the quantifiable benefits of a digital strategy implementation for both the manufacturer and their  channel partners. The strategies will be identified and developed utilizing the manufactuerer’s global digital initiatives  as well as input from key stakeholders within core channel partners. The framework will be of a four by four matrix,  consisting of four key digital strategies and four distributor types. Metrics will be applied within the matrix to identify  components of each strategy that are most critical for that particular type of partner and will provide the basis to  perform a stratification to identify and address channel partners with underdeveloped digital strategies. The matrixed  framework will also serve as a tool for continuing evaluation and tracking for distribution partners and be a platform  for future modification as digital needs continue to shift.",tamu.edu,Texas A&M University - College Station,United States,30.6108618,-96.35206061388457
17,REVENUE RECAPTURE STRATEGIES THROUGH CLEARANCE ANDMARKDOWN PRODUCTS: A CASE IN HOME GOODS & FURNITURECOMPANY,@gmail.com,"Revenue Management, Pricing, Returns, Revenue Capture ","Zmart is a major retailer in the furniture and home goods market with over $3 billion in sales and over 2,000 locations.  With a ~11% return rate, the company managed over $300 million in returns in 2019. While some of this revenue is  recouped through the clearance program, additional expenses generated in receiving, managing, and selling clearance  inventory are very high. Like most companies, Zmart tracks its sales, returns, and customer satisfaction (NPS) through  various tools and metrics. Unfortunately, allocations of returns, markdowns, and clearance costs lack transparency at  Zmart which leads to profit drain. To a ddress this revenue loss, a detailed costing analysis of returns and  clearance/markdown merchandise was conducted to document the costs associated with every step in the process.  Focusing on mattress sales, returns and clearance we identify the greatest opportunity for recovery by eliminating  profit drain at Zmart. Industry and non-industry best practices were explored to develop a set of revenue recapture  strategies, including cost analysis, pricing structures, inventory stratification, and customer service evaluations for the  overall product mix. The revenue recapture strategies developed to maximize clearance program revenues were  aligned with the business model and value proposition. These strategies can be utilized i n  any industry to identify  waste and eliminate profit drain. In Zmart’s case The ROI of implementing these strategies was projected at a 60%  increase in revenue capture and an 18% cost reduction, resulting in an additional $100 million in revenue and $40  million of profit annually. This paper outlines the revenue capture methodology and strategies developed that can be  generalized and adapted to most retail, industrial, and B2B environments.   Keywords  Revenue Management, Pricing, Returns, Revenue Capture  Introduction  In every industry, companies invest millions of dollars to ensure the right product gets to the customer. Despite their  best efforts, companies fail creating returns. Products are returned for various reasons: damage, customer expectations,  incorrect product, and others. Despite best efforts, returns happen and often it becomes the company’s responsibility  to absorb the loss. For large distributors, this means paying for pickup services. For others, this means having a product  that is no longer considered new and having to sell it for decreased profit than projected. In either scenario, a loss is  incurred. There are two potential solutions to mitigate this loss: by reducing the amount of returns or mitigating the  loss when a return occurs.   Methodology  To evaluate returns, Zmart used a short questionnaire which was sent to members of the sales team, distribution center,  and leadership asking for the causes of returns. These results were sorted, categorized and ranked by each group and  the whole. A secondary survey was sent to the sales force with the identified reasons for returns and ranked from most  common to least common. A secondary metric was added to rank the impact on revenue (i.e. loss to the company).  After reviewing data to understand the causes of returns vs. impact of returns by analyzing voice of sales force vs.  voice of the customer, a pilot program was launched to help mitigate returns. The pilot program ran for a quarter and  collected data recording occurrences, reasons for returns, loss in revenue, and loss in gross margin. This allowed Zmart",gmail.com,,,46.3144754,11.0480288
18,COMMUNICATING UNCERTAIN INFORMATION FROM DEEPLEARNING MODELS IN HUMAN MACHINE TEAMS,@mst.edu,"Human Systems Integration, Recommendation System, Artificial Intelligence, Uncertainty, Human Machine Team ","The role of human -machine teams in society is increasing, as big data and computing power explode. One popular  approach to AI is deep learning, which is useful for classification, feature identification, and predictive modeling.  However, deep learning models often suffer from inadequate transparency and poor explainability. One aspect of  human systems integration is the design of interfaces that support human decision -making. AI models have multiple  types of uncertainty embedded, which may be difficult for users to understand. Humans that use these tools need to  understand how much they should trust the AI. This study evaluates one simple approach for communicating  uncertainty, a visual confidence bar ranging from 0-100%. We perform a human-subject online experiment using an  existing image recognition deep learning model to test the effect of (1) providing single vs. multiple recommendations  from the AI and (2) including uncertainty information. For each image, participants described the subject in an open  textbox and rated their confidence in their answers. Performance was evaluated at four levels of accuracy ranging  from the same as the image label to the correct category of the image. The results suggest that AI recommendations  increase accuracy, even if the human and AI have different definitions of accuracy. In addition, providing multiple  ranked recommendations, with or without the confidence bar, increases operator confidence and reduces perceived  task difficulty. More research is needed to determine how people approach uncertain information from an AI system  and develop effective visualizations for communicating uncertainty.  Keywords  Human Systems Integration, Recommendation System, Artificial Intelligence, Uncertainty, Human Machine Team  Introduction  Artificial intelligence (AI) recommendations are not only found in online shopping, streaming services, and smart  home devices. Increasingly, there are efforts to embed AI recommendations in high-risk work contexts such as the  military, healthcare, and manufacturing (Ashiku & Dagli, 2019; Gottapu & Dagli, 2018). Consequently, it is critical  to understand how people use AI recommendations in situations with varying uncertainty and potential impacts.  One popular approach to AI is deep learning. In the context of image recognition, deep learning models use  neural networks to find similarities in each image and categorize them accordingly (see Exhibit 1). Neural networks  are essentially rows of computational cells in layers that process information individually and pass information on to  the next layer. The network learns and thus improves the more it is used. These networks start to recognize patterns  between examples, which helps classify future examples or information. While neural networks excel at specific tasks  as they learn from data, they are poor at extrapolation. It is possible to give prediction probabilities for different choices  in clustering problems for deep learning models that use “softmax” functions in the last layer of the network. This  probability is valuable for AI systems that interact with humans as a representation of uncertainty or confidence for  each recommendation.",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
19,POLICY ANALYSIS FOR COMMUNITY RETREAT IN COASTALREGIONS,@virginia.edu,"Housing policy, Policy analysis framework, Natural disasters, General equilibrium model, Coastal areas, Climate ","Climate change is leading to severe storm events with increasing intensity and frequency. Public policies are needed  to address vulnerable households that might be damaged or destroyed by storm surges and coastal flooding. To ensure  the well-being of the public and save billions of dollars in damages the poli cies must mitigate the consequence and  likelihood of risky extreme storm events. Design and implementation of policies faces challenges in  prioritizing  community safety conscious of budget and community values as constraints. The intention of this research is to answer  the questions: 1) what are effective public policies to address climate vulnerability at the household level in coastal  areas and 2) how does this scale fit within a community comprised of diverse individual households?  To answer these, an economic framework is developed, addressing housing policy analysis in coastal areas  facing climate risk by modeling the decision of households to move from environmentally risky coastal areas to safer  inland regions. This happens through a threshold model capable of explaining the moving decisions of households.  Households, with different wealth levels, can decide either to stay or move to a safer location based on their perception  of future risk as well as their financial and physical mobility. Different intervention scenarios by the government are  developed and analyzed. This research has several theoretical and empirical outcomes demonstrating the behavior of  households and the optimum policies to be taken paving the way for a comprehensive policy analysis  platform that  would be adaptable to specific geographic, economic, and environmental conditions.  Keywords  Housing policy, Policy analysis framework, Natural disasters, General equilibrium model, Coastal areas, Climate  vulnerability.  Introduction  Climate change has increased the frequency and damaging impact s of weather-related natural disasters in the last  decade and promises to continue this trend in the future (IMF, 2017). In the United States, the annual average number  of weather and climate disasters with CPI -adjusted losses exceeding one  billion for 1980 – 2018, is 6.2 events.  However, for  the last five  years (2014 - 2018) this rose to 12.6 events (NOAA, 2019).  Coastal areas are one of the  most vulnerable locations for climate related natural disasters. For example, flooding is estimated to be from 300% to  900% more frequent in U.S. coastal communities than it was just 50 years ago. At the s ame time, almost 40 percent  of the U.S. population lives in high-population-density coastal areas, exposing them to the increased risks of sea level  rise; flooding, shoreline erosion, and other climate hazards (National Ocean Service, 2019).  Within vulnerable areas, there is heterogeneity among localities/neighborhoods regarding their risk of  exposure to natural disasters. For households living in localities that are exposed to high risk of disasters it might be  reasonable to move to safer locales. Assuming symmetric information, this will cause the price of property in risky  neighborhoods to decrease while the excess demand for housing in safer surroundings would increase property prices  in those locations. This puts residents in areas of high climate risk at an economic disadvantage relative to those in  safer locations. Low-income households and communities  are at the greatest disadvantage in this situation. They are  likely to have the least capacity for climate resilience in the form of insurance and damage-reduction modifications to  their property, and suffer the greatest losses as a percentage of household income as a result  (SAMHSA, 2017).  Furthermore, their relatively low assessed property value and high losses from disasters, makes them least likely to be",virginia.edu,"University of Virginia, Charlottesville",United States,38.0448061,-78.5166906
20,ASSESSING THE ASSESSMENT: AN EXAMINATION OF RESILIENCETOOLS IN WATER RESOURCE MANAGEMENT,@ttu.edu,"Social-ecological resilience, resilience assessment, resilience theory ","A variety of assessments have been developed to evaluate and, ultimately, enhance the resilience of water resource  systems.  This is no mean task given that such systems face a wide range of potential stresses including acute events  such as floods or sabotage to longer term pressures exerted by growing populations and economies.  The challenge is  exacerbated by the fact that the resilience theory undergirding such assessments remains unsettled.  This paper  examines two resilience methodologies: 1) Risk Analysis and Management for Critical Asset Protection (RAMCAP)  and 2) Climate Risk Informed Decision Analysis (CRIDA).  While some of the stresses enumerated in RAMCAP can  be considered chronic or long term in nature , the approach is, arguably, oriented to more proximal natural and  anthropogenic pressures on water systems.  CRIDA, on the other hand, was designed to prepare water resource  managers for the challenges associated with climate change including increased volatility and uncertainty regarding  water supply.  These two assessments are compared against the literature that identify absorption, adaptation, and  recovery as measurable resilience properties.  Additionally, the theoretical foundations of each tool are examined  regarding stakeholder identification and temporal horizon.  T he authors consider how these approaches might be  combined to create a more theoretically robust and operationally wieldy approach to resilience assessment and  enhancement.  In so doing, the contribution that systems thinking can make to effective water resource management  is highlighted.  The paper concludes with a brief discussion of two possible lines of future research.  Keywords  Social-ecological resilience, resilience assessment, resilience theory  Introduction  As resilience becomes more commonly identified as a goal of water resource managers and as assessment  methodologies mature, it is increasingly imperative to disambiguate acceptable variations  in resilience approaches   across applications from conceptually naïve and/or counterproductive ones.  Doing so involves tried and true methods  of conceptual analysis as well as careful reflection on what can be measured and what the resulting data represent.  In  this analysis, the authors seek to further the resilience dialogue by examining two assessment approaches.  The first is  the Risk Analysis and Management for Critical Asset Protection (RAMCAP) which is recommended by the U.S. EPA  to meet the federally mandated Risk and Resilience Assessment (RRA) required for water utilities.  The second is the  Climate Risk Informed Decision Analysis (CRIDA).  Both RAMCAP and CRIDA have been developed to not only  assess current system resilience but offer strategies for increasing it.  The dual desiderata of justification and efficacy  require that each tool be theoretically grounded and clear eyed regarding what is being assessed.    It should be noted that RAMCAP and CRIDA are not the only resilience assessment methodologies that have  been proposed/deployed (e.g. the I RAM approach developed by Biringer et al, 2013).  However, RAMCAP and  CRIDA were chosen for several reasons.  First, the fact that RAMCAP is prescribed to help meet federally mandated  RRAs suggests that it will be widely applied by municipalities in the years to come  and thus an appropriate method  to scrutinize.  Second, the complementary fo ci of RAMCAP and CRIDA mak e for an ideal compare and contrast  analysis as well as, potentially, a synthesis of the two.  This is an attractive possibility given how complex water  system management can be.  Third, space constraints for these proceedings encourage a focused versus comprehensive  approach to analysis.   This paper is divided into four parts.  Part 1 surveys the theoretical underpinnings for this analysis as well as  reviews several properties of resilience that are proposed in the literature to quantify system resilience.  In Part 2, the  RAMCAP approach is introduced.  The approach is then analyzed utilizing the theoretic foundations and properties",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
21,THE WORKFORCE ENGAGEMENT MANAGEMENT APPROACH TOIMPROVING SYSTEMS USING THEORY OF CONSTRAINTS ANDLEAN SIX SIGMA INTEGRATION,@wsu.edu,"Online, Engineering Management, Master’s Degree. Theory of Constraints, Lean, Six Sigma, Workforce Engagement ","Washington State University’s Engineering and Technology Management (ETM) online master’s degree teaches in- depth courses in Theory of Constraints ( TOC), Lean, and Six Sigma system improvement methodologies . However  proponents of using only one methodology can express devotion to their respective discipline as though the three are  mutually exclusive.  In reality, the three methodologies work together synergistically, and there is a need for business  leaders to learn how to integrate them.  Thus, the ETM program has completely revised a course to teach industry  leaders how to integrate the  methodologies with a management approach that promotes employee engagement for  long-term improvements. This paper describes how to integrate the TOC, Lean, and Six Sigma system improvement  approaches for global optimization. The method of instruction uses active collaborative learning techniques and is  divided into three learning modules (Theory, Application, and Management) with a management approach that focuses  on creating an environment for fully engaged employees  that can be summarized with the statement, “Take care of  your people, and they will take care of everything else.”  Keywords  Online, Engineering Management, Master’s Degree. Theory of Constraints, Lean, Six Sigma, Workforce Engagement  Introduction  Washington State University’s (WSU) Engineering and Technology Management (ETM) department provides a fully  online master’s degree in which classes are taught live-online via Zoom video conferences on weekday evenings, and  each class is recorded for students that prefer to watch the recordings at their convenience.  Live-online classes make  it possible for online education to help overcome the common misconception that online learning is a solitary, self - paced, non-instructor led activity (Bourne, Harris, & Mayadas, 2005), and it retains the social and participation aspects  that are a key factor in the success of online learning (Richardson & Swan, 2003).  The 2010 meta-analysis performed  by the U.S. Department of Education  (DOE, 2010) found that, on average , students in online learning conditions  actually performed modestly better than those receiving face-to-face only instruction.  Thus, the live-online learning  method employed by the WSU ETM department is an effective and convenient environment for industr y leaders to  learn the tools required to implement system improvement while working full-time.    The WSU ETM program teaches three systems improvement methodologies:  Theory of Constraints, Lean,  and Six Sigma.  Systems are limited by constraints that restrict achieving a goal, and the Theory of Constraints (TOC)  is a methodology used for identifying such constraints.  TOC promotes focusing efforts to eliminate that constraint or  bottleneck until it is no longer the limiting factor in order to improve throughput, reduce inventory, or lower operating  expenses.  Lean is a methodology used to improve the performance of a system by removing waste, and Six Sigma is  a methodology used to reduce variability.  Lean and Six Sigma are sometimes combined as Lean Six Sigma (LSS).    The ETM program teaches TOC, Lean, and Six Sigma methodologies in the following ETM courses:  • TOC:  EM 530 Applications of Constraints Management    • Lean: EM 538 Lean Tools for Systems Improvement   • Six Sigma: EM 585 Design of Experiments and EM 580 Quality Control and Reliability   Currently in the ETM program, the course EM 530 teaches finding and eliminating the bottleneck with TOC, EM 538  teaches reducing waste with Lean  tools, and EM 580/585 teaches reducing variability with Six Sigma, there can be  confusion in industry as to when to use Lean, Six Sigma or TOC.  Proponents of using only one methodology (TOC,  Lean, or Six Sigma) can  express devotion to their respective discipline as though th e three are mutually exclusive.",wsu.edu,Washington State University,United States,46.7337716,-117.14980348311619
22,A CONSTRUCTION PROJECT CLASSIFICATION SYSTEM FORQUALITY,@projectqualityconsulting.com,"Construction Quality, Project Quality, CoQ Classification, Construction Classification, Project Classification, Quality ","Construction projects require a classification system based on the domain the project is in. For the Quality domain, no  such classification system exists. Construction projects are executed to different quality standards depending on the  type of project and amount of money being invested. T here is a significant difference in the quality requirements  associated with residential, commercial, infrastructure and industrial projects. This results in significant difficulty  when attempting to identify and/or compare quality managem ent systems, cost of quality (CoQ), and process  requirements for the full spectrum of construction projects. This research investigates and discusses  measurement  system requirements, published construction classification system research for relevant domain s, and presents an  exploratory quality classification system for use by when attempting to classify  and compare construction projects.  The proposed classification system is aligned with the primary constraints in construction including its ease of use  and supports research into key elements such as planning and controls, materials, specifications, complexity and  human factors. Various case studies for cost of quality, prevention, appraisal, failures were classified using the system  to determine trends in qu ality research  and validate the classification system . It was noted that  construction  megaproject CoQ research has been minimal.  Keywords  Construction Quality, Project Quality, CoQ Classification, Construction Classification, Project Classification, Quality  Classification.  Introduction  This is a critical period in the construction industry. Pre -Covid 19 pandemic , there was annual global spending o f  roughly $ 10 Trillion (T) per year on construction related goods and services (Mckinsey, 2017). The con struction  industry is relied on to make up for lost time and ensure project outputs, including housing, is available on schedule.  Unfortunately, when looking at global statistics, 72% of construction projects are delayed with an average increase in  duration of 38%, 63% of construction projects experience cost overruns with an average increase in cost of 24%, and  Customer satisfaction with the completed work is low (Rivera et al, 2017).    Construction industry issues are also identified by looking at productivity. Fulford and Standing (2013) noted that  construction lags behind other industries in terms of productivity improvement. Construction productivity has  increased at an average annual rate of 1% annually since 1997 while the world economy’s rate and the manufacturing  rate have increased annually at 2.8% and 3.6 respectively. Since 1945 in the US, productivity in construction has  increased marginally while productivity in sectors such as retail, manufacturing, and agriculture has increased 1,500%  (McKinsey, 2017).   Given that the construction industry as a whole has significant issues with project delivery, practitioners and  researchers have a joint responsibility to ensure that performance is improved (Soderlund, Sankaran & Biesenthal,  2017). Measuring, analyzing, and use of quality management techniques is required. The problem addressed by this  research is the lack of transferability of quality measures within the construction industry which constrains the ability  of quality practitioners and researchers to communicate, compare results, implement solutions, and ultimately  influence construction industry improvement.   In the domain of project management (which includes construction, defense, software development, research and  development, and a host of other sub- domains), there were a number of attempts to identify a universal theory of",projectqualityconsulting.com,,,46.3144754,11.0480288
23,A MULTI-LEVEL ANALYTICAL FRAMEWORK FOR MODELING U.S.ECONOMIC GROWTH,@ttu.edu,"Strategic Management, Dynamic Stochastic General Equilibrium models, Forecasting, Auto-regressive model, Factor ","Knowledge of the historical and changing state of a countries economic performance as well as internal performance  of a company’s key performance metrics are critical to iterative development of strategy development and deployment.  This article offers an improvement in methods for monitoring external and internal performance of key performance  measures. We specifically address external monitoring related to the economy, however the framework can be applied  to other external or internal measures. Research in macroeconomics describes economic performance as a function of  key economic health indicators (KEHIs) such as output, unemployment, and inflation with the goal of understanding  the underlying drivers of KEHIs in order to help gover nments, businesses and people make informed decisions  regarding strategy development and deployment. The understanding of economic performance through the KEHIs can  be broken into the following components: describing historical performance (including current status) and forecasting  future values. Models used to: describe and forecast KEHIs can be partitioned into parametric and nonparametric  which differ by how they represent reality. Parametric models start with theoretical relationships and let data influence  the model parameters. Nonparametric models let the data, from individual or multiple economic series, influence the  model selection. The state-of-the-art parametric macro-economic models did not forecast the 2008 recession. This  paper suggests a 2-level analytical framework , based on a proposal by Blanchard, that develops a historical  understanding of the data as a foundation and builds knowledge with nonparametric models of increasing complexity  that can inform parametric modeling efforts, improving the reliability of external and internal monitoring.   Keywords  Strategic Management, Dynamic Stochastic General Equilibrium models, Forecasting, Auto-regressive model, Factor  Augmented Auto-Regressive model, Statistical process monitoring, Change Points.  Introduction  Knowledge of the historical and future state of a country’s economic performance is critical to decisions made by its  government, and businesses regarding strategy development and deploym ent. Decision makers can further benefit  from extending this understanding to the drivers of performance to help them develop strategic plans for maintaining  strong economic or business performance and growing economies or businesses that are faltering. This paper proposes  a framework to support an iterative model for strategic management (Exhibit 1) as discussed by Saunders, Mann, and  Smith (2009). The focus will be targeted on economic modeling that can help government leaders develop and deploy  a strategy. The same methods relate to businesses that monitor external economic health as well as internal Key  Performance Indices related to business strategy development and deployment.   Research in macroeconomics describes and forecasts the performance of economic indicators such as GDP,  unemployment and inflation. In recent years macro -economic modeling has run into a capability crisis (Blanchard,  2016a; Congress, 2010 ; Krugman, 2009; Romer, 2016 ; Stiglitz, 2018) . State of the art macroeconomic models,  Dynamic Stochastic Generalized Equilibrium (DSGE) models, failed to  forecast or provide a warning of the  impending 2008 recession. Congressional hearings were held in 2010 by the Committee on Science and Technology",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
24,BASIS ESTABLISHMENT FOR QUANTITATIVE RISK ANALYSIS (QRA),@ttu.edu,"HRO, QRA, System Dynamic Analysis ","The potential for catastrophic accidents is simply inherent in certain industries.   Organizations that are considered  Highly Reliable Organizations (HROs) have operations that have the inherent potential for a catastrophic accident,  but they operate for a long period of time without being subject to a catastrophic accident. The original analysis into  this field was done in 1990 by Karlene Roberts who studied flight deck operations on Navy aircraft carriers. Her  analysis defined standard traits that HROs possess. The next step is analyzing such operations is to perform  quantitative analysis in order to have  a better sense of the control an organization has over its inherently risky  operations. Quantitative Risk Analysis (QRA) has been in existence since the 1970s. QRA is the analysis of hazards  to understand what level of risk is being assumed by the operation of the analyzed process. The application of analysis  is beneficial for the person or entity that accepts the risk of and the responsibility for the operation. The emphasis on  tracking near misses as the patterned behavior that is usually present prior to catastrophic events has been identified  as a central management approach to achieve HRO safety levels. This paper expands this area of research to include  the strength of Systems Dynamic Modeling and the establishment of specific risk limits to measure the quantitative  analysis against to further enhance the strengths of HRO safety theory.  Keywords  HRO, QRA, System Dynamic Analysis  Introduction  Research into organizations that have the inherent potential for catastrophic outcomes was first performed by  Karlene Roberts.  Navy aircraft carrier operations are inherently risky with the constant potential for loss of life.  Dr.  Roberts studied those operations for over a year and a half in order to understand the underlying principles of  operation that allowed them to operate for long periods of time without catastrophic consequences (Roberts, 1990).   Those operations stand in stark contrast to occurrences such as the Bhopal chemical leak, Challenger and Columbia  space shuttle failures the Chernobyl nuclear power plant disaster, and the Fukishima reactor melt down.   Catastrophic consequences result in loss of life, harm to the public, the environment and significant damage to the  company responsible for the disaster.  The initial research performed by Dr Roberts was qualitative in nature,  identifying the principles that maintain safe carrier flight deck operations.  Subsequent analysis identified such  organizations as the Navy flight deck operations as Highly Reliable Organizations (HROs).  Roberts defined an  HRO as “To identify these organizations one must ask the question, ‘How often could this organization have failed  with dramatic consequences?’ If the answer to the question is many thousands of times the organization is highly  reliable.” (Roberts, 1990, pp. 1-2).    Additional research has identified the basic, underlying characteristics of HROs.  Weick and Sutcliffe  (2001)  defined five characteristics of an HRO.  1.  Preoccupation with failure.  An organization must accept the real possibility  of failure.  The existence of unknown unknowns.  2.  Reluctance to simplify interpretations.  In the review of small  failures, the tendency to gloss over causes must be resisted.  The oversimplification of analysis can allow significant  problems to continue to exist and cause catastrophic failures down the road when the circumstances line up to allow",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
25,SUCCESS FACTORS FOR LEAN PRODUCTION IN LOCALGOVERNMENT ORGANIZATIONS: A SYSTEMATIC LITERATUREREVIEW,@vt.edu,"Lean production, lean six sigma, success factors, challenges, local government. ","Lean production (LP) has been widely recognized as an improvement methodology for achieving significant  performance outcomes such as creating value for customers, reducing lead time, and rationalizing resources. Many  successful applications of LP have been documented over the last few decades, covering a wide range of industrial  applications beyond manufacturing, such as healthcare, finance, and educational institutions. Furthermore, with the  successful contributions of LP to these types of organizations, researchers and practitioners have more recently applied  LP to public sector organizations, particularly local government organizations (LGOs). While the potential benefits  of LP may be appealing to LGOs, its adoption carries unique challenges. Therefore, there is a need to document these  challenges that are particularly associated with LP implementation in LGOs. This work synthesizes the literature using  a Systematic Literature Review to identify these challenges, referred to in this work as success factors (SFs) . These  success factors describe enablers for or barriers to LP adoption . A co-occurrence network is used to examine the  relationships among the SFs documented in the literature . Examining the published literature to identify and better  understand success factors provides researchers and practitioners with insights to increase the likelihood of successful  LP implementation.    Keywords  Lean production, lean six sigma, success factors, challenges, local government.  Introduction  Lean Production (LP) is an improvement methodology that has been defined as a set of principles and tools that aim  for continuous process improvements (Bhamu & Sangwan, 2014; Womack et al., 1990). LP has also been described  as a methodology  - including a set of principles (or, philosophy), tools and techniques, and processes  - used to  maximize customer value and minimize waste (Bhamu & Sangwan, 2014; Pedersen & Huniche, 2011). To maximize  value for the customer, organizations are expected to pursue  the key principles of LP . These principles have been  categorized into specifying customer value, identifying value stream for products and customers, creating a continuous  flow among activities, producing in response to customer demand (pull production), and continuous improvement  (Womack et al., 1990). Furthermore, waste is defined as any activities that result in high cost and non- value-added  activities to the end user. The seven common types of waste identified in LP are excessive transportation, unnecessary  inventory, unnecessary motion, overproduction, over-processing, waiting, and defects (Ohno, 1988).   LP is rooted in the Toyota Production System (TPS) and was designed to eliminate waste from processes, while  Six Sigma (SS) was developed by Motorola with the aim of reducing variation within processes (Antony et al., 2016).  LP and SS are both improvement methodologies , each of which encompass a distinct set of principles and tools for  continuous improvement . However, based on the uniqueness and contribution of each approach, scholars and  practitioners have affirmed that integrating and synthesizing LP and SS provides a  complementary improvement  methodology that results in maximizing benefits and better achieving desired goals. This integration is known as Lean  Six Sigma (LSS), a combination of practices and tools from LP and SS (Antony et al., 2016; Gaikwad & Sunnapwar,  2020; Shokri, 2017; Sreedharan et al., 2018). The applications of LP and LSS have been successfully demonstrated in  manufacturing settings over the last four decades . However, LP and LSS are also applicable to non-manufacturing  organizations such as healthcare, education, and finance/banking (Singh & Rathi, 2019; Sreedharan & Raju, 2016) ,  and has increasingly been implemented in local government organizations  (LGOs) (Furterer & Elshennawy, 2005;  Suarez Barraza et al., 2009). The primary interest of this work is LP (vs. SS), but because a number of organizations  use an integrated LSS approach, both LP and LSS were identified as being of interest. Thus, for the purpose of this  work, the term LP/LSS is used to refer to the overall improvement methodology in LGOs.",vt.edu,Virginia Tech,United States,37.22192675,-80.42728184013652
26,ECONOMIC ANALYSIS OF SECOND USE BATTERIES FOR ENERGYSTORAGE IN MICROGRIDS,@northeastern.edu,"Economic analysis, Cash flow, Electric vehicle, Lithium-ion batteries, Second use batteries. ","Electric vehicles (E.V.s) are emerging due to their potential to reduce energy use and emissions in the transportation  sector. The batteries in E.V.s might not last for the vehicles' whole life span as their condition deteriorates over their  life. The concept of giving a second life or second use to electric vehicle lithium-ion batteries is to reuse the batteries  that can no longer fulfill its requirement in the automotive domain but could still be useful as energy backups in grid- connected microgrids. The microgrid industry does face a cost barrier when it comes to backup power and reusing the  lithium-ion batteries could be the solution to increase the value of services provided by the batteries. Previous studies  have focused on the technical feasibility and environmental  and sustainability impacts of reusing the E.V. batteries.  This study compares the usage of n ew and second  use E.V. batteries  in the photovoltaic microgrid system through  economic analysis. The result shows the economic advantage of using the second use of E.V. batteries compared to  the new batteries for energy storage in the microgrid system.  Keywords  Economic analysis, Cash flow, Electric vehicle, Lithium-ion batteries, Second use batteries.  Introduction  Electric vehicles are increasingly getting attention due to their potential to reduce CO2 emissions, energy dependence,  and transportation costs  ( Egbue & Hosseinpour, 2014 ; Elgowainy et al., 2016 ). According to the report by Grid  Integration Tech Team (GITT) and Integrated Systems Analysis Tech Team (ISATT) of the U.S. DRIVE Partnership,  that examined a range of Electric vehicles (E.V.s) market penetration scenarios (low, medium, and high), E.V. sales  in 2030 are estimated to total 320 thousand (2% of new vehicle sales), 2.2 million (12%), and 6.8 million (40%) in the  low, medium, and high penetration scenarios respectively (""Summary Report on E.V.s at Scale and the U.S. Electric  Power System,"" 2019). This results in a total E.V. fleet size i.e., cumulative vehicle sales of 3 million (1% of the total  passenger vehicle fleet), 14 million (5%), and 40 million (15%) vehicles by 2030, respectively (""Summary Report on  E.V.s at Scale and the U.S. Electric Power System,"" 2019).   In E.V.s, the battery pack is the source of energy. However, the batteries might not last for the whole life span of  the E.V.s as their condition deteriorates over their life (Amiri, Esfahanian, Hairi -Yazdi, & Esfahanian, 2009 ). The  dynamics of driving have a different impact on ba ttery degradation. In literature, many studies tried to develop an  accurate model, either performance- based models or energy throughput models, to simulate battery degradation  (Bishop et al., 2013 ). As the use of Lithium-ion batteries (LIB) in E.V.s is becoming increasingly prevalent due to  their high energy-density and high power- density (Mathew, Kong, McGrory, & Fowler, 2017 ), this study primarily  focuses on LIB. The calendar life, i.e., the battery life on rest without the occurrence of electro -chemical reactions  (Pistoia, 2010), of a LIB, is 15-20 years (Keeli & Sharma, 2012).   Degradation of batteries over a period leads to reduced energy storage capacity. Factors such as usage, climate,  and charging type affect the health of batteries. The bulk of E.V. batteries sold in the U.S. are covered under warranty  for at least 8 years or 100,000 miles (“How long should an electric car’s battery last,"" 2019). Most of the automotive  manufacturers recommend battery replacement when the storage capacity decreases  to 70% - 80% of the original  energy capacity (Keeli & Sharma, 2012; Viswanathan & Kintner-Meyer, 2011). With the increase in electric vehicle  batteries in the market, batteries' disposal will be  challenging  (Harper et al., 2019 ). It could, however, be used to",northeastern.edu,,,46.3144754,11.0480288
27,ESTABLISHING INTER-RATER RELIABILITY FOR AN ASSESSMENTTOOL IN SIMULATION-BASED HANDOVER TRAINING,@utsouthwestern.edu,"Patient Handover, Inter-rater Reliability, Simulation Training. ","A patient handover involves the transfer of information, responsibility, and authority in a healthcare setting. Structured  handovers are critical for effective communication between care providers. Poor patient handovers can contribute to  serious medical errors. Therefore, training health profession students on how to effectively perform a structured  handover is a core component of their education and will prepare them for clinical practice. This research describes  the study conducted to establish inter -rater reliability for  a new assessment tool for  evaluating learners performing  handovers in a simulated setting. This assessment tool focuses on critical items related to handover content, process,  and language present in high-quality, structured handovers. The handover simulation which is part of a course called  Transitions to Clerkship was recorded for 64 groups of learners. Out of these 64 recorded handovers, 30 videos were  selected, through a randomized block design, for grading by four  raters who were trained on how to use the tool. A  two-way random model was used to calculate the I ntraclass Correlation Coefficient (ICC) for inter-rater reliability.  ICC for absolute agreement and consistency were 0.507 and 0.617 , respectively, suggesting a  fair to good  level of  reliability in the context of this study.  The paper concludes with a list of potential factors le ading to these reliability  scores.   Keywords  Patient Handover, Inter-rater Reliability, Simulation Training.  Introduction  Patient handover or patient handoff is a two-way communication modality in which one care provider (e.g. a nurse)  efficiently transfers a summary of the most relevant patient information to another care provider (e.g. a physician),  ensuring continued quality for that patient’s ongoing care. Usually, this process involves the transfer of responsibility  and accountability from the sender (e.g. the nurse) to the receiver (e.g. the physician) as well as the relevant patient  information. Failing to mention critical information about the patient (such as  pertinent labs or medical history) or  relaying incorrect information (such as commenting a patient has no drug allergies when a patient actually has multiple  hypersensitivities) to the receiver may lead to unpredictable or irreparable consequences. Therefore, it is important  that today’s health profession students are appropriately trained to both deliver and receive structured handovers.   Structured handovers can have different formats.  For example, using a checklist or a mnemonic such as  SBAR, which is an  acronym for Situation, Background, Assessment, and Recommendation,  can enhance  communication between care providers (Methangkool, Tollinche, Sparling, & Agarwala, 2019). SBAR is a technique  that is widely used by health professions such as physicians, nurses, residents, etc. in present-day practice. SBAR was  designed to improve the effectiveness of communication by standardizing the method how information is conveyed   (Pope, Rodzen, & Spross, 2008).  Medical students need patient handover education as part of their curriculum  to be prepared for practice.  Various training methods for handovers exist such as tutorial videos and oral scenarios. One of the best ways to learn  handovers is through high-fidelity patient simulation (Lewis, Strachan, & Smith, 2012; Di Delupis, et al., 2014). The  Association of American Medical Colleges (AAMC) has identified the ability to deliver and receive handovers as a  core competency for medical students nationwide  (AAMC, 2017) . However, the AAMC has not specified an",utsouthwestern.edu,,,46.3144754,11.0480288
28,"TO STUDY THE EMERGING TRENDS IN PROJECT MANAGEMENT;AI IN PROJECT MANAGEMENT AND EMOTIONAL INTELLIGENCE,AND TO ASSESS THEIR EFFECTS IN THE INDUSTRY.",@northeastern.edu,Introduction ,"Ever since humans have had people and projects to manage, we’ve had challenges keeping the projects on time, on  budget and achieving the desired scope. The field and industry of project management emerged as a way to manage a  collection of people towards a goal. For project managers, it has been very common to be managing not only multiple  teams, but also multiple projects at the same time. AI has been entering every part of this world and so is into project  management. There is software which reduces the backend work of a project manager. The computer itself schedules  tasks, meetings and collaborates with various platforms in an organized way. This has led to the reduced demand for  a project manager. But again, there is an emerging trend known as emotional intelligence; which states that the  project’s success is based on the emotional quotient of the manager with the teammates and other collaborators.  Studies have found out that there is a huge difference in the success graph of the projects. Thus, according to this  paper, there are these two of many topics which are trending in project management with a very different individual  approaches yet complimentary and working towards the same problem of keeping the Iron Triangle balanced.  Keywords: AI, Emotional Intelligence, Project Management, Iron Triangle   Introduction  PMBOK defines a project as, “A project is a temporary endeavor undertaken to create a unique product, or service  or result. Projects are undertaken to fulfill objectives by producing deliverables. An objective is defined as an outcome  toward which work is to be directed, a strategic position to be attained, a purpose to be achieved, a result to be  obtained, a product to be produced, or a service to be performed. A deliverable is defined as any unique and verifiable  product, result, or capability to perform a service that is r equired to be produced to complete a process, phase, or  project. Deliverable may be tangible or intangible.” [13]  Project management  as a concept, has been in use for hundreds of years. Over the period there have been  successful outcomes of project management. The outcomes of these projects were the result of leaders and managers  applying key skills and knowledge of project management practices, principles, processes, tools, and techniques to  their work, to satisfy their customers and other people involved in and affected by the project. [13]  A macro level organization  is motivated to implement project management techniques to ensure that their  undertakings (small or major) have a balanced Iron Triangle  (Scope, Quality and Cost). On the other hand, a micro  level organization, combines project management  with an appropriate  information management system with the  objectives of (a) reducing project overhead costs; (b) customizing the project workplace appropriate to fit the  operational style of the project teams and the workforce ; (c) actively informing the executive management strata of  the strategic projects on a real- time basis; (d) ensuring that project team members share accurate, meaningful and  timely project documents; and (e) ensuring that critical task deadlines are met.[2]  Over time with research going on into this, experts have come up with various new additions to make the task of  managing easy and much more approachable. These trends keep on changing according to the effect that the technique  imparts and helps in the smooth functioning of the project and its success. Some of these trends have a personal touch  while others are thinking towards a machine mind. Some of them try to combine different methods of management to  extract the positives of all and help the project grow towards its goals. We will study two of these trends below.",northeastern.edu,,,46.3144754,11.0480288
29,ADAPTING PROCESS IMPROVEMENT METHODOLOGIES FOR K-12EDUCATORS,@gmail.com,"Six Sigma, Lean, process improvement, K- 12 education, engineering management, problem solving, workshop, ","Process improvement methodologies like Lean and Six Sigma have been lauded in the manufacturing sector for their  ability to streamline processes, reduce waste, minimize variation and defects, and improve the bottom line. Although  these methodologies have begun to spread to other industries, their impact has been relegated to certain areas and has  largely missed others. One area that bears strong consideration is education. Education is heavily process -based and  notoriously tight on resources. It is hypothesized that if educators were taught the basics of these strategies and were  able to implement them in the classroom setting, the effectiveness of both teaching strategies and classroom operations  would improve, saving educators’ time and improving school performance in a number of areas.  This study focused on developing and testing a workshop- based method to teach these strategies. Engagement  emerged as a significant obstacle, but based on the workshop results and positive feedback from educators, the study  found that overall, the workshop is a viable method for communicating these skills. Alternate ideas for future programs  are presented based on the challenges encountered with the pilot program.  Keywords  Six Sigma, Lean, process improvement, K- 12 education, engineering management, problem solving, workshop,  engagement  Introduction  Schools are unique. Unlike businesses, which largely operate on a voluntary basis, every child in the United States  under the age of 16 is legally required to attend school (State Education Reforms, 2017). Perhaps it is because schools  are so ubiquitous that they remain, for the most part, hierarchical and rigid.  Yet schools also bear the responsibility  for creating the foundation for future functioning member s of society and for creating a learning platform that suits  students of all different backgrounds, capabilities, and needs. Years of evidence from the manufacturing sector show  that process improvement methodologies like Lean and Six Sigma can be used to find inefficiencies and drastically  improve what was thought of as the best way to do something. Since the outputs of education remain subjective and  hard to quantify, applying these principles to education may not produce the same economic improvements seen in  manufacturing. But the impact that extra instructional time or additional resources may have on one student’s life  could be profound.  Currently, educators (who are defined in this study as teachers, administrators, and other school staff members)   face multiple resource and budget limitations while having to meet the instructional goals set by the school, district,  and state. An efficient allocation and use of these resources has the potential for improvement on student outcomes  and the educators’ quality of life. Furthermore, introducing the practices of data-driven problem solving and waste  reduction to schools  has the potential to create  generations of more capable problem solvers and critical thinkers ,  something that companies  commonly cite as desirable qualities  in potential hires . Based on the success of these  principles in industry and the success individuals have had in applying these principles in their own schools or districts,  there is a strong case for application in K-12 education. If it could be applied on a national scale, improvements could  be immense. According to the National Center for Education Statistics (2018), there are approximately 132,000 K-12  schools in the United States , and those schools are staffed by a combined 3.5 million elementary, middle, and high  school teachers and principals (Bureau of Labor Statistics).  The overarching goal of this study was to hel p educators gain access to increased resource flexibility by  developing activities to teach process improvement techniques and how to implement them. This study focused on the  development and testing of a workshop-based method of instruction. Although engagement emerged as a significant",gmail.com,,,46.3144754,11.0480288
30,HOW TO DEVELOP EFFECTIVE SYSTEM ENGINEERS?,@msstate.edu,"Effective system engineers, Systems engineering, Performance instrument, Systems skill, and Complex system. ","System engineering (SE)  is a structured systematized methodology that deals with designing, managing, and  optimizing systems performance. System engineers use the perspective of system thinking to make the successful use  and retirement of engineering systems. Since the role of system engineers ranges widely from technical support to  customer interaction, system design to management,  there is a demand to develop a cadre of effective systems  engineers. However, two critical questions are not well -defined in the extant body of SE literature:  (1) What are the  fundamental attributes of systems engineering that would influence the effectiveness of individual systems engineers?  (2) What are the corresponding leading indicators for appraising the performance of an individual systems  engineer? To respond to these questions, this paper proposes a new instrument to evaluate the performance of the  system engineers and subsequently identifies their strengths and weakness within the complex system domain. The  implication of this study would assist systems engineers in strengthening their system skills  and reflects a state that  can be improved through training, workshops, and education to prepare them to face the complex situations originating  from the problem domain.   Keywords   Effective system engineers, Systems engineering, Performance instrument, Systems skill, and Complex system.  Introduction          I n years past, each engineering discipline was seen as a self -contained domain. As systems and technologies  increase in complexity, the need for interdisciplinary teams and engineers who can consider the system in a holistic  way have become a standard requirement for any systems development activity. Systems engineers fill this role to  lead interdisciplinary teams and consider the entire life cycle of the system during the development, operation, and  disposition of a system ( Hossain, Jaradat, Hamilton, Keating & Georger, 2020; Hossain & Jaradat, 2018). The  International Council for Systems Engineering developed a vision of system engineering for 2025 ( INCOSE, 2014),  in which an imperative includes “Enhancing education and training to grow a system engineering workforce that meets  the increasing demand.” Education can either cover the breadth of systems engineering knowledge or be targeted  based on the needs of the individual or organization. Building an effective workforce is not just attaining the quantity  to meet the demand, but the quality of these engineers is even more important. INCOSE also publishes a Systems  Engineering Handbook (INCOSE, 2015 ) and Systems Engineering Body of Knowledge (SEBoK) ( SEBoK  contributors, 2020) outlining the processes and core competencies of systems engineers.  Besides the processes and technical knowledge, what differentiates a great, or successful, systems engineer   needs to be addressed. A study by Davidz & Rhodes (2005) looked at how to accelerate the development of senior, or  highly skilled, systems engineers. It was posited that systems engineers would need to be quickly developed in order  to handle the increasing complexity in the field of engineering. Frank & Carlo (2007) studied the characteristics of  successful systems engineers. This study resulted in a list of 38 characteris tics of a successful systems engineer. In",msstate.edu,Mississippi State University,United States,33.4386876,-88.79432320417112
31,DETERMINING THE CONSISTENCY RATE FOR OVERALLEQUIPMENT EFFECTIVENESS USING THE COEFFICIENT OFVARIANCE METHOD,@msstate.edu,"Process, Evaluation, Performance, Competition, Efficiency, Effectiveness. ","Evolution and improvement have always been an integral part of human development. To this end , innovative  methods, techniques and equipment  are continually sought to improve the processes that impact our lives . To  determine whether  a proposed improvement will be effective in meeting  the desired requirements, process  evaluation is necessary to obtain a rough estimate of a system’s performance. Various methods, such as basic  counting, time studies, and evaluation of customer satisfaction, have commonly been used to assess the efficiency  and effectiveness of a process. However, a s the demand for products increases and competition between  manufacturers intensifies, the need to find methods that evaluate the detailed steps and overall flow of a production  process have become increasingly necessary. Over time, many methods and techniques have been developed to  determine the efficiency and effectiveness of a production process, but to date, no method has been developed to  evaluate the consistency of a process. This paper concentrates on answering this question by developing a new  metric termed “Consistent Equipment Effectiveness” ( CEE). While the commonly used Overall Equipment  Effectiveness (OEE) metric provides static values that measure the performance of a process at an instant in time,  it fails to shed light on how performance changes over time. The CEE corrects this problem by introduc ing the  concept of consistency, which uses a statistical concept called ""Coefficient of Variation"" (COV).  The concept is  applicable to any type of discrete data. This paper is intended to apply the COV to the Overall Equipment  Effectiveness (OEE) metric. Since the output obtained from the OEE is discrete, ranging from 0 to 1, and the COV  is typically a fraction in a closely controlled manufacturing process , it makes perfect sense to combine these  formulas together to generate a new metric, which can aid manufacturers in measuring process performance.    Keywords  Process, Evaluation, Performance, Competition, Efficiency, Effectiveness.  Introduction  In recent years, the demand for consistent manufacturing processes has increased substantially. As a result, many  of the methods that have been developed to measure the efficiency and effectiveness of production processes are  being investigated. Based on the work of Nakajima (1988) in the area of Total Productive Maintenance (TPM),  various scholars, such as Parikh & Mahamuni (2015), have taken the lead in developing a framework to improve  quality and industry standards. Current research streams have focused mainly on increasing productivity, reducing  costs, and following asynchronous methods in the manufacturing process.  As humans are replaced by automated  machines, there is a need to analyse the performance of the machines to determine the level of efficiency. Process  performance has traditionally been based on evaluating the unit count (number of units produced by a machine  during a specific time  period) and process time (time taken to produce a specific number of units).  With  advancements in the manufacturing field, researchers  have realized that other factors such as availa bility, cycle  time, and ideal time need to be considered (Muchiri, & Pintelon, 2008). To achieve this goal, many industries  have implemented Total Productive Maintenance (TPM) to guide the organization in identifying the factors that  impact performance (Etienne -Hamilton, 1994).  TPM is governed by eight factors, denoted as the pillars of  equipment reliability (Nakajima, 1988). These are as follows:  • Autonomous Maintenance • Focused Improvement",msstate.edu,Mississippi State University,United States,33.4386876,-88.79432320417112
32,UNDERSTANDING FORMAL AGGREGATION OF EXPERTASSESSMENTS IN SAFETY DECISION MAKING,@vt.edu,"Expert Aggregation, Safety Risk Assessment, Judgement Aggregation, Belief Aggregation, Discursive Dilemma.   ","Expert assessment is critical to safety decision making. Expert assessment is used to interpret, qualify, or complement  historical data when characterizing uncertainties related to safety. When historical records are not available,  uncertainty characterization relies solely on the experts’ assessment. Because safety of engineered systems is a  multidisciplinary problem, decision makers need to reason through the information provided by several experts. Past  work on expert aggregation in engineering management has focused on using multiple behavioral and mathematical  aggregation techniques that are subjective in nature and lack the rigor to reason about the individual assessments in a  mathematically consistent manner. In this paper, we leverage work from the fields of business management,  economics, and cybernetics to present different techniques to mathematically aggregate expert assessment. We  differentiate between expertise provided through three different information mechanisms: belief distributions,  opinions, and judgements. We show that these three forms of expert information inherently require different  aggregation. In fact, we demonstrate that most of the existing techniques for expert aggregation can actually lead to  misusing expertise.   Keywords  Expert Aggregation, Safety Risk Assessment, Judgement Aggregation, Belief Aggregation, Discursive Dilemma.    Introduction  In the safety decision making process of Safety Critical Systems SCS, information is exchanged and gathered from  several Subject Matter Experts (SMEs), described according to the respective Safety Risk Management Process  (SRMP) of the system. The SMEs assess some uncertain attributes in the decision problem (Vismari & Junior, 2008).   The process of including SME assessment into the decision process can be viewed as a three-step approach of Expert  calibration (Jongsawat & Premchaiswadi, 2010) , Expert elicitation  (Goodwin & Wright, 2014)  and Expert  aggregation (van Steen, 1992) . The objective of this research is to study inconsistencies resulting from expert  aggregation in safety decisions.  In the past, several methods have been proposed to aggregate expertise, which can be broadly classified as  behavioral and mathematical (Mak, Bui, & Blanning, 1996). Behavioral methods involve information exchange and  negotiation of opinions among the SMEs to arrive at a consensus. Mathematical aggregation involves obt aining  quantitative values (for the information sought) from the SMEs and applying some statistical averaging technique to  combine the values (Chytka, 2003). Aggregation methods have been applied in several domains and contexts (Leung  & Verga, 2007)  and their characteristics such as robustness, traceability, prediction capabilities and accuracy have  been studied (Mak et al., 1996). However, the literature is scarce in providing concrete guidelines in expert aggregation  specific to safety decision making.   Judgement aggregation is a problem for any group that tries to conclude a judgement based on sets of rationally  interconnected propositions (List & Pettit, 2002) . However, since these problems are often associated with issues in  economics, social choice and political theories, it is possible that in safety risk assessment the problems of judgement  aggregation might not have even been identified in the first place.",vt.edu,Virginia Tech,United States,37.22192675,-80.42728184013652
33,FINDING LEADERSHIP FROM SUCCESSFUL OWNERS OFDISADVANTAGED BUSINESS ENTERPRISES,@cwu.edu,"Disadvantaged Business Enterprises, Entrepreneurial and Managerial Leadership. ","Disadvantaged Business Enterprise s (DBEs) are for -profit, small businesses owned by socially and economically  disadvantaged individuals such as women or minorities. Successful DBEs have a significant contribution to economic  development and highway construction. Researchers have investigated in a variety of leadership topics such as  theories, styles, and competencies in different fields. However, l imited research studies focus on disadvantaged  business enterprises, in which the owners are mainly women or minorities. This research study collected qualitative  data from successful DBEs through semi-structured interviews. The researcher conducted 12 interviews with DBEs.  Interviews’ data were recorded, transcribed, coded, and analyzed to generalize common themes and patterns.  Specifically, the author identified entrepreneurial leadership competencies for starting a business and managerial  leadership competencies for developing a business. Most DBE interview participants favored a combination of  situational and servant leadership. Some stated that their primary leadership is to take care of employees and provide  the necessary support for growth. The research contributes to the body of knowledge by (1) revealing the leadership  styles of DBE owners, and (2) uncovering additional leadership styles and attributes of DBE owners.  The research  also provides a foundation for developing future l eadership training programs for DBE owners as DBE supportive  services.  Keywords  Disadvantaged Business Enterprises, Entrepreneurial and Managerial Leadership.  Introduction  A Disadvantaged Business Enterprise (DBE) is a small, for -profit business at least 51% owned and controlled by  socially and economically disadvantaged individuals such as women or minorities. There are seven presumably  disadvantaged groups (49 CFR Part 26): Black, Hispanic, Native, Asian-Pacific, Subcontinent Asian Americans,  women (i.e., white Caucasian), and any additional groups designated by the Small Business Administration (SBA).  The US DOT spends 42 to 46 billion dollars each year between 2016 and 2020 on construction contracts (Fixing  America’s Surface Transportation Act of 2015) . DBEs perform approximately 10 percent (i.e., about 4.4 billion  dollars) of the federally assisted contracts. DBEs perform a large portion of federally assisted contracts and contribute  to the success of construction projects in the transportation sector. DBEs are often subcontractors taking a vital  leadership role and performing a specific task in a project (Dang, 2019). Such leadership, driving the quality of work  and growth of the business, is an essential element of a successful DBE.  Literature Review  Leadership is a complex yet universal phenomenon with various conceptual and operational definitions (Bass and  Bass, 2009; Yukl  and Rubina, 2010). Definitions of leadership differ based on the contextual information and  knowledge domain. As a general definition, the Oxford dictionary defines leadership as ""the action of leading a group  of people or an organization"" (Oxford, 2018). The description of different leadership style s is driven by a variety of  theories developed over the centuries. Some common themes in scholarly definitions are directing and influencing  group activities, articulating visions and embodying values, and achieving objectives and goals. Some styles, such as  entrepreneurial and managerial leadership styles, might be suitable for owners of disadvantaged business enterprises  since they help strengthen business and increase profit and company value.  Exhibit 1 lists different definitions of  leadership from the reviewed literature.",cwu.edu,Central Washington University,United States,47.00646895,-120.53673039883022
34,PROJECT MANAGEMENT BUILDING BLOCKS: A HANDS-ONAPPROACH TO TEACHING EARNED VALUE MANAGEMENT,@mail.mil,"Project Management, Earned Value Management, experiential learning. ","Earned Value Management (EVM) is one of the most important topics in Project Management courses. EVM is  recognized as an effective method to monitor and control expenditure, and progress in projects. However, for students  in an introductory Project Management course, EVM is one of the hardest topics to conceptualize due to the amount  of formulas and terminology. We introduced a three-day instruction period that includes a team-based hands-on lab,  an introductory lecture on EVM, and a problem -solving lab. We then conducted a retrospective review of graded  events to compare the effectiveness of this method against prior semester s that utilized traditional teaching methods  such as lectures and problem sets.  We compared the grades achieved by students on EVM -associated events during  five semesters before and three semesters after the change in teaching method. We identified a statistically significant  increase on student grades after the implementation of this improvement. The previous method’s cohort achieved a  mean grade of 82.7%, whereas the mean score for the improved method was 91.2% on a standardized end of course  assessment. Additionally, the rate of failures for the assessment decreased by 54% after implementation. Further  analysis by semester, student’s major, and grade point average reinforced these findings.  Our non-traditional approach  in the instruction of EVM proved t o be an effective method in teaching difficult concepts in Project Management.  Extensive use of this type of approach could provide an effective means to deliver Project Management education in  the future.  Keywords  Project Management, Earned Value Management, experiential learning.  Introduction  The Department of Systems Engineering (DSE) at West Point was founded in 1989.  Its mission states that “The West  Point Department of Systems Engineering educates, develops, and inspires leaders of character  who identify,  formulate, and solve complex, engineering, and socio -technical problems for our Army and Nation. We use an  interdisciplinary, integrative approach, that applies systems thinking, engineering design, data analysis, mathematical  modeling, simulation, decision science, and project management.”  DSE offers three majors (Systems Engineering,  Engineering Management, and Systems and Decision Sciences), in addition to a core engineering sequence for cadets  that are not majoring in either of the three, and a co-sponsored major (Operations Research) (West Point Department  of Systems Engineering, 2020).    The Project Management course is one of the Engineering Management program’s signature courses.  It was first  taught in 2005, and since then it has been offered to all cadets majoring in the DSE, and to cadets from other  engineering and management disciplines.  The course is taught during the fall and spring semester, and occasionally  during a compressed summer semester.  Course sections do not exceed more than 18 cadets.  Each semester has 35,  75-minute long classes.  The main graded events for the course include quizzes, a midterm examination, a three-phase,  semester-long group project, and a final exam. Furthermore , all lesson objectives are  nested within the P roject  Management Body of Knowledge  (PMBOK®), the official project management gui de published by the Project  Management Institute (PMI) (Project Management Institute, 2020).  Each cadet is expected to learn and understand  the lesson the night before the section meets and be prepared to demonstrate knowledge of the topic.  Frequently, this  involves an assigned reading and practice problems or discussion questions that cadets will prepare prior to the  instructional lesson.  This style is known as the “Thayer Method,” first introduced by Colonel Sylvanus Thayer, West  Point’s 5th Superintendent,  also known the “Father of the Military Academy”(Thayer, 1828).",mail.mil,,,46.3144754,11.0480288
35,DEVELOPMENT OF AN EARLY LIFECYCLE CONCEPTUALCONFIGURATION MANUFACTURABILITY ASSESSMENT METRICFOR TRADESPACE ANALYTICS,@cavse.msstate.edu,"Manufacturability, Tradespace, Conceptual, Design, Lifecycle, Risk, MOE, Value Curve, WBS ","Systems Engineers conduct tradespace studies for the purpose of determining those preferred product design  configurations that can provide enhanced value based on key selected parameters. The results of such analyses can  identify those unique configuration sets that potentially provide more desirable “value” outcomes as defined for the  product being evaluated. A significant limitation of most tradespace studies is the lack of consideration of  manufacturing related criteria. Such studies may unintentionally infer that all unique design configurations are  essentially equal in terms of their “manufacturability” while manufacturing related costs often vary significantly for  different configurations over a product’s lifecycle. Department of Defense (DoD) sponsored research has resulted in  the development of a methodology for calculating a manufacturability value based on subject matter expert (SME)  evaluations of conceptual design criteria that can be available for inclusion in the tradespace early in a product’s  lifecycle. This methodology includes the consideration of eight key, fundamental parameters of design, manufacturing  operations, and supply chain as they collectively relate in terms of value inherent with a specified conceptual design  configuration. The design’s perceived manufacturability is calculated as a relative index value in terms of SME  assessed risk using a typical risk matrix considering the likelihood and consequence of problematic situational  occurrences. Once the SME performed assess ments have been concluded, the compiled manufacturability database  can subsequently be utilized for enhanced tradespace analytics by program decision makers. The methodology is  explained via discussion of a simplified notional aircraft case study.  Keywords  Manufacturability, Tradespace, Conceptual, Design, Lifecycle, Risk, MOE, Value Curve, WBS  Introduction  The manufacturability research team at Mississippi State University has aggressively studied a variety of  manufacturability assessment methods over the last few years. Previous research resulted in the development of a  novel method of calculating an index value, resulting from SME analysis of a “ready for production” design (i.e. fully  described). This method (Walden et al., 2016; McCall & Fuller, 2018) was developed based on the concept that the  effective “manufacturability” of a design results from the interaction of basic, key aspects of manufacturing with the  cumulative aspects of design. Accordingly, a review matrix was developed representing seven strategic aspects of  manufacturing and three key aspects of design, providing for 21 essential assessments that when cumulatively  considered provide for the means to calculate an insightful measure of effectiveness (MOE) for the manufacturability  of the assessed design. This method is applicable for both individual component parts and assemblies of numerous  parts.",cavse.msstate.edu,,,46.3144754,11.0480288
36,IMPACT OF BROADBAND INTERNET ON THE WELL-BEING OFRURAL COUNTIES: A BENEFIT-COST ANALYSIS,@mst.edu,"rural broadband, benefit-cost analysis, infrastructure system, decision analysis ","Access to broadband internet is of paramount importance for the support of economic activity in both urban and rural  communities. Having access to high-speed internet is essential for rural communities to attract new enterprises, support  the expansion of e xisting businesses, as well as for attracting and retaining their local population. Variations of  economic activity directly impact the amount of tax collected for the support of the local community. Having a reliable  high-speed internet infrastructure con tributes to the protection and potential expansion of the tax revenue for rural  counties. To achieve the successful planning and deployment of the required rural broadband infrastructure, towns  and local governments incur different costs.  Even when there might be federal funding available to promote the  implementation of rural broadband, often, local fund matching is required. Also, there are negative externalities  associated with high-speed internet, some of these being cyber-bullying, online gaming addiction, and online gambling  addiction. A benefit-cost analysis (BCA) of a broadband project is conducted for a hypothetical county defined using  data from various governmental agencies . A proposed model considers the change in tax revenue as a means to  monetize the impact of rural broadband. The cost associated with treating problematic internet use is monetized as  mental health expenditure. A sensitivity analysis of the BCA reveals that the initial revenue of the county, as well as  the year-over-year population change, impact the net present value of the broadband infrastructure projects to a greater  extent versus other model parameters like the unemployment rate.   Keywords  rural broadband, benefit-cost analysis, infrastructure system, decision analysis  Introduction  Broadband internet is an essential tool for economic activity, and this is no different in rural communities. The Federal  Communications Commission (FCC) states that “broadband is a foundation for economic growth, job creation, global  competitiveness and a better way of life ” (FCC, 2011).  The agency defined closing the digital divide as their #1  strategic goal and estimates that up to 6 million rural businesses and homes could benefit from the Rural Digital  Opportunity Fund (RDOF) (Pai, 2018; FCC, 2020). The United States Department of Agriculture (USDA) estimates  that rural broadband and the adoption of next generation precision agriculture could lead to a potential $47-65 billion  in annual gross benefit for the USA (USDA, 2019).    The U.S. Federal Government is providing financial support to close the digital divide. Through the RDOF, the  FCC will direct up to $20.4 billion over ten years period to address the digital divide (FCC, 2020), while the USDA  ReConnect Program will be providing up to $600 million in funding to facilitate the deployment of broadband in rural  USA (USDA, 2018).   As consumers and businesses benefit from access to rural broadband, this should have repercussions at the  finances of local governm ents such as towns and counties. Still, not all impacts related to broadband are positive.  There are several  adverse effects associated with high -speed internet, some of these being cyber -bullying, online  gaming addiction, dissemination of fake news, distribution of illegal media content , and online gambling addiction.  Also, spending too much time online can lead to sedentarism, which, in turn, is associated with obesity (DiNardi,  Guldi, & Simon, 2019; Matusitz & McCormick, 2012).    The benefit-cost analysis (BCA) presented in this paper takes into account the cost of treating problematic internet  use (PIU). Considering the negative externalities of broadband internet as part of a BCA is a gap addressed in the  current research. The ob jective of the BCA is to understand the overall impact of broadband on the financial well - being of rural counties. T he next sub- sections provide additional details regarding the benefits and costs of the  technology.",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
37,A HOLISTIC APPROACH TO UNDERSTANDING ALIGNMENT WITHINORGANIZATIONS AND A CONCEPTUAL MODEL OF THE LIFECYCLE OF ECONOMIC MEASURES,@oregonstate.edu,"Performance Measurement, Metrics, Alignment, Life Cycle ","Developing proper economic measures that correctly identify key indicators among varying departments has been  shown to help improve performance and growth within an organization by providing leaders with the ability to track  performance and control outcome.  Ensuring that economic measures provide effective results is challenging due to  two main disconnects: misaligned departmental economic measure management and organizational internal strategy,  and varying economic measures between the organization and specific departments within that organization.  The  objective of this study was to perform a literature review and establish a conceptual model based on the gaps identified  in the literature review.  Through the literature review process, stages of a life cycle were identified and provided  direction for the development of a conceptual model of the life cycle of economic  measures.  The conceptual model  presented will provide direction for future research.    Keywords  Performance Measurement, Metrics, Alignment, Life Cycle  Introduction  For this study, the grouping of metrics and performance measures is termed “economic measures” in order to acknowledge  that financial and non- financial forms of measures both contain some level of economic justification for existence. The  terms ‘metrics’ and ‘performance measurement’ have been utilized somewhat interchangeably in existing literature  despite being separate terms.  The  introduction of the Balanced Scorecard provided an inclusive performance  measurement definition with all measures from financial to non-financial measures, incorporating metrics and  performance measures from varying perspectives in order to provide a broad and comprehensive look at organizational  performance (Kaplan & Norton, 1996).  Gunasekaran et al. (2015), in their review of performance measurement and  metrics literature, ignores the differentiation between these two terms due to potential duplication issues and grouping  within research, acknowledging that performance measures and metrics are separate levels but often treated as being  identical.  Clear distinction between these two terms clarifies the subject area of economic measures in order to  perform a systematic literature review that can be utilized to examine possible differences between organization levels  and departments.    A m etric  is data that provides three main characteristics: a quantitative or qualitative and verifiable performance  measure that assesses what is happening; assessment is conducted through a reference or targeted value; and there is  an association of the metric with being either above or below a specified target (Melnyk et al., 2004).  To this  definition, it can be clarified that for purposes of examining the literature in order to establish departmental standards,  metrics will be considered those utilized within an organization for the purpose of providing the current state of  systems.   A performance measure provides the broad detail of a metric that includes efficiency and effectiveness (Neely et  al., 1995).  In addition to the characteristics provided by a metric as noted above, a performance measure includes  both the internal and external purpose for targeting the specified information (Kaplan & Norton, 1995).  Performance  measures are therefore considered as a higher-level measure that can provide additional information that potentially  spans multiple departments, and historical information for long-term performance.    Justification for redefining and creating new economic measures is supported through a variety of research  designs within multiple  organizational standpoints.  Concepts within the business standpoint  (Griffis et al.,  2004),",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
38,OBSERVING ERRORS AND OMISSIONS IN FAILURECLASSIFICATION SCHEMES,@uah.edu,"systems engineering, failures, failure classification, ontologies ","With the goal of improving systematic investigation and communication of failure, many organizations use applied  ontologies such as failure classification schemes. Though the use of failure classification schemes is popular, errors  or missing information within a scheme may have significant impacts for an organization using them. Such errors or  omissions could include improper structure,  lack of disjointness, ambiguity, and lack of exhaustiveness in the  classification. In this paper, 400 failure classification schemes are analyzed. Several frequent errors and omissions are  identified in the analysis: ambiguity due to linguistic errors such as polysemy or synonymy due to a lack of definitions,  and lack of disjointness and lack of exhaustiveness from poor classification structure. The findings of this paper  provide evidence of errors and omissions in engineering failure classification schemes. A significant number of the  400 identified failure classification schemes (44.5%) do not define the factors within the scheme, which is necessary  for classification or ontologies. As failure classification schemes can be used as a tool by engineers and managers to  improve project outcomes, errors and omissions may have significant impacts for engineering management. These  findings are a warning to engineering organizations and managers to be cautious in selecting classification schemes,  and are a plea to researchers to ensure rigor in the formation of engineering failure classification schemes.  Keywords  systems engineering, failures, failure classification, ontologies  Introduction  On April 20th, 2010 the Deepwater Horizon oil spill became the largest environmental disaster in U.S. history. Over  200 million gallons of oil leaked into the Gulf of Mexico. Eleven people  died. Seventeen people were injured. The  National Commission on the BP Deepwater Horizon Oil Spill and Offshore Drilling report (Deep Water: The Gulf Oil  Disaster and the Future of Offshore Drilling. Report to the President, 2011) acknowledged communication failures  between specialized groups as one of the root causes of the disaster. It has been suggested that communication is the  single largest challenge in the design of an engineered system (Bloebaum et al., 2012) and that interdisciplinary  communication must occur for successful systems (Soban et al., 2012).   Interdisciplinary communication is often benefited by models, such as failure classification schemes. To better  communicate failures, organizations can classify failures using a failure classification scheme. A failure classification  scheme is a systematic arrangement of elements that relate to failure. A scheme can include failure categorization or  a broader evaluation of failure, such as a group or list of factors that lead to engineering project failures. Niknazar and  Bourgault describe the broader “classification scheme” as “demonstrat[ing] how entities are assigned to categories  and how categories are differentiated from one another” (Niknazar & Bourgault, 2017). A failure classification scheme  offers an organization a structured method for identifying the most common causes of failure which can be  disseminated throughout an organization to many different disciplines.   A failure classification scheme may be considered a form of applied ontology (Munn & Smith, 2013) as failure  classification schemes often 1) aim to organize failures into different categories (factors), 2) aim to define the  categories, and 3) aim to define the relationships among the categories. Ontologies are representations of  conceptualizations (Guarino et al., 2009) , meaning the ontologies are used to model shared understanding. The key  aspect of an ontology is that it improves understanding and communication through organizing elements. Therefore,",uah.edu,University of Alabama at Huntsville,United States,34.7252,-86.6405
39,EMERGING TOPICS IN ENGINEERING TECHNOLOGY ANDMANAGEMENT PROGRAMS,@cwu.edu,"Engineering Technology and Management, Emerging and Contemporary Topics , Engineering Technology, and ","Experienced professionals in engineering and construction often struggle to advance their careers without additio nal  training or adequate skills in leadership and management. The lack of such skills inspired individuals to seek a master’s  degree in business management. However, many expressed unique needs of a master’s degree in engineering  technology and management. The authors investigated emerging and contemporary topics from previous literature,  peer programs and institutions, current students and alumni, and industrial professionals in engineering (e.g.,  mechanical and electrical engineering ) and construction  (e.g., heavy civil and commercial) industries . A mixed - method approach is used to collect surveys and interviews from research participants. The quantitative data is analyzed  using descriptive statistics. The qualitative data is coded in narratives. The results of the data analysis revealed core  contemporary topics including managerial leadership, organizational performance management, project management,  and risk management. The authors further reviewed two special fields: engineering technology and construction  management. The top emerging engineering technology topics are finite element analysis, robotics, and innovation.  The top emerging construction management topics are sustainability, construction finance and business management,  innovative contracting, and virtual design and construction.  Keywords  Engineering Technology and Management, Emerging and Contemporary Topics , Engineering Technology, and  Construction Management.  Introduction  Many engineers and managers discover career opportunities or paths starting with obtaining a bachelor’s degree ,  especially in engineering technology or construction management. Individuals who held a bachelor's degree often  need additional training or education to advance their career into technical, administrative, leadership, or management  positions. Despite accumulative professional work experience, corporates seek capable individuals with an  interdisciplinary knowledge base and a combination of technical and managerial skillset. Pursuing an advanced degree  (i.e., a master’s degree) becomes a preferred option to advance careers and boost lifelong learning. The opti on is  typically a master’s degree in a specific discipline  or one of the four following options:  a Master of Business  Administration (MBA), a Master of Engineering Management (MEM), a Master of Engineering Technology (MET),  or a master’s degree in engineering technology and management (ETM). Exhibit 1 illustrates the relative relationships  of these four options. The MBA program typically offers professional or executive options. The ETM is a combination  of MEM and MET. Although MBA has become the most popular qualification in the management profession, both  MEM and MET offer more relevant or unique content in engineering  at a relatively less cost. Both often require a  bachelor’s degree in engineering or in other Science,  Technology, Engineering, and Mathematics (STEM) fields.  Either MEM or MET provides an interdisciplinary learning experience and is tailored more towards the challenges of  managing technical or technological organizations. Depending on the career path, an engineer may prefer MEM for  management positions and MBA for business strategies and operation s. To manage technical organizations, an   individual in a business major may pursue  the MEM or MET to gain knowledge in engineering and management  disciplines.   Engineering technology is often defined as the “practical applications arm” of engineering (Gani, 2020).  Engineering management is “the art and science of planning, organizing, allocating resources, and directing and",cwu.edu,Central Washington University,United States,47.00646895,-120.53673039883022
40,THE EVOLUTION OF STRATEGY ALONG THE MARKETLIFECYCLE: A DYNAMIC DESIGN APPROACH,@gmail.com,"Market lifecycle, decision-making, hi-tech decision-making, innovation, innovation management, corporate strategy. ","Strategy actively accompanies a firm along the market lifecycle fro m a wide array of early market repositioning  choices that are internally generated by the firm to externally imposed choices in market maturity, where the focus  shifts from strategy making to implementation. Along the lifecycle, firms evolve from the intuitive faith-based strategy  making leap of the entrepreneur to the collective, well-researched and much deliberated strategic choice for the mature  firm. Firms evolve from pursuing top line growth in early stages to mid-term margins squeeze, to late stage cash flow  enhancement focus.  Along the market lifecycle, the firm strategy evolves from gambling on positioning, to defense of chosen position.  The choice is between flight or fight. Young firms evade the fight as they have alternatives to choose , while asset  laden and cumbersome late stage firms challenge the competitors or collude with them for a win-win result.   This paper will trace the succession of the various generic strategic options and demonstrates how the wide funnel  of choices in early markets get whittled down to a market-imposed strategy, where the only competitive option left is  to better implement this given strategy. In essence the focus shifts from developing strategy to implementing it . We  will demonstrate how the firm goes from “thinking fast to thinking slow” and how this adaptive learning process  approximates the progression from the effectiveness to the efficiency climate zone of the market lifecycle.  We will  also demonstrate the role that market steady and transient states play in the evolution of strategy.  Keywords  Market lifecycle, decision-making, hi-tech decision-making, innovation, innovation management, corporate strategy.  Introduction: The Lifecycle Context  Most human endeavours change in time. Understanding the lifecycle evolution is an extremely useful tool to explain  how the market as a whole, or a specific company behaves. Markets are characterized by their lifecycle evolution and  so are companies, alliances of companies and their products – all have their characteristic lifecycles. In addition, one  can talk about innovation lifecycle, finance lifecycle, compensation lifecycle, etc.  There is a strategy lifecycle influenced by both external and internal contexts. It follows a  development that is  predictive in time with rational explanations at any point on this lifecycle. Once a firm has been located on its lifecycle,  it is possible to trace its history and understand its potential evolution forward.  The scope and constraints on strategic choice may be conceptualized as a funnel. This strategy funnel narrows as  the market develops. In early markets the strategy funnel is wide as small firms try to get traction and the strategy is  all about finding the most appropriate market spot that aligns the young firm’s product/service offerings with the  potential customer profiles. In a relatively new (and empty) market that is just born, there is a lot of space that can be  occupied by the firm, which can change position relatively cost-free as the firm’s products and skill sets are portable  and there is not yet a substantial asset base to drag around. And there are many choices hence the funnel is wide.",gmail.com,,,46.3144754,11.0480288
41,A CROSS-REFERENCING SYSTEM FOR CURRICULUMCOORDINATION IN MULTI-INSTITUTION ONLINE GRADUATEENGINEERING DEGREE PROGRAMS: CASE STUDY OF THEVIRGINIA ENGINEERING ONLINE PROGRAM,@virginia.edu,"Graduate engineering education, online learning, Virginia Engineering Online. ","Online graduate engineering education has been developing rapidly in the U nited States in recent years. As these  programs grow, students have options beyond taking courses from a single university. Some universities are  cooperating to provide online graduate engineering degrees. For example, the University of Virginia, Virginia  Polytechnic Institute and State University, Old Dominion University, George Mason University, and Virginia  Commonwealth University, offer the Virginia Engineering Online (VEO) program. In VEO, graduate students can  take courses from participating universities and get a Master of Engineering degree from one of them by completing  the degree requirements of that university. Negotiating the equivalence of courses between universities, and the  sequence in which courses should be taken has been a challenge for the VEO program. Courses at the five universities  may have similar content but different course numbers and syllabus descriptions. This study used the Systems  Engineering (SYS) Master of Engineering (ME) program at the University of Virginia (UVA) to create a model for  convenient cross-referencing of courses between the VEO institutions that offer this degree. The system consists of  Excel tables listing all courses at UVA SYS and their equivalent courses at the other VEO institutions. The system  has been in use at UVA for one year. It has dramatically reduced the number of repeat requests for transfer course  approvals and the time spent by the VEO academic advisor evaluating these requests. This system may be a model for  improving the course approval process across VEO and other multi-institution online degree programs.  Keywords  Graduate engineering education, online learning, Virginia Engineering Online.  Introduction  Online education enrollments have been increasing steadily in the United States of America since 2002, and the growth  continued even when overall university enrollments declined from 2012 to 2015 (Seaman, Allen, & Seaman, 2018).  In 2016, there were over 6 million students who were enrolled in at least one online course, and the proportion of  students enrolled in at least one online course was 31% of all students (Lederman, 2018). The number of online  programs is expected to keep growing for the next five ye ars (Palvia, Aeron, Gupta, Mahapatra, Parida, Rosner, &  Sindhi, 2018).  As a part of online education, the development of online graduate engineering education has included   collaborations among universities and colleges. Indeed, it  has been more than two de cades since some universities  started a consortium approach to provide online graduate engineering education across multiple institutions. For  example, in 1998, Arizona State University, the University of Arizona, and Northern Arizona University began to  provide distance education at the graduate, certificate and non-credit levels in six engineering areas, cooperating with  six high tech companies (Elliott, 1998). This happened at a time when  closed circuit TVs and videotapes were the  common tools of distance education. As online education develops, students are enjoying greater flexibility in taking  courses from multiple institutions in order to satisfy the requirements of the institution that is granting their graduate  degree.  The Virginia Engineering Online (VEO) program is a good example. It is offered collaboratively by the University  of Virginia (UVA), Virginia Polytechnic Institute and State University (VT), Old Dominion University (ODU),  George Mason University (GMU), and Virginia Commonwealth Universi ty (VCU).  In VEO, students can take",virginia.edu,"University of Virginia, Charlottesville",United States,38.0448061,-78.5166906
42,DISRUPTIVE INNOVATION ANTECEDENTS AS A SOURCE OFCOMPETITIVE ADVANTAGE,@pdx.edu,"Disruptive innovation, strategic management, competitive advantage, antecedents, strategy, organization culture, ","It has been argued that disruptive innovation occurs in a process. In order for such process to evolve and for new  markets to develop in the fall of old ones, several strategic factors are claimed to have a special impact on the process  of disruptive innovation. Factors such as organizational structure, organizational culture, and human resources are  claimed to be some of which. By conducting an in-depth literature review, this paper looks at the disruptive innovation  enablers from a new lens through a strategic perspective. It aims to mainly provide an inclusive overview and  demonstrate to which extent those key factors behind the evolution of the disruptive process could not only foster the  innovation growth, but also create a competitive advantage. Understanding the dimensions and impact of these factors  on the disruptive innovation strategy, could enable managers and practitioners to effectively manage these strategies  in order to gain innovation growth and competitiveness.  Keywords  Disruptive innovation, strategic management, competitive advantage, antecedents, strategy, organization culture,  human resources, organizational structure, dynamic capabilities.  Introduction  In today’s dynamic business ecosystem, companies need to respond rapidly and decisively to disruptive changes in  the market. However, many businesses lack the ability to respond swiftly to these competitive threats. More  importantly, the lack of action, or  postponing a response to the market change, results in companies’ loss of  their  competitive advantage. In addition, disruptive innovation can trigger established business models to be revised to  create new opportunities and advantages. Frequently, this could result in either a product platform transformation, or  operational changes that support innovation. Nonetheless, in both cases, there are key enablers to the disruptive  response. These strategic enablers, or antecedents, are claimed to impact and provoke the process of disruptive  innovation, and, as a result, enhance the competitiveness in dynamic environments. Some of the key enablers are  organizational structure; organizational culture; human resources. Moreover, existing research has explored the effect  of the enablers on the disruptive innovation process limitedly. There have not been many scholarly papers that  observed the effect of these enablers extensively. Therefore, this paper intends to fill this gap by conducting a litreture  review on these key antecedents in order to demonstrate their  collective impact on the evolution of the disruptive  process and the development of new markets as the old ones fall.  Disruptive Innovation as a Strategy   The Disruptive Innovation Theory (DIT), advanced by Christensen (Capron et al.,1998; Dan and Chieh, 2008; Ahuja  and Katila, 2001; Gulati, 1995) was constructed based on successions of previous technological innovation studies. It  has been argued that disruptive innovation theory (DIT) created a significant debate within the literatures of strategy  and entrepreneurship, an impact that has been extended to diverse fields of studies as well ( Christensen and Raynor,  2013). The history of disruptive innovation theory was originated by Joseph Schumpeter’s in his well-known Creative  Destruction view ( 1942), where he introduced the notion of revolutionizi ng economic structure through destroying  old markets and creating new ones.  However, researchers argue that the concept of disruptive innovation was not  popularized until after series of technological innovation studies conducted by Christensen et al. (1995; Christensen,  1992a, 1992b, 1993), and then crowned by his book entitled The Innovators Dilemma (Christensen, 2013), where he  classified innovations as 1) sustaining innovations (i.e., incremental innovations); and 2) disruptive innovations (i.e.,  discontinuous innovations). According to Christensen, disruptive technologies are technologies that provide different",pdx.edu,Portland State University,United States,45.51181205,-122.68493059820187
43,A MODEL-BASED SYSTEMS ENGINEERING APPROACH TOREPRESENTING SUBSTANCE USE,@mst.edu,"Substance Use, MBSE, SysML Model, Addiction Science. ","In the United States, the use and misuse of substances such as tobacco, alcohol, and illicit drugs affect the lives of  millions of people  as substance use is often an antecedent and a consequence associated with mental and physical  illness. Dealing with these challenges requires a greater understanding of substance use in society. In this paper, the  authors propose using Model-Based Systems Engineering combined with Addiction Science to represent a framework  for improved understanding of substance use as a healthcare structure. Systems modeling language is used to develop  a descriptive model of the system architecture, parameters, and interdependencies to illustrate the existing structure of  substance use. This paper focuses on the representation a nd modeling of substance use behavior, which includes the  antecedents and consequences of use. The authors have used alcohol use in college student s as a case study. The  authors propose that the use of systems engineering architecting coupled with Addiction Science will expand the  current information base and improve our understanding of substance use. By using a systems engineering approach,  the authors not only represent alcohol use in college students but also provide increased information to improve   understanding of it thereby creating a framework that can be used to facilitate decision making about substance use.    Keywords  Substance Use, MBSE, SysML Model, Addiction Science.  Introduction  The United States Healthcare System is vast, complex, and multifaceted with substance use as one of the elements.  Even though the rate of substance use in the United States is lowering substantially, the use of mind-  and behavior- altering substances continues to take a significant toll on the health of individuals, families, and communities  nationwide (""Substance Abuse,"" 2020). For this research, the term substance use includes the use of alcohol,  prescription and over the counter (OTC) medicine, heroin, cocaine, marijuana, cigarettes, and other tobacco products  (Goldberg, 2018). In 2017, 8.5 million American adults suffered from both a mental health disorder and a substance  use disorder, or co -occurring disorders (Thomas, 2020). According to the 2015 National Survey on Drug Use and  Health (NSDUH), about three-fifths of young adults aged 18 to 25 were current alcohol users in each year between  2002 and 2015 (ranging from 58.3 to 62.0 percent) (Bose, Hedden, Lipari, & Park-Lee, 2018).   When it comes to drug addiction, many people think that the people who are addicted to drugs have a lack of  moral principles or will power, and they could stop their drug use by choice. However, in reality, addiction to any  substance is considered a complex disease, and quitting (or continuing to use substances) is not a testament to one's  morality. For some people, the initial decision to take a drug is voluntary; however, repeated use can lead to changes  in brain chemistry and function. It can affect the person's self-control and interfere with their ability to resist intense  urges to take drugs. These brain changes can be persistent, which makes the addiction to many kinds of drugs a  relapsing disease. Long -term substance use causes changes in neural systems and biochemical circuits, affecting  functions such as learning, judgment, decision- making, memory, and behavior (Understanding Drug Use and  Addiction, 2018).   College is a critical transition where students must balance the demands of obtaining a degree and maintaining  high academic performance while adapting to their increasing autonomy and responsibility (Forster, Grigsby, Rogers,",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
44,CYBERSECURITY AND WATER UTILITIES: FACTORS FORINFLUENCING EFFECTIVE CYBERSECURITY IMPLEMENTATIONIN WATER SECTOR,@gmail.com,"Strategy, Digitalization.  ","Cybersecurity has been regarded as a top priority for the water industry. The water utilities industry provides critical  lifeline services for their customer s, communities, regions, neighbouring countries , industrials and agricultural  sectors. To fully support complex business processes/operations as the water industry, it  requires effective secure  Information Technology (IT) and Operational T echnology (OT). In general, the Information Technology and  Operational Technology networks are facing diverse security threats from cyber attackers. Most recent cybersecurity  attack lies in the use of traditional cyber-security approaches that requires constant update and configuration of the  systems which sometimes lead to system downtime. With the advent of digitalization in the water sector comes with  unlimited opportunities, new and profound challenges. This paper is expected to answer some research que stions  and present the current trends, challenges, risk and responsibility of cybersecurity in the water utilities  industry.  More so, t his paper highlights the best practices recommendation presented by Water Information Sharing and  Analysis Center (WaterISAC) and its goal to assist the water utilities and critical infrastructural stakeholder s in  preventing cybersecurity attacks. This paper makes a significant contribution by presenting important factors for  influencing effective cybersecurity implementation in the water utilities.  Keywords: Water Utilities, Cybersecurity, Industrial Control System, Operational Technology, Cybersecurity  Strategy, Digitalization.   Introduction  Cybersecurity is described as “the protection of internet -connected systems, including hardware, software and data,  from the cyber -attacks (Kabanda, 2018). Cybersecurity incident is described as a maliciously launched directive  from the cyber space with an intent to cause adverse consequences to a target sector/organization/industry or   specific entity (Hassanzadeh et al 2019). Cybersecurity has been regarded as a top priority for the water industry,  and the industry entities, and the stakeholders need to put considerable attention and resources to ensure  cybersecurity preparedness and feedback from both governance and technical point of view. In the United States, it   is confirmed that cyber risk is the top direct threat facing business and critical infrastructure (Germano, 2019). In  this digital age, cyber-attacks are real and could caus e significant damages to the water utilities industry. One of  ways to address cyber -attack or cyber -crime is the adoption of countermeasures which allows to minimize or  prevent the damage of this kind of attack (Panguluri et al, 2011b).               The wa ter industry operate s such a complex water infrastructure or treatment plant which involves some  levels of integration and data sharing across different water “business network s” and “control networks” (Panguluri  et al, 2011b). Cybersecurity attack s on water sector operations would have significant effects on different sectors   which could lead to a  devastating damage to public health and safety, serious threat to national security, water  operational malfunction, service outages and los s of valu able operational data. S ensors, pumps and actuators for  process control are part of cyber components of the Cyber- Physical System (CPS). Cyber -Physical Systems are  found in critical water distribution infrastructure and some of the cyber components such as [Supervisory Control  and Data Acquisition (SCADA), Human Machine Interface, Remote Terminal Unit (RTU) and Programmable Logic  Controllers (PLC)] are vulnerable targets for the cyber criminals (Adepu et al 2019).",gmail.com,,,46.3144754,11.0480288
45,APPLICATION OF DIGITAL PRODUCTS IN THE WATER ANDWASTEWATER INDUSTRY,@uj.ac.za,"Water and wastewater, digital products, digitalization, energy, maintenance, industry 4.0. ","The Water and Waste Water (WWW) industry is required to operate continuously to meet the demand for fresh water.  The plants used by the industry to treat WWW face challenges of high- energy demands and unexpected failures of  the process equipment. It becomes necessary to seriously consider the optimization of available energy and  maintenance to sustain fresh water supply and reduce operating costs and CO2 emission.   The industry is driven to employ technologies that provide decision makers, management, and operators with  real-time information to make effective decision. The real-time information needs of decision makers, management,  and operators are met using digitalization.  This study conducts a review of three large WWW utilities in Gauteng (GT) province of South Africa (SA), which  are considered representative of the industry, to evaluate energy utilization, operations and maintenance management  and energy saving practice. The study’s key findings indicate that the maximum energy demand is exceeded; energy  monitoring practice is not employed to prevent exceeding the energy consumption target; energy-intensive equipment  is used for treatment process; and predictive maintenance is not applied. The study proposes solutions to optimize the  results by integrating Energy Management System/Load Management System (EMS/LMS), Condition-Based  Maintenance (CBM) system, and Variable Speed Drive (VSD) system with the Enterprise software system using  Industry 4.0 in the WWW industry.  Keywords  Water and wastewater, digital products, digitalization, energy, maintenance, industry 4.0.  Introduction  Water demand is on the rise and the municipalities must manage the large volumes of appropriated treated effluent to  improve the availability of water. Challenges such as poor operation and maintenance procedures, high power  demands and unexpected breakdown of equipment interrupt treatment processes (Wang et al., 2014; De Bruyn, 2011).  The importance of this study is to improve and sustain water resource management with the aim  to ensure that  water losses are minimised using digitalisation concept to enhance maintenance and increase transparency of the water  treatment processes. Moreover, the study seeks to improve the efficiency of the limited energy for sustainability.  The emergent Information Communication Technology (ICT) makes it possible for maintenance to follow the  route of digitalization and Industry 4.0, with special reference to a Condition- Based Maintenance (CBM) strategy to  monitor and detected incipient failures of equipment in operation using sensors (Jantunen et al., 2017).  WWW treatment plants comprise systems to facilitate the treatment process, control and monitor equipment (e.g.  pumps, motors, aerators and so forth) used in the process. These systems include low voltage power distribution  switchgear, motor control centers and the Supervisory Control and Data Acquisition (SCADA) system.   An Enterprise software system is n ecessary to bridge the separation of the systems in the treatment plants to  enable convenience and access to monitoring data so that energy management, maintenance, and energy efficiency  can be optimized. The integration of the systems with an Enterprise software system is guided by the recommended  practice of integrating Operational Technology (OT) infrastructure (e.g. SCADA) with Information Technology (IT)  infrastructure (e.g. ERP) as illustrated in Exhibit 1. L0 to L3 include the OT infrastructure, L4 is the IT infrastructure  of Enterprise Resource Planning (ERP) and L3.5 is the enterprise architecture where IT and OT infrastructures merge  for integration and visibility of the plant’s activities.",uj.ac.za,University of Johannesburg,South Africa,-26.18493745,27.99979246435022
46,STRATEGIES TO IMPROVE CAVE AVAILABILITY AT A SOUTHAFRICAN BLOCK CAVE MINE,@uj.ac.za,"Mining, Cave Availability, Hang-ups, Production Performance, Block Caving ","Block caving is relatively a cheaper method of extracting massive orebodies from un derground massive Mines.  Caving initiation commences in the undercut level, with drilling and blasting, for the propagation of ore to create  sufficient material for the continuous loading in the production level. As the cave propagates, the more the material  flows under the influence of gravity , which is regulated through effective draw control strategies to minimise  environmental and safety risks, and to ensure high cave availability. One of the key areas in draw control is the  reduction in the number of hang-ups and idling draw-points and the minimisation of early dilution. Achieving this  successfully requires that key factors that contribute to low cave availability be identified in order to implement  strategies to minimise impact to production performance. This research study follows a quantitative methodology  and aims to outline strategies to employ in order to improve cave availability and production performance.  The  results indicate that focusing on i mproving equipment availability is inadequate  to improve production  performance, if considered as a sole determinant . The main findings also indicate two strategies that involve  factoring grade distribution and the effective prioritisation in the treatment of hang-ups in sectors that have high- grade content and that are susceptible to frequent hang-ups.   Keywords  Mining, Cave Availability, Hang-ups, Production Performance, Block Caving  Introduction  This research study relates to the work conducted at a copper block cave mine in South Africa: Palabora Mining  Company (PMC). The mine extracts and processes copper and its by-products  by means of a block cave mining  method, which is divided into four sectors as presented in Exhibit 1.   Exhibit 1. Cave sector locality plan  The initiation of caving in block cave mines is by means of developing an undercut level, which is drilled and  blasted to induce fracturing of the rock. PMC ’s strategy of the undercut initiation commenced centrally and  progressed outwardly towards the eastern and western re gions. This strategy yielded positive outcomes at the  commencement stage of the undercut due to quicker maturity of the cave centrally, which facilitated the ramping  up process (Ngidi & Pretorius, 2011). The negative effect of this initiation strategy led to challenges with frequent",uj.ac.za,University of Johannesburg,South Africa,-26.18493745,27.99979246435022
47,LEAN & INDUSTRY 4.0 INTEGRATION: PROPOSED ROADMAP TOASSESS MATURITY AND SELECT NEW TECHNOLOGIES,@pucpr.edu.br,"Lean Manufacturing, Industry 4.0, BPMN, Evaluation Roadmap. ","In the face of the global competitive scenario, the adoption of solid strategies capable of following the trends and  technological innovations has become a necessity. In this environment, practices arising from productive systems,  such as Lean Manufacturing and, more recently, from Industry 4.0 (I4.0), have provided an important contribution to  the increase in productivity and efficiency, especially when adopted simultaneously. Some authors study the  relationship between these two concepts, however, few process  models approach their joint application in the  organization in a practical way.  To fill this gap, the present study aims to remodel the maturity assessment process  and selection of new technologies by a large oil and gas company to propose a joint assessment of the domains of  I4.0 and Lean Manufacturing, and assist in prioritizing new technologies for the production environment. The  roadmap entitled Lean I4.0 Maturity & Technology Assessment (LI4MTA) was guided by the concepts of Business  Process Management (BPM), consisting of 5 macro-steps (0 to 4): Model, Evaluation, Results, Action Planning and  Maintenance (Support). The model was applied to the organization in its maturity assessment phase. As a global  result related to the I4.0 domain, the assessed entity obtained “grade 1.8 (0 -3)”, which places it at the “survivor”  level.  Keywords  Lean Manufacturing, Industry 4.0, BPMN, Evaluation Roadmap.  Introduction  Over the years, many systems, concepts and practices have been developed, replaced or improved in the quest to  optimize production processes. In this sense, two milestones stand out: the birth of the Toyota Production System or  “Lean Manufacturing” and the emergence of the Fourth Industrial Revolution based on ""Cyberphysical Systems""  and the ""Internet of Things"" (Ramos, Loures, Deschamps et al, 2019).  Although different in conceptual terms, both Lean Manufacturing and Industry 4.0 have principles that  complement each other. Roy et al.  (2015) comment that technologies in the productive environment can offer the  organization the opportunity to reach higher levels of maturity in relation to lean principles.  Despite the potential benefits, there are still challenges to be overcome for a complete consolidation of both  concepts. Regarding Lean Manufacturing, they are related to financial metrics, tool implementation, project  prioritization and time for knowledge diffusion (Kilpatrick, 2003). When dealing with Industry 4.0, the key  challenges are related to ""scientific, technological, social and mainly technical issues related to technology, security  and privacy, and standardization"" (Xu, Xu and Li, 2018).   The lack of a standard process that guides the gradual and assertive implementation of new resources  constitutes a barrier still present in organizations. In  this environment, business process management (BPM) can  support the alignment of strategies and operations for a process -based and value-oriented organization (Margherita,  2014), contributing to the standardization of maturity assessment and decision-making processes for new employees  and investments.  Recognizing such gaps, the present study aims to improve the current process of evaluating maturity and  selecting new resources and technologies in the productive environment through an orientation roadmap. A  case  study was developed in a large industrial company to illustrate and offer a significant contribution in this context.",pucpr.edu.br,,,46.3144754,11.0480288
48,INTEGRATING SUSTAINABILITYIN A QUALITY MANAGEMENT COURSE,@unl.edu,"Sustainability, Quality Management, Operations Management ","In 1992 at the United Nations Earth Summit in Rio de Janeiro, a declaration by the US engineering community and  the National Academy of Engineers endorsed a commitment to engineering sustainable development (ESD). While  great effort has been made over these years to incorporate sustainability into engineering education, progress has been  slow, and to date most engineering curricula have little to no formal sustainability component.   This paper discusses the process and some details of infusing sustainability into a Quality Management course during  Spring 2020 term. Although the main topic of the course is not sustainability, objectives were added for the  sustainability component of the course. The purpose of this undertaking is to help students understand the role  engineering profession plays in sustainability, to create awareness, interest, and commitment to ESD, and to make the  connection between lean, green Six Sigma and ESD.   Sustainability resources include instructor lectures, journal articles, blogs and podcasts, TED Talks, and open-source  videos. Course activities that engage students in the topic are several group discussions, a quiz, instructional videos,  and an opportunity to develop a systems diagram and later work within a group to consolidate and expand the diagram  based on contributions from the team. Additionally, the course project gives student an option to adopt a topic related  to sustainability.  To inform the decision of whether the sustainability infusion in this course should be continued in future semesters,  informal feedback surveys from students was collected to determine efficacy, interest, and satisfaction level.  Keywords  Sustainability, Quality Management, Operations Management  Introduction  In the past three decades sustainable development ha s gained popularity as much attention has been given to the  environment and the role that humans play in its destruction and preservation.  To live sustainably, we must use our  planet and its resources responsibly to meet our needs, but leave it such that future generations would also meet their  needs. A 2002 declaration by the US engineering community, addressed to the World Summit on Sustainable  Development, brought to the forefront the role that engineering plays in achieving sustainability  (WSSD). In 2005,  The United Nations proclaimed that year to be the start of the Decade of Education for Sustainable Development,  EfSD (Quist, et al, 2006). The trend to promote sustainability gained momentum in 2006 with the release of Al Gore’s  documentary film, An Inconvenient Truth (Gore 2006).  While E fSD is an important component of instruction in all disciplines, integrating sustainability in e ngineering  education is imperative because engineers design, develop, and implement products and technologies and can directly  impact sustainability of such products and technologies (Desha, et al, 2011).  Engineering education and curriculum in most industrialized nations incorporate, to some degree, non-technical and  professional skills such as social and management skills as well as ethics and social aspects of technology (De Graaff  and Ravestejin, 2001). However, instruction at most engineering schools is not keeping pace, beyond the technical  content, to embrace and incorporate engineering sustainable design in the curriculum.  Sivapalan, et al (2016) report on the commitment made by the higher education sector at the 2012 Rio World Summit.  The Higher Education Sustainability Initiative plan include d five areas  among which are encouraging research,  modeling sustainability through campus operations, supporting and building community at the local level, and sharing  knowledge and learning through international collaborations and frameworks. But, on top of the list wa s higher  education commitment to teach sustainable development  concepts in the core curriculum and to produce graduates  who are literate in sustainability an able to design and develop sustainably.",unl.edu,"University of Nebraska, Lincoln",United States,40.8205941,-96.70260581368638
49,THE USE OF PROJECT MANAGEMENT TECHNIQUES IN SOUTHAFRICAN START-UP BUSINESSES,@uj.ac.za,"Start-up businesses, project management techniques, small to medium enterprises, business success ","Small to medium enterprises are a vital part of the economy yet have a high rate of failure. Whilst research has explored  the causes of start -up busines s failure, little research has been conducted to determine the potential project  management techniques that may increase the rate of success of start-up businesses. This study is an initial exploration  of the possibility to employ project management techni ques towards start -up business success. A comparison of  projects and start-up businesses is conducted to draw an analogy in which start -up businesses could be treated as a  special type of project; and that project management techniques could impact the suc cess of start -up businesses.   Literature indicates that the analogy is viable.  Consequently, a survey was conducted in South Africa to determine  the possibility of employing project management techniques for business success. The results indicate that project  management techniques positively influence the chance of start-up business success. On this basis it is recommended  that certain project management techniques, in particular checklists, communication plans and contingency plans, be  employed by entrepreneurs of small to medium enterprises towards enhanced business success. It is recommended  that further research into the effectiveness of the techniques be conducted.  Keywords  Start-up businesses, project management techniques, small to medium enterprises, business success  Introduction  Small and medium enterprises  Small and medium enterprises (SMEs) are a vital part of the economy of every country and the global economy as a  whole, as well as being a major driving force of economic growth (Soininen, Puumalainen and Sjögrén, 2012;  Nikolić, Jovanović, Mihajlović and Schulte, 2018). Small and medium enterprises have been found to constitute an  increasing portion of most countries’ economies and have the ability to compete on a global level (Muhammad,  Char, Yasoa' and Hassan, 2010; Cressy and Bonnet, 2018). The increase in SMEs can be attributed to the expansion,  internationalisation and reduced costs of international logistical companies, as well as the reduced costs of sourcing  basic goods through these international corporations (Muhammad, et al., 2010; Nikolić, et al., 2018).  In 2016, it was estimated that small and medium enterprises (SMEs) accounted for 43.5% of global employment  rates (Aysan, Disli, Ng and Ozturk, 2016). Small and medium enterprises, in many countries, also represent the largest  portion of their country’s job creation (Aysan, et al., 2016). Small and medium enterprises usually begin as start -up  businesses (SUBs). Globally, approximately 50% of start-up businesses (SUBs) survive longer than 4 years (Lussier  and Halabi, 2010). The failure of such a large portion of SUBs has a major impact on the stability of an economy and  affects multiple parties, including the employer, employees, the local community and the investors (Lussier and  Halabi, 2010).  The rate of new SUBs is higher in developing countries and the resources for these SUBs are abundant, however,  there is also a much higher failure rate for SUBs in developing countries (Naudé, 2010; Lussier and Halabi, 2010). In  a developing country, such as South Africa, other factors should be explored and considered. If the effect of the factors  affecting the failure rate of start-up businesses in South Africa can be decreased, it could lead to the survival of more  start-up businesses. These SUBs might then become stable SMEs, contributing and strengthening the economy.  Small and medium enterprises represent roughly 55% of all jobs in South Africa (Van Scheers, 2011). Start- up  SMEs contribute to roughly 75% of new job creation in South Africa, and although this may seem high, it lags behind  the 80% achieved by Asian companies and other African countries in which small and medium enterprises comprise  a much higher percentage of employment and economic growth (Smit and Watkins, 2012). The rate that new SMEs   in South Africa are being started is also one of the lowest in the world and still declining (Fatoki, 2014). The low rate",uj.ac.za,University of Johannesburg,South Africa,-26.18493745,27.99979246435022
50,A MODEL-BASED SYSTEMS ENGINEERING APPROACH TO TRADESPACE EXPLORATION OF VIRTUAL REALITY TRAINING SYSTEMS,@mst.edu,"Trade Space Exploration, Virtual Reality, Model Based Systems Engineering, SysML, Systems architecture. ","The requirements for performance of VR systems vary for different application domains. Likewise, there are various  technological options to achieve each of the requirements. Therefore, an early evaluation to determine optimal  configuration of VR systems based on domain specific requirements for simulations and interactions is of imme nse  value. In this paper, the authors have demonstrated the use of systems engineering, particularly Model-Based Systems  Engineering (MBSE), to describe a complex decision space in Virtual Reality development. Further, the authors then  employed a descriptive decision analysis framework to identify suitable configurations to enable VR development.  MBSE is used to develo p descriptive models to specify needs of the application domain, requirements of the VR  system and to define the system architecture – functional and physical. This will serve to demonstrate how MBSE can  be used to generate information to improve decision making for VR designers as well as engineering managers and  decision makers. VR application in emergency response training is the exemplary case considered in this paper.  Keywords  Trade Space Exploration, Virtual Reality, Model Based Systems Engineering, SysML, Systems architecture.  Introduction  Complex systems are made up of numerous components interacting together to create a behavior that is “not  predictable from isolated components but occurs through the interaction of multiple components” (Bloebaum,  Collopy, & Hazelrigg, 2012; Hmelo -Silver & Azevedo, 2006). Other features of complex systems are the  “heterogeneity of their components and their multiple levels of organization”. These features distinguish complex  systems from complicated systems (Johnson, 2006). It can be argued that Virtual Reality (VR) does not qualify to be  categorized as a complex system, that the behavior of the system can be modeled and accurately predicted before it is  deployed. However, considering software development, human interactions and the potential for collaborative  applications within the system boundary lead to complexities. Additionally, given that VR d oes not provide purpose  without context, the interfaces of the integration of VR with other domains like education, medicine, among others  are plagued with inherent complexities. Researchers have explored methods of assessing VR configurations as a means  to evaluate the effectiveness of their applications in training (Schmidt et al., 2019). Most studies have focused on the  operational phase. It is therefore imperative to have a tool with which developers can assess the various possible  configurations of their VR system in the conceptual phase.  Trade Space Exploration (TSE) is a means that is usable to achieve this goal. TSE involves the examination and  evaluation of candidate system configurations for achieving the specified system objectives. Increase in co mplexity  and number of components present in the system can result in a substantial increase in the design space.  TSE helps  system designers to learn about possible design configurations of a system, explore how design choices affect the  whole system and sometimes lead to the discovery of important system attributes that were not previously considered  in the model formulation (Collopy, 2018).  In this paper, Model -Based Systems Engineering (MBSE) is used to model the system. System Modeling  Language (SysML) is used to develop a descriptive model to specify needs of the application domain, requirements  of the VR system and to define the system architecture. The system architecture represents the system and the  relationships between the elements and the system functions. In addition, it highlights the relationships between these  elements and the system environment (Crawley, Cameron, & Selva, 2015). The aim of this paper is to use MBSE to",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
51,"A FRAMEWORK FOR CHARACTERIZING MULTILEVEL WATERGOVERNANCE: A CASE STUDY OF BALTIMORE, MARYLAND",@virginia.edu,"Multilevel Water Governance, Water Quality, Water Supply, Water Resource Management   ","Water is essential for any type of human development. It is a government’s duty, therefore, to prioritize consistent,  safe, and well-managed water resources for its constituents. The last century has seen significant water infrastructure  improvements and novel designs. In the United States there are federal standards for water management which are  upheld by the states and executed locally. The process by which decisions are made cascades in its effects. Water does  not recognize physical boundaries and is often managed intrastate or intercounty. Like many of our vital resources,  mismanagement leads to depletion and quality control failures. As climate change wreaks havoc on our environment,  droughts and floods have become more frequent occurrences, straining the water supply system. The first step in  improving water supply management is understanding the organizations involved in the supply chain, from the  watershed to the treatment plant, consumer , wastewater treatment plant and ultimately , the receiving water bodies .  Characterization of the m ultiple levels of governance involved in water supply management can help to identify   critical deficiencies in policies, programs, and processes that contribute to mismanagement . This paper presents a   theoretical framework for characterizing a water supply system, identifying the government agencies and their roles,  and modeling their interrelationships across local, state, and federal levels. The framework is applied to a case study  of the City of Bal timore, Maryland, as it confronts the challenge of providing safe and reliable water supply service  in the face of underlying economic and racial inequalities and regulatory compliance issues.  Keywords  Multilevel Water Governance, Water Quality, Water Supply, Water Resource Management    Introduction  Water Issues  Anthropogenic climate change has accelerated the earth’s natural carbon cycle (NASA, 2020). The excessive  greenhouse gas emissions have melted the ice caps, displacing coastlines around the wor ld (Church, 2008). The  Eastern United States are greatly affected by this change. In this region, storm events are increasing in frequency and  severity. As the sea level is rising about an eighth of an inch each year, the risk to coastal cities from saltwater intrusion,  storm surge, and pluvial flooding is exacer bated. Contaminated water sources, decreased water quality, and  mismanaged resources all threaten water security.   A recent, infamous case occurred in Flint, Michigan, starting in 2014. The po or governance on the part of  decision-makers resulted in the poisoning of an entire city. The Flint River was a cheaper option for water withdrawals,  but improper testing, treatment, and poor oversight ended in a horrible injustice. Flags were raised by the citizens  about water that was unnatural in color, odor and taste. Residents were ignored while continuing to get physically ill  (Denchak, 2018). The decision-making was careless and negligent, at best, but the underlying systemic racism in Flint  housing, employment, and income created an inequitable response (MCRC, 2017 ). It is critical to understand the  governing network that creates the human water cycle, but even more important to apply responses and decision - making that will remedy the inequities within a specific community or across communities.   This research characterizes multilevel water governance (MWG) in the United States by defining the problem  and its components, employing literature derived models to assess management systems and adaptive wat er  governance. The novel contribution is the synthesis of these resources alongside unilateral duty assessments and the",virginia.edu,"University of Virginia, Charlottesville",United States,38.0448061,-78.5166906
52,A MODEL FOR THE GRADUATION REQUIREMENT FOR THEENGINEERING MANAGEMENT MASTER’S DEGREE AT THEUNIVERSITY OF IDAHO,@uidaho.edu,"engineering management master’s degree, non-thesis, Certified Professional in Engineering Management. ","The Engineering Management Master’s Degree program at the University of Idaho has been in existence for nearly  30 years.  In 2016 we became certified by the American Society of Engineering Management, (ASEM) only the fourth  program to be currently certified. Feedback from that certification process recommended that we pursue a more formal  means of program completion that also satisfies the university ’s non-thesis requirement, such as an examination or  final defense.  At the 2017 Annual ASEM conference a model was presented based upon ASEM’s process to become  a Certified Professional in Engineering Manage ment (CPEM) along with the advantages and disadvantages of this  approach.  Now, in our third year of implementation and with survey results, we have enough experience and  information to provide more summary results on the impact of this approach.  It is anticipated that information on  ASEM’s CPEM process could be applicable to other graduate programs seeking to formalize their graduation  requirements.  Keywords  engineering management master’s degree, non-thesis, Certified Professional in Engineering Management.  Introduction  The Engineering Management (EM) Master’s Degree program at the University of Idaho (UI) is a non-thesis masters  that has been in existence for nearly 30 years.  In 2016 we became certified by the American Society of Engineering  Management (ASEM), only the fourth program to be certified by ASEM.   Feedback from that certification process  recommended that we pursue a more formal means of program completion.  The College of Graduate Studies at the  UI has a non-thesis requirement such as a project, paper, portfolio, recital, or examination, as determined by the  program (COGS, 2020).  We were in need to satisfy both requirements.    Prior to ASEM certification, the non-thesis requirement for our Master’s in Engineering Management gave  the student two options.  O ption 1 was to conduct a three -credit project and then give an oral presentation on the  project along with a  final oral examination on the project and course work.  Option 2 was that a student takes  an  additional elective in leu of the project and pass a written comprehensive exam on their course work.  The exam was  a mix of short answer, essay, and multiple -choice questions focused on the required courses in the degree program   with additional questions based on the electives each student used in their program of study . Most students chose  Option 2 but there were a few problems with this approach  for both the student and faculty.  Each exam was unique  and required substantial work to build to ensure academic integrity. Likewise, the exams were time consuming to  grade.  It was difficult to provide a consistent experience for each student because of the uniqueness of the exams and  students did not like the inconsistency.      Following advice from the ASEM program certification process, in 2017 we transitioned into the current  practice for meeting the university’s policy for a non-thesis requirement. We still give the students two options.  Option  1 is the past practice of a project and final oral examination. Option 2 however, was changed to accommodate ASEM’s  program to become a Certified  Professional in Engineering Manage ment (CPEM) which provides an international  standard of engineering management comp etency (CPEM, 2020).  To pursue Option 2 students must complete an  additional 1-credit course called EM 596 Capstone Integration.  The use of this course facilitates administration of the  process rather than trying to keep track of the process on an ad hoc basis and records the accomplishment on the  student’s transcript. The requirement to pass EM 596 is that students must pass the CPEM examination administered  by ASEM and become a CPEM.  If the student fails the CPEM exam twice, they must pass both a written and oral",uidaho.edu,University of Idaho,United States,46.72379775,-117.02043898436474
53,MINIMIZING FOOD WASTE IN THE PERISHABLE FOODDISTRIBUTION NETWORK—A REVIEW OF OPTIMIZATIONMODELS,@mavs.uta.edu,"Distribution network, perishable foods, refrigerated vehicles, shelf life, temperature, food waste.  ","Seventy-two billion pounds of food goes to waste each year. Waste can be defined as food that is still edible and goes  unused before the end of the shelf life. Food banks have tried to preserve food shelf life as long as possible by adopting  refrigerated vehicles to transport food to their destinations. Refrigerated vehicles are meant to preserve food shelf life  by maintaining the temperature at a certain level through the distribution process, but the product can lose part of their  shelf life when the temperature goes high inside the container. Mainly the ambient hot air comes inside the container  when its door is open for  loading and unloading  activities at the distribution center or demand delivery drop  off locations respectively. Recently, studies have been conducted on how to best optimize this transportation network  and reduce food waste. The objective of this literature review is to understand the current distribution network models  and their impact on perishable foods and to optimize the modern distribution  network to maintain the shelf life of  perishable foods. Through a literature review, we will be comparing models of optimized distribution networks to find  the best way to strengthen the distribution network and enhance the maximum shelf life of perishable foods during  transportation.  Keywords   Distribution network, perishable foods, refrigerated vehicles, shelf life, temperature, food waste.   Introduction   One of the main concerns of minimizing perishable food waste is how this impacts the ability to feed people, followed  closely by the economic impact  (Amin Gharehyakheh & Sadeghiamirshahidi, 2018) . Finding a sufficient  optimization model will help us out in the long run by reducing food waste. About  forty-one million people are food  insecure in the United States of America (Feeding America, 2017). That is about one-sixth of the American population  who rely on food banks as a food source. Last  year, a food bank was interviewed,  and they reported that they often  have a surplus of food  (Amin Gharehyakheh, Krejci, Cantu, & Rogers, 2019) . Some of this food is  perishable and expires due to the First- In-First-Out invent ory management system. This allows foods that  were received first to be at the front for customers, regardless of the shelf life. This system  is a reason for how much  food goes to waste each year but, there are other factors besides the time when measuring the shelf life of a product.   Collaboration between food supply chain actors from farmers, food manufacturers, warehouses, and distributors  to the retailers and end -users makes a wide range  of foods available  for the end -users even far from the origin of  production. However,  the management of food supply chains  is very challenging due to the unique perishability  features of food products,  safety regulations, and the end -users’ dynamic and high expectations  (Zhu et al.,  2018). Optimization models are widely applied to tackle these challenges and to provide the most efficient solutions  focusing on unique characteristics of food distribution networks.  Hence, it is vital to have a review of the  current literature and understand how optimization models can help to reduce food waste in the distribution network.   The way  these perishable foods  are transported could impact shelf life as well. Many of these foods are  transported in refrigeration trucks  to help decrease bacteria contamination  (Amin Gharehyakheh, Cantu , Krejci, &  Rogers, 2018). Temperature is a variable that directly impacts the shelf life of foods. When the temperature is higher  than acceptable, bacteria can grow  on the product and shorten the shelf life.  The opening and closing of  the doors on refrigeration vehicles is a cause of temperature fluctuation, but there are other possible factors to consider  such as how long the product takes to load or unload at a location. When there is a high level of bacteria that causes a  change in color, smell, or shape, the product is considered food waste or spoiled. A lower level of bacteria that grows  on perishable foods does not necessarily mean the food is spoiled. There are models that can be used to measure these  bacteria levels and help optimize the distribution network by focusing on the quality of the products. This paper will  talk about how temperature, bacterial growth, and quality cost models can help improve the distribution network by",mavs.uta.edu,,,46.3144754,11.0480288
54,M,Missing,,,Missing,,,46.3144754,11.0480288
55,TOWARD MODEL-BASED ENGINEERING MANAGEMENT (MBEM),@adelaide.edu.au,,"The field of engineering is quickly evolving as a result of rapid technological advancements and globalization.  Traditional document-based methods in engineering are no longer effective since they lack the flexibility needed to  adapt to the constantly changing environment. New innovative approaches are required to cope with this sweeping  transformation, and model-based engineering is one such approach. According to Systems Engineering Body of  Knowledge (SEBoK), “A model is a simplified representation of a system at some particular point in time or space  intended to promote understanding of the real system. As an abstraction of the system, it offers insight  about one or  more of the system’s aspects, such as its function, structure, properties, performance, behavior, or cost. Further,  modeling can occur at different levels: at the component level, subsystem, system, and system of systems level and  throughout the life-cycle of the system.” One of the engineering disciplines that has already benefited from adapting  model-based approach is systems engineering, and Model -Based Systems Engineering (MBSE) has emerged as a  promising method for engineering of complex systems. The authors propose to expand MBSE to the field of  engineering management and offer a Model -Based Engineering Management (MBEM) framework as a way to help  organizations to effectively adapt model-based thinking in managing engineering systems, projects, and programs. In  turn, this will allow organizations a more efficient transition from document -based to model -based engineering  management. A case study of the Adelaide Desalination Plant is presented to illustrate an application of the proposed  method.    Keywords  model-based systems engineering, engineering management,  model-based engineering management, complex  systems.  Introduction  Many fields are starting to embrace model-based thinking in their attempts to keep up with the dynamic nature of the  business and engineering environments. Engineering projects, like other projects, are becoming more complex and  more integrated, and significantly increased computer automation is occurring. Because of these rapid and dramatic  developments, the complexity of engineering management (EM) is increasing. For this reason, new approaches are  needed in order to keep up with the ever -changing nature of the engineering environment.  A m odel-based EM  approach is an example for such development into the future. This paper outlines the benefits of integrating model - based thinking into key EM areas identified in the EM Handbook.  The benefits include promoting a scientific habit  of mind, illuminating core uncertainties, and disciplining the policy dialogue,  among others. Given the rapidly  changing environment in which engineering managers must operate, such new approaches as model  based thinking  will be imperative to being successful in managing emerging engineering projects and systems.    Model-based Thinking  Modeling is a common human activity. When people need to solve a problem, they do not advance immediately to  the solution. Instead, what they do is consider the problem as it exists in the real world and  then recreate that real  problem in a cognitive view as a model in their mind. This model is based on their understanding of the real -world  problem space, highlighting the imp ortant components . Once a model has been created, then that model can be  improved and optimized. The optimized model will then lead to a solution which can be deployed in the real world.  Mapping a problem in the real world into the world of models is a process of abstraction,  and then the solution is  mapped back into the real system. Modeling allows for optimization prior to implementing a solution when it is not  possible to prototype or experiment in the real world. Exhibit 1 portrays this process in a visual fashion.",adelaide.edu.au,University of Adelaide,Australia,-34.9189226,138.60423667410745
56,OPTIMIZING BUILDING RETROFITTING SCHEMES THROUGH BIM-BASED VALUE ENGINEERING ANALYSIS FOR SUSTAINABILITY,@esf.edu,"Building information modeling, Value engineering, Building retrofitting, Sustainable buildings, Life cycle cost. ","Value engineering analysis has been increasingly attracting attentions from stakeholders in sustainable building  retrofitting activities to identify optimal schemes that can maximize the associated values and functions while reducing  costs. Though useful in improving the effectiveness of retrofitting investments, value engineering process often needs  significant amount of quantitative information for a reliable analysis. Traditional manual processes of data collection  based on 2D drawings are often time-consuming or malfunctional. On the other hand, as a widely adopted intelligent  technology, building information modeling (BIM) may represent a significant potential to assist in value engineering  analysis during retrofitting projects by overcoming information collection inconvenience. This paper proposed a novel  approach to more conveniently implement building retrofitting value engineering analysis based on 3D BIM platform.  Firstly, building retrofitting objectives are identified by investigating the target building. Secondly, 3D BIM model is  developed based on the existing 2D drawings complemented by historical maintenance records. Thirdly, possible  retrofitting option schemes are designed with the corresponding parametric information of functions and costs  obtained through BIM-enabled automation process. Finally, the associated scheme values are quantified using the  obtained function and life cycle costing information to carry out a quantitative value engineering analysis to locate  optimal retrofitting option. The proposed approach is applied to a campus building to demonstrate  its utility. The  potential implications to engineering managers are investigated and discussed.  Keywords  Building information modeling, Value engineering, Building retrofitting, Sustainable buildings, Life cycle cost.  Introduction  The needs  for sustainable building s and infrastructure systems with reduced energy consumption and lower   environmental impacts have been increasing. Rising energy costs and growing environmental concerns are the drivers  facilitating this trend. According to U.S. EIA (2020), the electricity price in residential sector appeared to have been  growing steadily since 2014 with an average percentage increase of more than 1.2%. On the other hand, while  environmental benefits and human health advantages  of sustainable building s over traditional buildings  being  recognized, sustainable buildings could be economically feasible as an option to offset the energy cost increase.  For  example, based on Azhar, Carlton, Olsen, & Ahmad (2011), with a small amount of upfront cost burden increase (e.g.  2%) to initiate a greening strategy on building design,  a significant cost saving (e.g. as much as  20% of the whole  project expense) could be achieved through building life cycle . It would result in a large return on investment ratio  since the savings could be as high as ten times of initial outlay.    Over new construction, greening existing buildings through retrofitting activities could significantly contribute to  building sustainability in the U .S. due to its significant share of investments on construction market (Wang, 2013).  Many building retrofitting projects have been designed and implemented  for energy efficiency and building  sustainability at either single or multi family or communal levels. As a frequently used method for product and process  improvement in construction industry, value engineering (VE) can optimize designing schemes and construction  processes in building retrofitting programs for sustainability  - by systematically examining functions of building  systems that will provide improved services at reduced cost. Several studies adopted value engineering analysis for  effective sustainable building desig n, construction and development. For example, Amri  & Marey-Pérez (2020)",esf.edu,State University of New York College of Environmental Science and Forestry,United States,43.0347222,-76.1355556
57,ELECTRIC OR INTERNAL COMBUSTION ENGINE? ANALYSIS OFELECTRIC VEHICLE DRIVERS’ PERCEPTIONS AND EXPERIENCES,@uscupstate.edu,"Electric vehicles, electric vehicle owners, adoption ","Due to the popularity of electric vehicles, almost all major automakers are producing or have plans to produce  these vehicles. As a result, electric vehicles sales have increased significantly during the last decade. However,  there is currently no widespread adoption of these vehicles. This paper analyzes survey results from electric vehicle  drivers to determine their motivation for purchasing electric vehicles as well as their perspectives on the benefits  and limitations of electric vehicles compared to conventional vehicles. In addition, this research analyzes electric  vehicle owners’ preferences as well as their choice for their next vehicle. Result of this research provide valuable  information to engineering mangers on the benefits and barriers to the widespread deployment of electric vehicles  Keywords  Electric vehicles, electric vehicle owners, adoption  Introduction  Electric vehicles (EVs) offer many benefits including increased energy efficiency, reduced emissions of pollutants  and greenhouse gasses and enabling greater diversity of fuel choices for transportation. According to Energy  Information Administration (2019), the United States transportation sector consumes more petroleum than any  other sector with its share increasing from 50% in 1950 to & 70% in 2018. Therefore, a transition to EVs can  drastically reduce petroleum consumption. Currently, EVs represent a very small fraction of vehicles in use. Due to  their benefits many governments are eager to see EV adoption increase significantly. To incentivize EV adoption, a  notable tool is the use of government incentives aimed at making EVS more competitive with conventional vehicle  (Zhang, Bai et al.  2018). Some barriers to adoption of EVs include higher cost, limited driving range for battery  electric vehicles and unwillingness to try new technology.  Several studies have been conducted on EV adoption. Nazari, Rahimi et al. (2019) modeled adoption  behavior of battery electric vehicles (BEVs) in the United States using a bivariate model where a binary probit  model of BEV adoption is estimated with a log-linear regression of willingness to pay. The authors found that BEV  owners are more likely to add another BEV to their household compared to other types of vehicles. Canepa,  Hardman et al. (2019) focused on understanding whether light -duty plug-in hybrid electric vehicles (PHEVs) are  being adopted in disadvantaged communities (DACs) in California. The authors found that adoption of both new  and pre-owned PEVs in DACs occurred at very low rates. Key barriers identified in the study include prohibitive  prices of PEVs, lack of knowledge about PEV incentives, lack of ease of accessing PEV incentives and lack of  access to charging infrastructure.  Egbue, Long et al. (2017) studied a combination of factors that influence  customer intent to adopt EVs to address the question “Who are potential buyers of electric vehicles?”. Dua, White  et al. (2019) developed a data mining  approach and apply it to a nationally representative survey of new car buyers  to identify potential BEV buyers using revealed preference data. They found that if BEVs with added features  are  sold at a lower price, they have the potential to reach an annual U.S. market share of 2.4%.  This study aims to better understand EV owners’ experiences with their vehicle and their views about the  benefits and costs of EVs. The study also analyzes factors that motivate EV owners to adopt an EV and determines  if they are willing to purchase another EV as their next vehicle. Furthermore, this research compares EV owners’  attitudes and perceptions with those of individuals that do not own EVs to gain bet ter insight into issues affecting  adoption. In this study, an EV is defined as a vehicle that is partially or fully powered by electricity. Three types of  EVs are  considered in this research. A hybrid electric vehicle (HEV) adds a battery and electric motor to a  vehicle that uses an internal combustion (IC) engine which is usually powered by gasoline or diesel. A HEV cannot  be charged using electricity. Unlike the HEV, a PHEV has both an electric motor and an ICE but can also be  charged by electricity . A battery electric vehicle (BEV) has an electric motor and traction battery to power the",uscupstate.edu,,,46.3144754,11.0480288
58,ENGINEERING MANAGEMENT PRINCIPLES FOR IMPROVINGQUALITY AND EFFICIENCY IN PATIENT CENTRED CARE,@adelaide.edu.au,"Patient centred care, Healthcare 4.0, engineering management, patient empowerment ","Engineering Management (EM) combines management and engineering practices . The topics addressed within  EM include  professional ethics and legal issues, management and leadership theory, intellectual property,  operations research, modelling and simulation, decision analysis, engineering economics, risk management,  quality management, innovation and entrepreneurship, among others. Each of these topics can be brought to bear  on present problems arising in healthcare, particularly concerning the implementation of patient centred care   (PCC). The optimal delivery of healthcare is an area of contention in the literature. Various problems include  perceptions of depersonalisation and a lack of continuity in care. Patient centredness has been the focus of much  literature and encourages the empowerment of patients and replacement of the red uctionist view of healthcare  delivery, when components are delivered in a silo fashion, with a more holistic and integrated approach. While  definitions and interpretations of  PCC vary, there is widespread support for the adoption of this patient centred  approach. Literature suggests that both quality and cost efficiency will be improved through care coordination and  a focus on the patient.  Not only is much of modern healthcare delivered in a siloed fashion, PCC is still not clearly  developed in its applica tion. The benefits of adapting EM principles with the discipline’s scientific process - oriented approach and the implementation of new technology, together with the focus on management, is a  promising approach to today’s challenges experienced in the delive ry of PCC. The arrival of Healthcare 4.0  powered by the technologies seen with the arrival of Industry 4.0 make this integration even more relevant  and  timely.   Keywords   Patient centred care, Healthcare 4.0, engineering management, patient empowerment  Introduction  Over the past few decades  there have been i mprovements in healthcare systems (Tortorella et al., 2020) .   Nevertheless, as life expectancy goes up and the population becomes older, costs associated with healthcare are  increasing and there is a constant search for new ways to deliver care more efficiently. Expectations about  healthcare treatment place pressure on delivery and therefore there is a search for new and more advanced  solutions using science and technology. At the same time, the healthcare industry is different from many other  industries in its focus upon human interactions and this calls for a highly personalised experience for patients.  There have been many attempts to replace older management approaches of the top -down variety with more  patient focused and empowering management styles; one such approach is patient centred care (PCC). Approaches  to providing PCC are many and diverse. A huge ri se in technological solutions to health issues and disease can  easily lead to what is perceived as unfeeling treatment of patients. Such depersonalisation as a result of  technological advances has spawned a move to deliver care that is more focused upon th e patient (McKendry &  Green, 2018). As these authors comment: “ Personalisation can be difficult where scarce resources have to cope  with a growing elderly population with long-term health problems.” (McKendry & Green, 2018, p. 552).       But what is personalised care? According to Savard (2013, p. 2): “Personalised medicine can be defined as the  use of genetic knowledge about a patient to predi ct disease development, to influence decisions about lifestyle  choice, and/or to tailor medical treatment plans/options for that same individual (NHMRC 2011)”. Other writers  prioritise different implications of patient personalisation. For example, Qi et al. (2017) discuss the role of the  Internet in enabling an improved personalised healthcare system through Internet enabled devices.  Bantel,  Laycock, Ward, Halmshaw & Nagy (2013) present their thoughts on a personalised approach to pain management  within intensive care, through appreciating individual differences in terms of tolerance of pain and acceptance of  various treatment options. Many authors have discussed the need for personalisation within the treatment of cancer  (Davies & Batehup, 2011). Discussion of personalisation within public healthcare systems is also present in the  literature (Ricciardi & Boccia, 2017) . Providing personalised care, however, may be more time -consuming and",adelaide.edu.au,University of Adelaide,Australia,-34.9189226,138.60423667410745
59,IMPROVING FISCAL STEWARDSHIP WITHIN THE GOVERNMENTSECTOR: DEVELOPMENT & IMPLEMENTATION OF THEEXECUTION RATIO,@westpoint.edu,"Execution ratio, resource management, fiscal stewardship, Lean Six Sigma, process improvement ","Active financial stewardship ensures that the Department of Defense maximizes  its purchasing power. On average,  the Army de -obligates between 3 -5% of its annual Operating & Maintenance budget which equates to billions of  dollars of lost purchasing power (McConville & McCarthy, 2018). Following the Lean Six Sigma methodology, this  process improvement effort examined the factors that led to unliquidated obligations and used the findings to develop  an improved process through which organizations  can manage contracts during the period of availability for new  obligations. This project documents the development and implementation of a new metric, the execution ratio,  intended to help organizations quickly  assess contract performance. The new process for contract management was  piloted by the Combined Security Transition Comm and-Afghanistan between February and June 2020. During the  pilot period, the organization was able to de-obligate 5% of funds on existing contracts, preventing the loss of $19.2  million in purchasing power. The preliminary findings suggest that the inclusion of a key performance indicator for  contract execution can result in an improvement in the final disbursement rate.   Keywords  Execution ratio, resource management, fiscal stewardship, Lean Six Sigma, process improvement  Introduction  Active financial stewardship ensures that the Department of Defense maximizes its purchasing power. On average,  the Army de -obligates between 3 -5% of its annual Operating & Maintenance budget which equates to billions of  dollars of lost purchasing power (McConville & McCarthy, 2018). Following the Lean Six Sigma methodology, this  process improvement effort focused on developing a holistic understanding of the factors that lead to unliquidated  obligations within the Combined Security Transition Command-Afghanistan (CSTC-A) and the creation of improved  management process to improve the overall disbursement rate for the Afghan Security Forces Fund  (ASFF), a two- year congressional appropriation. This paper provides a brief overview of the military organization, reviews the Lean  Six Sigma methodology, documents the unique fiscal challenges associated with appropriation funding, and introduces  a new  performance indicator  - the execution ratio . The preliminary findings suggest that the inclusion of a key  performance indicator for contract execution can result in an increase in the final disbursement rate.   Background  The research effort centers on local contract fiscal stewardship within the Combined Security Transition Command-  Afghanistan. This multinational organization conducts the “train, advise, and assist” mission in support of the Afghan  National Defense and Security Forces, the military arm of the Afghan government (Resolute Support, 2020). In order  to conduct operations, CSTC-A budgets, requests, and executes the Afghan Security For ces Fund, an annual, two - year appropriation provided by the United States Government. ASFF is a specific section of DoD Overseas  Contingency Operations (OCO) funding, enacted by Congress, permitting the government to pay for the provision of  equipment, supplies, services, training, facility and infrastructure repair, renovation, and construction (Office of the  Secretary of Defense, 2020). To build the budget, CSTC-A is required to identify capability gaps within the Afghan",westpoint.edu,,,46.3144754,11.0480288
60,A TWO-LAYER ROUTING ALGORITHM FOR UNMANNED AERIALVEHICLE IN TRANSMISSION LINE INSPECTION,@mst.edu,"UAV, Transmission line inspection, Routing algorithm. ","The inspection of transmission lines using Unmanned Aerial Vehicle (UAV) has already grabbed many utility  companies’ attention because of the high efficiency and successful applications in different areas. In this paper, a two- layer routing algorithm is proposed to guide the motion of the ground team and the schedule of the landing points of  the UAV considering the simultaneous motion between the UAV and ground team in transmission line inspections.  The constraints for the UAVs flight route in the upper layer include the limited communication distance with the  ground vehicle and the bottleneck of battery capacity. Meanwhile, the rough terrain or the lack of accessible road is  the constraint for the ground vehicle in the lower layer. The inspection trip is first separated into several transmission  line segments considering the locations of substations.  Then, considering battery replacements, each transmission line  segment is further separated into several inspection segments. The algorithm will check the feasibility of both ground  route and flight route for each inspection segment considering the constraints and then revise it accordingly. Finally ,  the inspection sequence of all transmission line segments is identified considering the transportation time between  substations. A case study of three real transmission line segments connected by four substations is implemented to  show the performance of the proposed algorithm.  Keywords  UAV, Transmission line inspection, Routing algorithm.  Introduction  Transmission lines play an essential role in the overall performance of electricity networks because the electricity is  relayed and transmitted at a transmission line to supply power to customer nodes. Failure of transmission lines can  lead to power outages and therefore disrupt industries and our daily lives. To ensure a reliable power supply, the  transmission line in power systems needs routine maintenance because of the potential usage deterioration and  environmental corrosion. In earlier years, transmission line inspection was done by linemen, which is not an efficient  method due to low efficiency and dangerous working condition. Then, the helicopter patrol was adopted by utility  companies (Whitworth, Duller, Jones, & Earp, 2001; Earp, Eyre-Walker, Ellam, & Thomas, 2011). However, it is still  risky for the pilots and the team members to be close to the high voltage line.   Recently, Unmanned Aerial Vehicle (UAV) has been used as a promising approach in transmission line inspections  (Wang, Han, Zhang, Wang, & Li, 2009; Wang, Chen, Wang, Liu, Zhang, & Li, 2010; Li, Mu, Li, Wang, & Liu, 2016).  The existing study in this area mainly focused on the accuracy of the UAV inspection and concerned about the  hardware, such as data transmission and video image analysis. Only a few papers discussed the routing algorithm. For  example, Liu, Wang, & Liu (2009) proposed an inspection mission planning, but it was focused on the robot control,  not the routing optimization. Another path planning algorithm was proposed for the transmission line inspections  using UAV in (Cui, Zhang, Ma, Yi, Xin, & Liu, 2017), but the inspection path was for the different parts of towers",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
61,ENABLING DESIGN DECISIONS IN SET-BASED DESIGN WITHMULTIRESOLUTION MODELING,@uark.edu,"Literature survey, multiresolution modeling, sequential decision process, engineering management decisions, complex ","Complex system design requires significant uncertainty and risk reduction to develop feasible and resilient design  solutions.  Modeling and simulation offer a method for resolving uncertainty and reducing risk during early system  design and prior to prototype development and testing .  This paper surveys 22 multiresolution and variable fidelity  modeling publications and technical reports to gain perspective on their current role  in system development .  This  research identifies and defines the differences between multiresolution modeling (MRM) and variable fidelity  modeling (VFM) . We identify MRM requirements for complex system design programs with an emphasis of  informing system design decisions.   We then introduce a decision framework for complex system design programs.   This framework, developed for set-based design (SBD) applications, emphasizes the iterative use of decision analysis  and MRM to inform complex systems design decisions.  This research supports our ongoing efforts in developing a  quantitative SBD methodology for assessing and resolving uncertainty to inform engineering management and system  design decisions.  Keywords  Literature survey, multiresolution modeling, sequential decision process, engineering management decisions, complex  system design, set-based design  Introduction  Complex system design is fraught with uncertainty associated with requirements, technical readiness, performance,  design, schedule and budget.  Design uncertainty comes in two forms, irreducible aleatory uncertainty, and reducible  epistemic uncertainty.  It follows that epistemic uncertainty resolution is a key component of any complex system  design program, requiring deliberate and integrated  methods.  Multiresolution and variable fidelity modeling are a  family of modeling methods and techniques that can inform uncertainty resolution in system design programs.  This  paper surveys 22 publications on multiresolution and variable fidelity modeling techniques and practices.  This  survey’s purpose is to identify modeling methodologies to enable decision making supporting complex system design  and development.  This paper’s organization follows.  In the following section, we introduce and define multiresolution modeling  (MRM), variable fidelity modeling (VFM), and concepts associated with each.  Next, we present a synopsis of relevant  research.  We then  identify MRM requirements for complex system design , and propose a sequential decision  framework integrating multiresolution modeling to inform design decisions for a complex system development using  set-based design ( SBD).  We conclude with a short discussion of future research supporting decision making in  complex system design.  Defining Multiresolution and Variable Fidelity Modeling  On the surface multiresolution and variable fidelity modeling appear to be two names for the same methodology using  multiple analytical models.  There are even instances within the literature using the descriptors multiresolution and  variable fidelity interchangeably (Davis & Bigelow, 1998) .  However, MRM and VFM are two separate modeling  methodologies in terms of objectives and implementation.  Therefore, we require a clear definition of both concepts  before proceeding.  A literature search identified several definitions for both MRM and VFM (Davis & Bigelow,  1998; Davis & Huber, 1992; Davis, 1993; Hebert, Holzer, Eveleigh, & Sarkani, 2016; Mejia -Rodriguez, Renaud, &",uark.edu,University of Arkansas - Fayetteville,United States,36.0970389,-94.17033216657404
62,TYPES OF DIGITAL TWINS AND APPLICATION POSSIBILITIES INPRODUCT MANAGEMENT,@csun.edu,world ,"Competitive advantages are necessary and inevitable for the economic viability and sustaina bility of  enterprises, as well as for them to enable value creation. New technologies open strategic windows and provide  possibilities to create such advantages. The digital twin is one such revolutionary technology that helps  companies create a competitive advantage for themselves, particularly in the areas of product management.   This article presents the digital twin as a means to optimize the value creation in product development and  management. The paper will present how to integrate the concept of the digital twin into the process of technical  product management (TPM). To start with, the importance as well as the basic concept of digital twins will be  explained. Secondly different types of digital twins with their be neficial contributions will be discussed .  Thirdly the digital twin will be applied to the technical product management process. In this context, a  comprehensive framework will be developed to show the application possibilities in the different phases of the  TPM process.   Keywords: digitalization, digital twin, innovation, product management, technology, virtual and physical  world  Introduction  The digital transition  in multiple industries, implicates the evolution and implementation of new and revolutionary  technologies. These technologies provide a wide range of new and smart applications in nearly all industrial areas ,  including robotics, big data, internet of things, cloud computing, and artificial intelligence are driving industry towards  internet of things in terms of smart execution and performance (PwC, 2018). The capability of smart industry creates  benefits for companies to establish competitive advantage s and to achieve added value . Supply chain and  manufacturing activities can be realized better, faster, cheaper, and even customized with higher precision and quality.  Especially the process of technical product management is qualified for the application.   According to a study done by PricewaterhouseCoopers (2018), in the German manufacturing industry, robotics,  internet of things, big data, cloud computing, cyber security, and 3D printing would be invested in and thus drive the  industry towards smart manufacturing, for which digital twin was an integral part. This was also indicated by Gartner  in 2019, citing digital twin to be in the top 10 strategic technology trends which will be implemented by almost 50%  of industries by 2025.   The concept of the digital twin was presented the fi rst time by Grieves ( 2014). The basic definition of a digital  twin consists of three components: physical product, virtual product , and the connection in terms of communication  between them (Glaessgen & Stargel, 2012). The digital twin is a virtual image o f a corresponding physical object;   they are linked and able to communicate to each other in order to exchange data. The objects may be final products,  single parts or components as well as processes or even entire production facilities (Grieves, 2018, p. 67; Schonschek,  2018, p. 42). A definition crafted by NASA (Glaessgen & Stargel, 2012) stated “A Digital Twin is an integrated  multiphysics, multiscale, probabilistic simulation of an as-built vehicle or system that uses the best available physical  models, sensor updates, fleet history, etc., to mirror the life of its corresponding flying twin.” According to NASA,  the purpose of the digital twin was threefold: calculating predictions, monitoring safety, and doing diagnosis (Kunath  & Winkler, 2018).",csun.edu,"California State University, Northridge",United States,34.2455346,-118.52632210641266
