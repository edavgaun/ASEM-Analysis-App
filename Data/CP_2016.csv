,Title,email,KeyWords,Abstract,domain,Institution,Country,Latitude,Longitude
0,GLOBALIZATION AND COMMODITY PRICING: A CRITICAL VIEW,@me.com,"Globalization, Commodity, Emerging, Economies, Prices, Trade. ","Advent of globalization has brought benefits of varying proportions to consumers and countries irrespective of  geographical locations. Consumers have benefitted from relatively cheap products that would have been otherwise  remaining out of reach . Countries have penetrated markets that would have otherwise being closed to trades.  Prosperity from globalization has permeated the lives of individuals and comm unities across the globe from China  to Chile, Australia to Argentina: consumers basked in prosperity largely due to globalization. Globalize trade is  beneficial in varieties of ways. For instance, it provides alternatives, and creates larger markets which promote  specialization and lower cost. Over the years international trade has integrated developing and or emerging  economies into the global economy , though extent and pace of integration differed among the economies and  regions.   This paper examines the influence of globalization on trade integration and commodity pricing. Is there a  correlation between the commodity  pricing and globalization? What are the interacting variables influencing the  relationship? These and other critical questions will be examined; challenges and opportunities will be discussed.     Keywords  Globalization, Commodity, Emerging, Economies, Prices, Trade.    Introduction  Commodities are globally traded and most commodities serve as raw materials to variety of products which are  consumed across the globe. Establishment of minimum standards in commodity trading provide requisite premise  for uniformity irrespective of country  of origin of the commodity. For example, oil, copper, soybean, cocoa, coffee,  steel, etc. are globally traded commodities with minimum acceptable standard -  quality may vary slightly but the  uniformity of standard is never in doubt.   Commodity dependent e conomies are mostly developing countries (or emerging economies) that relies on  commodity export for its foreign exchange resource –  such economies includes the likes Saudi Arabia, Brazil,  Russia, South Africa, Nigeria, Australia, Argentina, Kazakhstan, Ku wait, Qatar, Oman, UAE, Malaysia, Indonesia  to mention a few.   Globalization of the world economy , volatility in c ommodity price s, coupled with increasing commodity  markets liberalization  have led to profound changes that seriously affect the weaker econom ies of the developing  countries, such as those mentioned above . Fluctuations in c ommodity prices have a significant negative effect on  economic growth, social development, financial resources, and income distribution, among others. All of which may  result in increasing inequality and or poverty (decreasing funding for education and health, for example) – leading to  social unrest and political instability.  Prior unsuccessful attempts to deal with commodity price fluctuations relied on stabilization schemes set up  in the context of international commodity agreements  (typical is Lomé Convention 1 through IV) .  Globalization of  commodity markets and the lowering of trade barriers, along with priorities  that focused on sustainable development  and poverty alleviation, call for a more innovative approach to commodity pricing risk management. Failure (even  after 14 years) of World Trade Organization  (WTO) Doha round of  negotiations further exacerbated the situation.  Frustrations with the failure of the Doha  round have led to series of bilateral and regional trade deals. For example,  the United States recently concluded the Trans -Pacific Partnership with eleven other countries (including Canada,  Japan, and Vietnam among others). In addition, America and the E uropean Union are negotiating the Transatlantic  Trade and Investment Partnership. China, which is not part of the Trans -Pacific Partnership, is signing series of  bilateral and regional agreements.  Regional agreements, while providing unique collaborative o pportunities can easily become a new tool for  trade segregation, dividing the world into different trading blocs with different rules on issues like labor, ",me.com,,,46.3144754,11.0480288
1,HUMAN OPINION COUNTS-MAKING DECISIONS IN CRITICALSITUATIONS WHEN WORKING WITH HIGHLY AUTOMATEDSYSTEMS,Missing,"Decision-Making, Governance, Automation, Cockpit, Pilot Survey ","Flight deck automation has improved the safety and efficiency of commercial aviation , but a broad consensus has   developed over the last 30 years that this technology is deficient in some areas.   Pioneering research by Edwards,  Billings, Wiener, Sarter, Woods, and  Degani indicated flight deck automation development has focused on the  technology rather than the human element, resulting in multiple cases of pilot- automation interactions adversely  impacting decision -making and contributing to loss of life and aircraft.   (Air France Flight 447, 2009; Turkish  Airlines Flight 1951, 2009; Asiana Flight 214, 2013, and others ).  More research is needed to address this gap and  improve pilot-automation decision governance.  Our research seeks to address such questions as:   (1) Do pilots trust  and rely on current automated cockpit systems?  (2) Does current cockpit design provide pilots the sufficient amount  of correctly timed feedback to minimize decision risk under time pressure? (3) Do pilots think that automated  cockpit systems should conform to Crew Resource Management (CRM) principles?     This paper reports survey results of 77 pilots from different carriers and geographical regions and holding  various type ratings.   The survey was designed to measure pilot perception of cockpit automation across five  dimensions: trust in cockpit automation, cockpit automation monitoring practices, effectiveness of cockpit  automation interface design, flight automation/company policies, and sufficiency of training in cockpit automation.   Line pilot surveys can guide the empirical research for development of improved cockpit automation wh ich better  supports decision making in complex and dynamic environments, under time pressure.     Keywords  Decision-Making, Governance, Automation, Cockpit, Pilot Survey    Introduction  Commercial air transportation is the safest means of transport (Savage, 2013).  Over the last thirty -five years, the  accident rate has steadily declined.  This is largely attributed to the deployment of more automated flight   management systems in the 1980 s.  While safety has improved, concerns have been raised since the 1990 s about  how these automated systems can sometimes contribute to accident causation (Sarter, Woods & Billings, 1997).  Advances in cockpit automation have taken away the easy parts of the job while making the difficult parts  more difficult (Bainbridge, 1983). Workloads have increased where it was already high and decreased where it was  already low (Wiener 1989). This leaves the pilot, whose reliability is variable, to watch out for things that are not  under their direct control.  People are poor monitors of systems whi ch rarely fail (Bainbridge, 1983; Parasuraman,  Molloy & Singh, 1993)  but this has become the pilot’s role.  The more advanced the automated system, the more  crucial the contribution of the human operator becomes to the successful operation of the system ( Bainbridge, 1983).   Accident and incident reports cite more problems with  human-machine interaction and this gave birth to the  term  “automation surprises” (Sarter, Woods & Billings, 1997; Chialastri, 2012 ).  A Federal Aviation Administration  (FAA) report (The Interfaces between Flightcrews and modern flight deck systems , 1996) cited problems with the  then current automation designs, saying they did not reflect a human centere d approach.  Automated systems are  limited by their  intransigence in many circumst ances (Boy, 2011).  Pilot error is often  the consequence of more  complex issues in the automated systems (Dismukes, Berman & Loukopoulos, 2007).      Case Studies  There are many examples of aircraft accidents that reflect inadequate feedback from automation.   China Airlines  Flight 140 crashed in 1994 while the pilot was trying to land. The take off/go around (TOGA) mode was accidently ",Missing,,,46.3144754,11.0480288
2,RELIABILITY PROJECT OPTIMIZATION:A SOUTH AFRICAN RAIL CASE,@transnet.net,"Transnet, Rail, Projects, Project Reliability, Project Failure, Railway Projects ","The delivery of projects in the rail industry is a challenge. Transnet SOC Ltd (The South African national rail entity)  is the sole bulk freight logistics rail company in South Africa. Transnet Capital Projects (TCP), a division of  Transnet SOC Ltd, acts as a service provider to all the operating divisions of Transnet. TCP projects constantly  failed due to what seemed like project integration challenges.   The purpose of this research is to determine Transnet’s deficiencies and recommend  solutions based on  project management best practice in the field of railway engineering.   This was achieved through the use of a  questionnaire that was designed based on the lessons learnt, via an international best practice review, specific to  railway projects. The research found that the major deficiencies lay within var ious levels within Transnet, specific   issues being people and other issues  being structural and technological . The research recommends a change in the  TCP organizational structure, the re cruitment of rail experts, skills development of project managers, the  introduction of new railway software and the establishment of an Enterprise Project Management Office as potential  international best practice solutions.    Keywords  Transnet, Rail, Projects, Project Reliability, Project Failure, Railway Projects    Introduction and Problem Statement   According to the Project Management Body of Knowledge (PMBOK), “A project is a temporary endeavour  undertaken to create a uniq ue product, service, or result” (Project Management Institute, 2013) . In the conte xt of  railways, numerous organiz ations have applied the PMBOK philosophy in ensuring the successful delivery of rail  projects (Xaba, 2011).   South Africa’s (SA) freight logistics cost are at 12.7% of the cou ntry’s GDP. This is relatively high in  comparison to the rest of the world and can be attributed to the fact that South Africa has about 1% of the world  population, produces 0.4% of the world’s GDP and yet requires more than 2% of global freight in terms of ton - kilometres (Havenga & Simpson, 2013). This has brought about concern with the manner that rail projects are being  executed in SA and the fact that the sole rail bulk freight logistics company in the country has announced a n R300bn  (USD 22bn) capital investment in rail projects over 7 years, calls for even greater concern  (Transnet, 2013).   Transnet SOC Ltd is responsible for the movement of general freight and commodities in South Africa.  Transnet Capital Projects (TCP), a division of Transnet SOC Ltd, is responsible for executing all the Capital Projects  of Transnet’s operating divisions. The operating divisions are Transnet Freight Rail (TFR), Transnet Pipelines,  Transnet Property, Transnet National Port Authority, Transnet Port Terminals and Transnet Engineering.   TFR is responsible for all the railway operations and maintenance within Transnet but with rail project,  TCP acts as a service provider to TFR.    The Transnet Strategy  Brian Molefe, The Transnet Chief Executive Officer (CEO) stated the foll owing, “We are poised to become one of  the world’s largest freight logistics groups. The Market Demand Strategy projects Transnet’s revenue grow from  R46bn in 2011/12 to R128bn in 2018/19”  (Transnet, 2013). Transnet embarked on the biggest project to date, this is  known as the R300bn Market Demand Strategy (MDS) which aims to boost infrastructure development, job creation   and investment in South Africa (Transnet, 2013).   ",transnet.net,,,46.3144754,11.0480288
3,OPTIMAL DECISIONS FOR VOLUME DISCOUNTED GROUPPURCHASING WITH TRANSPORTATION PLANNING,@uncc.edu,"Group purchasing, quantity discount, transportation planning, logistics, supply chain, optimization, mathematical ","Group Purchasing (GP) is a procurement strategy that can be deployed by businesses in different industries for  purchasing input material or service for a lower price by leveraging the negotiation power through aggregation of their  demands. It can help small as well as big companies to save on purchasing cost which can lead to higher benefit. In  the process of GP, the suppliers send their price curves to the GP agent who decides about purchasing plan; such that  the total purchasing cost is minimized. Besides procurement, transportation cost is a vital factor in determining the  competitiveness of a business. The GP agent can ele vate the benefit to customers by taking into account the  transportation planning problem in the GP decision making problem and use the aggregated order size to plan for the  transportation of material from suppliers to customers. In this research, we study optimal decisions for the procurement  of materials as well as the transportation, simultaneously to minimize related costs. We develop mixed integer linear  programs to model different transportation strategies and present numerical results under various GP scenarios.    Keywords  Group purchasing, quantity discount, transportation planning, logistics, supply chain, optimization, mathematical  modelling.    Introduction  In this paper the GP problem will be a ddressed along with the transportation problem  as a value added service in the  supply chain. Some suppliers may offer quantity discounts for their products; in GP the buyer’s demand is aggregated  to benefit from this discount opportunity. The GP agent has the task of managing the customers, dealing wi th the  suppliers and making the decision for purchasing quantity from each supplier. GP is used in both business to business  (B2B) and business to customer (B2C) environment (Chen & Roma, 2011) .   The literature in GP has addressed the economic effect of GP on different members of the supply chain, and  the decision making on purchasing quantity from different suppliers(Chen & Roma, 2011; Davenport & Kalagnanam,  2002; Ozelkan E. C., Geismar,  & Srikandarajah, 2003; Ozelkan, 2006; Van Horne, xd, & rge, 2013; Zhou & Xie,  2014). Transportation can be an important component of the total procurement cost; e.g. up to more than 15% of the  total procurement cost in some industries such as the food industry (Edwards, 1970) . Therefore, it is important to  consider the distance between the suppliers and customers when deciding about GP quantities as some suppliers might  have lower prices while the transportation cost might be a limiting factor.  In the next  section the relevant literature in GP and transportation planning will be reviewed. Then three  transportation strategies will be proposed and modelled in a GP framework. Numerical analysis under different  scenarios will be presented in the third section. The last section includes a summary, conclusions and future research  extensions.    Literature  GP Literature can be categorized in two general classes depending on the diversity and  volume of the products that  are considered in the model. Combinatorial model s typically consider short term supply of several types of products  (Davenport & Kalagnanam, 2002) in which each supplier can offer a single product or a bundle of different products  at a special price. The problem becomes identification of optimal  bundles to address customer demand . Volume - discounted models, which are studied in this paper involves one type of product to be provided for the customers. Due  to the high volume of the demand in these type of GP problems each supplier may provide a portion of total demand  due to limited capacity (Davenport & Kalagnanam, 2002). ",uncc.edu,University of North Carolina at Charlotte,United States,35.311795,-80.741203
4,M,Missing,,,Missing,,,46.3144754,11.0480288
5,MATERIAL DELIVERY TRANSFORMATION OF A FREEZER PLANT,@stcloudstate.edu,"Forklift elimination, material delivery, a freezer plant ","Forklift free strategy is widely embraced in the industry due to its efficiency and safety factors in material handling in  industrial manufacturing. A project was undertaken at a freezer manufacturing plant to implement such a forklift-free  strategy. The goal of the project was to transform the material delivery method of an assembly line and replace one  of the forklifts with a tugger -trailer. A scientific basis of replacing the forklifts with tugger -trailers combination was  provided in the first place by analysis of the pros and cons of  the selection of tugger -trailer among other material  handling transportation facilities in the system. With a foundation laid, the project methodology was detailed with a  sequential procedure of 4 phases focused on material delivery mapping, part selection, workload balancing through  time-motion study and customized carts design. As a result of the study, customized cart order, feeding spots and  carts’ quantity confirmation system were implemented. The project was completed  with a plan of eliminating  forklifts completely for the sake of safety and delivery efficiency.    Keywords  Forklift elimination, material delivery, a freezer plant    Introduction  This article will document a project of  forklift elimination practice in a freezer  manufacturing plant. The forklift  safety concern will be introduced  in the first place followed by  analysis of alternatives of industrial trucks’ material  delivery. With basis set, the comparison of tu gger-trailer and forklift delivery will be detailed to justify the strategy  of replacing forklifts with tugger -trailers. The remainder of this article will elaborate on  several stages adopted to  plan and implement the forklift elimination strategy.     Forklift safety concerns   Every year hundreds of accidents spring up  from forklifts globally. Federal Occupational Safety and Health  Administration estimates that “in the United States alone, there are nearly 100 worker fatalities and another 20,000  seriously injured in forklift-related incidents annually” (Henshaw, 2004, para. 6).  Nevertheless, National Institute of  Standards and Technology stated that “110,000 forklifts accidents each year and $ 135 million immediate costs are  incurred due to forklift accidents ” (Bostelman, 2009, p. 1). Consequentially, manufacturing costs rise due to  compensation for workers and factory’s operating environment. Forklift operating environments are characterized by  the presence of pedestrians, blind spots, narrow aisles, and building columns, the need for  both indoor and outdoor  use, and 24 hours per day operations, and chances of tight turning radii. Among all, visibility and pedestrian -forklift  interaction topped concerns in the freezer  manufacturing plant. The combination of th e forklift operator enclosure,  forklift mast assembly and the forklift load result in a  limited visibility for forklift operators and the resulting blind  spots are the culprits of potential accid ents. According to the 2014 edition of Injury Facts from the National Safety  Council, “66 workers died as a result of operating or working near a forklift. A forklift was a secondary source of  fatal injuries for an additional 24 workers . These injuries often occur to workers who are walking or working near  forklifts, and who are not seen by the trained forklift operator.” (Stewart, 2014, para. 4).        ",stcloudstate.edu,Saint Cloud State University,United States,45.55139,-94.14833
6,A FUZZY PROGRAMMING APPROACH FOR AGGREGATEPRODUCTION PLANNING WITH FLEXIBLE REQUIREMENT PROFILE,@uncc.edu,"Aggregate production planning,  Flexible requirement profile , Fuzzy mathematical programming, mixed integer ","Aggregate production planning in a multi-period setting, involves determination of production quantities, workforce ,  and inventory levels for a selected planning horizon. Due to the uncertainty of some planning parameters such as  demand and costs, generation of a stable plan may be a challenge, which  would result in nervousness in the  organization and a state of instability throughout the supply chain. In this paper, a Flexible Requirement Profile  (FRP) based fuzzy aggregate production planning model is investigated to deal with uncertainties and for creating  stable plans for mitigating “plan nervousness ”. This planning technique  considers flexible upper and lower bounds  for production levels of periods ahead. We consider a cost minimization fuzzy optimization model with a fuzzy  objective function and fuzzy constraints. The problem is solved using  a max-min optimization method considering  aspiration levels of fuzzy membership functions . Preliminary results show t hat the fuzzy model is promising,  resulting in  lower planning cost and higher stability compa red to non- fuzzy and the traditional non -FRP based  models for a textile industry data set.     Keywords  Aggregate production planning,  Flexible requirement profile , Fuzzy mathematical programming, mixed integer  programming.    Introduction  In production-based industries, development of an appropriate production plan is of high importance, as it could be  used for cost, workforce, inventory and all capacity related estimations in the immediate and near future. However,   as an inevitable part of each planning problem, uncertainty of demand and cost estimations, could lead to lack of  planning stability in production environments. In addition, frequent adjustments to the production plans can lead to  increase in production and inventory/shortage costs, reduced productivity, lower customer service levels, and a state  of nervousness and confusion (Demirel, 2014). Several techniques have been proposed for dealing with planning  instability; for example, frozen horizon technique considers a frozen planning period during which no change to the  available plan is allowed. One big issue related to this technique is that it may not be as responsive to changing  demand and consequently could lead to inventory pile -ups or shortage s. In industries where the customers are of  high importance or where a wide range of different products are produced, this issue can be even more serious   (Demirel, 2014). Another technique is consideration of inventory safety buffers for reducing the effect of demand  uncertainties. This approach is often criticized for increased holding cost and potential for obsolete products for  short shelf l ife products.  (Costanza,1996) and (Srinivasan, 2004) introduced an alternative approach, called  the  Flexible Requirement Profiles (FRP), which enforces flexible bounds for production levels in different periods, and  hence, production amounts could change within the calculated upper and lower bounds, which is expected to  improve the production  stability. This paper utilizes the FRP concept in a mathematical production planning  optimization model. Our research not only utilizes the FRP approach for reducing nervousness due to the planning  uncertainty in form of a mixed integer linear programming model , it also incorporates the uncertainty of model  parameters such as  demand and cost, using fuzzy logic . As will be elaborated in subsequent sections fuzzy logic   considers interval values for each uncertain parameter, defined by a specific membership function.   The rest of this paper is organized as follows: next section  represents the literature review o n FRP and  aggregate production planning with fuzzy uncertainty, while our model formulation and fuzziness discussion are ",uncc.edu,University of North Carolina at Charlotte,United States,35.30649445,-80.73497049689712
7,LONG-TERM PROGRAM PLANNING: A WEIGHTED SUM MAKE-SPANMINIMIZATION,@uncc.edu,"Program planning, Rolling horizon planning, Multi-project, Deliverable items. ","In long-term program planning and management, p rograms consist of  multiple projects (with precedence relations  among some of them) such that each set of projects may lead to a specific deliverable item to be delivered in related  phases of the program.  Due to uncertainty and the long make -span (total duration) of the pr ograms, it is not logical  to do the detailed scheduling for all the phases  The objective of this paper is to develop a more realistic overall plan  for the whole make -span to determine overall start and finish times  of the deliverable items and projects for  the  different phases and then to elaborate on the nearest phase (the main idea behind rolling horizon planning) . The  advantage of this approach it that the remaining work later can be revised based on the implementation  progress of  the program.  This paper presents an optimization for an integer programming model of  planning the whole program  considering different deliverable items, item -related projects  for each deliverable, precedence relations among  projects, and desired finish time for deliverable items based on the phases they are assigned to. A real-life case of a  wastewater recycling program in Iran  is used for this model here to illustrate the methodology and to show that t he  overall plan can be utilized as a baseline for further det ailed scheduling and it can also be re-planned, if needed, in  the future.    Keywords  Program planning, Rolling horizon planning, Multi-project, Deliverable items.    Introduction  Programs consist of multi -projects, which require a great amount of money to be invested  for long- term running of  these related projects. According to PMI (Project Management Institut e, PMI, 2016)), programs are defined as a set  of related projects to be run for gaining the outputs not achievable from running each project individuall y. Some  examples of programs include constructing a dam, a power plant, an airplane, aircraft, and even comprehensive  R&D programs (Bendoly , Smith & Bachrach, 2010). Programs are conceptually different from both projects and  portfolios of projects. Most importantly,  programs consist of multiple related projects, hence they are different from  single projects. In a portfolio of projects, the projects are not necessarily related, but they might just be dependent on  some resources in common. In programs, on the other hand , the projects are related to each other through  precedence relations and also through the desirable outcomes , which form the ultimate goal of running the whole  program.   Efficient project execution is a key business objective in many domains , and particularly in capital projects  in the industries (Young & Samson, 2008). But before executing the projects, having a reliable baseline plan is an  indispensable part of each project ’s successful implementation  because determination of time, cost, and  required  resources for running the project is  only possible while having a baseline plan available  (Herroelen & Leus, 2004).  While it is emphasized to develop a baseline plan, it is also necessary to keep in mind that unexpected disruptions  might happen during each project implementation,  which sometimes result s in noticeable biases from the main  baseline. The reasons why these disruptions might happen are: (1) the suppliers` performance is unreliable, (2) the  amount of effort needed for implementing eac h activity may not be completely known, (3) activity duration  estimation is not easy, (4) it is not always easy to determine the exact level of available resources, and so on. This  concern is more dramatic when program needs to be implemented compared to a  single project or a small multi - project network implementation. The programs are implemented for a longer period of time,  and a huge amount of  money and effort are needed for running them. As disruptions may happen during the program implementation, it is   absurd to have a detailed plan for the whole program life time horizon. In other words, it is more advisable to do an ",uncc.edu,University of North Carolina at Charlotte,United States,35.30649445,-80.73497049689712
8,REVERSE LOGISTICS FRAMEWORK FOR PET BOTTLES,@gmail.com,"Reverse Logistics, PET Waste Bottles, Recycling, Waste Management ","Reverse logistics (RL) is an engineering strategy used by manufacturing companies to develop environmental  sustainability through recycling. The result of not having appropriate legislation and frameworks  in Zambia, specific  to RL for plastic bottles, huge volumes of Polyethylene Terephthalate (PET) bottles are dum ped on the  Environment.  Only 30% of the waste generated, in Zambia, is collected for disposal in dumpsites and the remaining  70% is not recovered. Of the 30% waste collected, there is no data to indicate the exact amount of PET bottles  disposed.   This paper focuses on analyzing RL activities performed by beverage manufacturing companies in  conjunction with community involvement. Examining the regulat ions set by the regulatory bodies in monitoring  waste management issues. Three separate questionnaires are issued, one for the beverage companies, one for the  regulatory body and one for the municipality. Structured interviews and direct observations were also used. The  results indicate that, RL of PET bottles is not practiced by the beverage companies. However the companies  recognize the importance of recycling PET plastic bottles and have printed symbols of recycling on their bottles.  Measures taken to pr otect the environment indicate regulations from the regulatory body are in place though not  effectively enforced on PET plastic waste This paper focuses on analyzing the data collected via the three tier  questionnaires and providing some insights into opti ons to implement RL, within the Zambian constraints. A  Container Deposit logistics Refund Legislation (CDRL) framework was developed and proposed for use in the  recovery of PET bottles and any other recyclables    Keywords  Reverse Logistics, PET Waste Bottles, Recycling, Waste Management    Introduction  Several million tonnes of plastics produced every year are used for packaging materials and almost any type of  consumer product (Papong, Malakul, Trungkavashirakun, Wenunun, Chom -in, Nithitanakul, 2014; Blanco , 2014).  Post initial use, packaging material (PET bottles) become waste which is later disposed to the environment. With the  global consumption of Polyethylene Terephthalate (PET) packaging forecasted to reach 19.1 million tonnes by 2017,  with a 5.2% increa se per annum between 2012 and 2017 (Smithers Pira organization, 2012). Bottles for water,  carbonated soft drinks, and other beverages account for 83 –84% of global PET resin demand (Information Handling  Services, 2012). The huge increase in plastic consumption, has led to various issues such as environmental pollution,  health concerns for scavengers and low utilization for this reclaimed waste (Michiko, 2004). According to Rubio,  Chamorro & Miranda (2008), research on strategic aspects of reverse logistics i s scarce. Besides, very few attempts  in the supply chain research area are conducted to study reverse and recycling supply chains (Wong 2010).  Formigoni and Rodrigues (2009), and Coelho (2011) studied the recycling collection system and found that PET  bottles in Brazil are entirely recycled by informal sectors, and that the main problem of the reverse chain is selective  collection. Zhang and Wen (2014) studied the consumption and recycling of collection system of PET bottles and  find that, 90% of the post consumed PET bottles were collected by the informal collectors and were reprocessed by  small factories and the main problem was merging the two sectors ( informal and formal) into the formal sector. The  Smithers Pira organization (2012) report, which forecasted global PET packaging usage in 2017, found that  collection and recycling are key issues along the PET supply chain even though there seems to be a lack of emphasis  and research on the management of the End of Life (EoL) of disposable soft drink plastic bottles (WRAP, 2009c). ",gmail.com,,,46.3144754,11.0480288
9,ERP IMPLEMENTATION PITFALLS: HOW TO CONTROL AND AVOIDTHEM.,@itesm.mx, ,"Nowadays companies are embedded in an increasingly dynamic and more challenging environment than ever before.  The ability to obtain real time data from cu stomers, consumer trends, global financials and market information,  provides them with valuable inputs (that if used properly) will allow them to obtain larger market share, increase  profits for the shareholders and be more efficient and responsive to chan ges, thus strengthening their competitive  skills.   From an information systems and process perspective this translates, for the organizations into have a more  integrated and streamlined operation that allows them to have a better use of information than tr iggers the decision  making process and flows throughout every step in the operation if the company.  By using and implementing ERP´s  (Enterprise Resources Planning) they improve their key performance indicators and optimize the resources  management in sync with the market. These implementations differ from company to company and I can find out  pitfalls at each company that can be documented and shared so that new companies can consider and avoid for future  ERP implementations.  The purpose of this paper is t o collect and present ERP pitfalls and success factors and how they can be  avoided by applying ERP implementation best practices. The results were obtained of a large manufacturing  company´s ERP implementation, called Company MX. After the continuous improvement journey some lessons were  learned and several best practices were defined, now these are exported to other companies of the same brand in order  to have more successful ERP implementations. The survey was based on interviews with the project leaders,  people  involved in the project of different departments  called process owners , and internal and external consultants of this  ERP implementation process. In my case I was an external consultant in some parts of the full ERP implementation  process at compan y MX working with the SME´s (Subject Matter Experts) . The theoretical framework is based on  ERP philosophy and Supply Chain Management (SCM).    Keywords    Enterprise Resources Planning (ERP), integration, pitfalls, best practices.    Introduction    Nowadays, companies are immersed in a strong competition where integration among their processes could be a  success factor. ERP is a strong tool to achieve this integration. ERP is a philosophy which tries to fully operate an  enterprise in conjunction with  the environment, there needs to be distributed applications for planning, scheduling,  manufacturing, logistics, costing, and so on to the multiple layers of the organization: work centres, sites, divisions,  and the whole corporation.  Multiple languages an d currencies are also being included for global applications (Moctezuma, 2000).  According to APICS Dictionary, another definition of ERP can be: “Framework for organizing, defining, and  standardizing the business processes necessary to effectively plan and control an organization so the organization can  use its internal knowledge to seek external advantage”. The problem is that the implementation of this philosophy is  a hard journey, where a company must invest a lot of resources such as money, time, and people. For this reason it is  important to assess the right mix of people, knowledge, training, leadership and timeline to avoid implementation  pitfalls that will translate into a larger costs and additional resources.   There are several companies that have implemented an ERP system successfully, so companies can take  advantage of those lessons learned to avoid ERP implementation pitfalls. Also there are several success factors that  companies have put in place, we can call them best practices. Originally, ERP  gives companies two major benefits  that did not exist in the days of non-integrated (although interfaced) departmental systems (Dillon, 1999):  ",itesm.mx,Instituto Tecnológico y de Estudios Superiores de Monterrey (ITESM),Mexico,25.6516,-100.2895
10,AUTOMOTIVE INDUSTRY - A CASE STUDY OF PAKISTAN,@yahoo.com,"Productivity measurement, enhancement model, automotive industry, manufacturing.  ","Basic methodologies for gauging productivity are not used in the industry, relatively nonstandard tools are practiced   to measure and evaluate productivity. In Pakistan,  specifically not enough efforts have been put in to measure and  enhance the productivity of manufacturing industry. This research was focused on identifying the prevalent  condition of productivity in automotive industry of Pakistan and suggesting a productivity enhancement model. This  is a qualitative research in which open ended questions were asked. In order to develop a productivity enhancement  framework, a field survey was conducted fr om top management of automotive industry. A total of 40 interviews  were conducted from CEO’s and Directors of 28 automotive factories of Pakistan. Open ended questions used for  survey were compiled from the internationally published work for validity and r eliability requirements. Role of  technology in productivity enhancement of this industry was explored. Findings of this research have shown that   lack of  training, less education, resistance and lack of skilled manpower are the major problems faced by the  industry, in implementation of latest techniques and technologies.  Results of this research depicted that there is an  immediate need to implement these concepts for productivity enhancement. The cost effective solutions provided in  this model show that this model can be used for manufacturing industries in general in the developing countries.     Keywords  Productivity measurement, enhancement model, automotive industry, manufacturing.     Introduction  With expansion of businesses and vastness of economy, geographical boundaries are no more a limit. Complete  world has become a common market, anyone from anywhere, can come into the field of competition. Total  productivity is generally used as measure of competiveness (Porter, 2011) . In order to remain competitive i n the  global market companies and firms are striving for higher productivity standards. With the changing scenarios,  methodologies used for measuring productivity and even defining productivity needs more thorough research and  studies (Kumar et. al, 2008; Sumanth, 1997). In past few decades a lot of research studies have been carried out on  productivity all over the world (Azadeh et. al, 2014; Das, 1999; Kumar et al., 2008; Sumanth, 1997; Tompkins  et.  al, 2010) but unfortunately in Pakistan not enough effor ts have been put in to describe and gauge the productivity,  specially of manufacturing industry.   The difficulties faced in measuring productivity are several, different objectives being the main deviation.  Objectives of organizations  and nation are multidimensional. The objectives of a nation are to improve the living  standards of the citizens, increase employment and create more jobs. While m ain objectives of the firms are;  to win  market shares both domestically and internationally, enhance profits, and c ompete globally. Different methods of  measuring productivity have been proposed, use of partial productivities is generally resorted to but they cannot  depict complete picture so measuring Total Productivity was proposed by (Sumanth, 1997). Several industr y wide  surveys in different countries have reported that basic methodologies for gauging productivity are not in use rather  nonstandard tools are used to measure and evaluate productivity (Hannula, 2002; Singh, Motwani, & Kumar, 2000;  Sumanth, 1984). A lot  of research is required in different fields to identify the productivity problem areas and to  Copyright, American Society for Engineering Management, 2016 ",yahoo.com,,,46.3144754,11.0480288
11,OPTIMIZATION OF OPERATING ROOM SCHEDULING,@gmail.com,"Operating Room Scheduling, Optimization, Healthcare. ","Operating room scheduling is one of the most complex tasks in hospitals due to the integration of many actors with  conflicting objectives such as surgeons, patients, nurses, and operating room management. At the same time, operating  rooms are one of the most important areas in a hospital for generating a big portion of hospital costs and revenues.  With having increased demand for operating rooms, health care providers are facing a pressure of how to decrease  hospital costs and improve the quality of care delivered to patients. One of the ways of decreasing the costs and  improving the quality of care is having an effective operating room scheduling that will optimize the utilization of  operating rooms and minimize the patients’ length of stay. When patients stay longer in hospitals, the hospital costs  increase as they require hospital resources for a longer period of time, and the quality of care is improved when patients  are given the care at the right time. Thus, in this study, a model is developed which allocates a hospital's limited  resources to multiple operating rooms in order to produce schedules that improve service quality and minimize the  patients’ stay and associated costs.   Keywords  Operating Room Scheduling, Optimization, Healthcare.  Introduction  The increasing demand and cost of health care have made the health care industry one of the largest and fastest- growing industries in the developed world and the No. 1 domestic industry in the United States (Agdestein, S. D.,  2012 - Carter, M., 2002). Therefore, health care providers are under pressure to use the limited resources in the most  efficient way. One of the most significant goals of hospitals is to provide effective and efficient quality and safe service  (Paz, J., 2010). According to a report of the Committee on Quality of Health Care in America, in 1998, between 44,000  and 98,000 Americans die due to medical errors (Paz, J., 2010). The costs of these medical errors are between $17  billion and $29 billion (Erdogan SA, Denton BT, 2010).  Improving the quality of care given to patients and decreasing hospital costs are two of the most challenging  problems that health care providers need to manage along with the limited resources (McLaughlin, M. M., 2012).  While improving the quality of care and decreasing costs, hospitals need to minimize patients’ length of stay, waiting  time in hospitals, and optimize the utilization of service rooms to improve efficiency (McLaughlin, M. M., 2012).  Operating rooms (ORs) are considered the most important part of the hospitals and are responsible for  generating more than 40% of a hospital’s total revenue (Everett J, 2002) and resource costs (Erdogan SA, Denton BT,  2010; Fei, H., Meskens, N., & Chu, C., 2010; Globerman, S., Esmail, N., Day, B., & Henderson, D. R., 2013).  Managing the operating room (OR) planning and scheduling is not an easy task due to limited resources of ORs and  surgeons, increased demand to ORs, and conflicting priorities of patients and surgeons. One of the effective ways of  managing the OR planning and scheduling is an optimal allocation of available resources to ORs. In the United States,  hospitals need to optimize the inpatients’ length of stay in hospitals since health insurance providers, such as Medicare,  Blue Cross, etc, do not reimburse hospitals for any unnecessary long length of stay of inpatients (Zhang et al., 2009).  Since an inefficient OR planning and scheduling brings an unnecessary long length of stay, optimally allocation of  limited resources to have efficient OR planning and scheduling is a great interest for hospitals (Zhang et al., 2009). In  this study, a mixed integer programming (MIP) model is developed to optimally allocate the limited resources in the  hospitals. This MIP model minimizes the patients’ length of stay and optimizes the utilization of the ORs.     Literature Review   Since patients’ length of stay is one of the measures of health care quality (McLaughlin, M. M., 2012) and one of the  performance metrics (Persson, M. J., & Persson, J. A., 2010) and related to hospital costs (Vanberkel, P. T., 2011),  this goal needs much attention for researchers. Zhang et al., 2009 examined how to best allocate OR capacity to  minimize patients’ length of stay by developing a MIP model. They state that reducing patients’ stay time decreases  health care cost since patients will be using beds and other resources while they stay. Accordingly, their work is the  Copyright, American Society for Engineering Management, 2016 ",gmail.com,,,46.3144754,11.0480288
12,APPLICATION OF A FUZZY INFERENCE SYSTEM TO CIVILENGINEERING PROJECTS,@tdea.edu.co,"Civil engineering, auditing, fuzzy inference system, decision making ","This paper proposes a Fuzzy Infer ence System (FIS) for auditing purposes in order to evaluate the achievement of  goals in civil engineering projects. Thanks to interviews with experts, seven relevant variables of the process were  identified: quality of materials, equipment, payments, amou nts, length, logistics and physical risks. Then the  relationships among these variables were established in order to measure three aspects in general: technical aspects,  budget and administrative control. Based on this, it is possible to have a project performance evaluation in terms of  satisfaction. The reasoning processes of auditing experts are represented by means of a cognitive map that provides a  structure of fuzzy rules (the knowledge base of the system). By applying this, along with approximate reas oning  processes, it is possible to have a quantitative assessment of the progress of projects. In addition, it is possible to  identify the strong and weak points of the planning and executing process, which supports the making of decisions  that seek improvement. The model proposed yields consistent results for the hypothetical levels of the analyzed entry  variables, which would result in objective evaluations during auditing processes.     Keywords  Civil engineering, auditing, fuzzy inference system, decision making    Introduction  The modeling of diverse engineering problems has been framed within parameters of classical logic, where  everything either has or does not have a strict degree of belonging to sets. However, as it has become evident in  recent times, th ere may be intermediate points in which circumstances are not necessarily false or true, accepted or  not accepted. Therefore, fuzzy logic has performed a very important role by providing processes being modeled in  the knowledge era with different nuances.  Fuzzy logic takes into account technical, budgetary and administrative control aspects such as quality of  materials, equipment, payments, amounts, length, logistics and physical risks directly related with the administrative  and financial capacity, market understanding, technology, and synergies of the project. By using an analyst's  perceptions and evaluations regarding the previous subjects, it is possible to modify an auditor's decision and add  elements above even the results obtained through the decision  criterion (NPV, IRR, or assessment through real  options). (Magni, 2002) and (Hwang, 1992) point out to the problems and inconsistencies of the classical bivalent  decision theory, and suggest new polyvalent approaches. With the onset of a world where odds eliminated its  precision, and with the aid of vast amounts information, a new paradigm of a grey and fuzzy world, full of  possibilities, has opened up.   It is precisely that fuzzy world that generates the advantages of fuzzy logic, which is based on the possibility  of establishing a reasoning similar to that of the human mind, and of defining reality's behavior by means of ",tdea.edu.co,Tecnológico de Antioquia,Colombia,6.28027755,-75.582804
13,DEFINING RESILIENCE: A PRELIMINARY INTEGRATIVELITERATURE REVIEW,@mst.edu,"Resilience, resiliency, resilient, critical infrastructure, State-of-the-Art Matrix ","The term “resilience” i s ubiquitous in technical literature ; it appears in numerous forms , such as  resilience,  resiliency, or resilient, and each use may have a di fferent definition depending on the interpretation of the writer .  This creates difficulties in understanding what is meant by ‘resilience’in any given use case, especially in discussions  of interdisciplinary research.  To better understand  this problem, th is research constructs a preliminary integrative  literature review to map different definitions, applications and calculation methods  of resilience  invoked within  critical infrastructure  applications. The preliminary  review uses a State- of-the-Art Matrix ( SAM) analysis to  characterize differences in definition across disciplines and between regions. Q ualifying the various us ages of  resilience will produce a greater precision in the literature and a deeper insight into type s of data required for its  evaluation, particularly with respect to critical infrastructure calculations  and how such data may be analyzed.  Results from this SAM analysis will create a framework of key concepts as part of the most common applications for  “resilient critical infrastructure” modeling.     Keywords  Resilience, resiliency, resilient, critical infrastructure, State-of-the-Art Matrix    Introduction  “Resilience” is a term that is used extensively in recent technical literature, but is defined somewhat loosely. Its  definition typically varies significantly across disciplines, meaning one thing to psychologists, another to mechanical  engineers or environmental scientists, urban planners, businesses, engineering managers and so on. While this may  be useful within a discipline, confusion becomes significant in an interdisciplinary communication.  It is therefore  useful to codify the different definitions of the terms and to coalesce thes e definitions into something that can be  useful in interdisciplinary research (Department of Homeland Security, 2012).   Towards this end, this paper performs an integrative literature review to identify various definitions of  resilience, its applications and calculations. A major focus of this study is to identify any shifts in these usages over  the past 10 years. A State-of-the-Art Matrix Analysis (SAM) is conducted on these review results. The SAM will be  broken up into three main categories:  definitions, applications, and calculations of resilience. The ultimate goal for  this research is to provide guidance in constructing a definition of resilience that spans multiple disciplines.    Methodology  A SAM Analysis characterizes, organizes and analyzes research (Guadalupe and Beruvides, 2012 ; Egbue and Long,  2012), in this case, r esearch to understand  varied usages of ‘resilience’ within technical literature.  This systematic  analysis method has been well -used to present an overview of the technical literature and allow statistical analysis  based on category and key variables  (Sumanth, et al, 1990) . A temporal dime nsion to the research is instituted to  ascertain shifts with resilience utilization during the past ten years. The literature review approach used for this study  sampled how authors defined resilience and how they measured resilience for the systems studied. For this work, the  search considered articles published between 2006 -2016 that specifically attempt to define the term “resilience”. ",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
14,EVALUATING RESILIENCE THROUGH HETEROGENEOUSGOODNESS OF FIT PREFERENTIAL ATTACHMENT IN SCALE-FREENETWORKS,@wne.edu,"Preferential attachment, supply chain, scale free network ","In an effort to fortify networks from disruption , resilience has emerged as a major area of research  within network  science. It has been suggested by literature that the scale- free topology characterized by a power -law degree  distribution presents an inherent resilience to random disruptions. As such, it has been suggested that this topology be  adopted in a variety of ap plications including supply chain networks . This paper is a part of a larger effort to study  resilience in supply chain and evaluates whether a preferential attachment methodology maximizing the goodness of  fit of the power law degree distribution increases the resilience of a heterogeneous network. The Barabási-Albert (BA)  model is a basic methodology for  developing scale -free networks which will be used to comparatively r ate the  performance of the goodness of fit preferential attachment methodology.     In this study resilience is measured by the network’s largest connected component  as simulated disruptions are  randomly applied to the system. It will be shown that achievement of a power law distribution is not singularly  representative of network resilience . Rather, it is the interaction of nodes to form hubs in general during network  emergence that is critical.     Keywords  Preferential attachment, supply chain, scale free network    Introduction  Resilience can be defined as the ability to maintain operations and connectedness under the loss of some structures or  functions (Zhao, Kumar, Harrison, & Yen, 2011) . In a supply chain, this means the ability to  keep as many suppliers  operational as possible so that the product may continue to ship despite the presence of a disruption. In its most basic  form, the supply chain can be modelled as a network with nodes representing various supplier organizations and arcs   representing relationships such as infor mation, material, or financial flows. One key element of the supply chain is  the heterogeneity of the nodes. Although redundancies can and do exist, each supplier has its own purpose and cannot  simply link with any other supplier. Only those which serve a logical chronological purpose will attach to each other.    In the 1990’s Barabási and Albert famously discovered the scale free network naturally occuring in society  This topology is characterized by a power -law degree distribution  where degree is the number of arcs entering and  exiting a particular node (Albert, Jeong, & Barabasi, 2000). The power-law degree distribution causes a small number  of high degree nodes (hubs) to form in the network while the majority of the nodes have a small degree.  In network  science, it has been presented that the scale free topology has an inherent resilience towards random disruptions   because a disturbance is more likely to affect a small degree node rather than an important hub. As a result, literature  suggests that the scale free topology be adopted in the supply chain context.  This paper is part of a larger research  goal to quantitatively analyze the usefulness of the scale free topology in supply chains.    The Barabási-Albert model is a basic methodology for f orming scale free networks. It is dependent on two  elements: growth and preferential attachment. In this study, a new preferential attachment mechanism is presented  which uses a power-law goodness of fit method developed by (Clauset, Shalizi, & Newman, 2009). In this alternative ",wne.edu,Western New England University,United States,42.1125825,-72.51478135708936
15,THE ROLE OF THE PROJECT ENGINEER IN WORK PACKAGEBASED ENGINEERING DESIGN PROJECTS,@gmail.com,"Project engineer, work packages, engineering design project, consulting engineering, heavy industrial projects, ","This paper examines the role of  the project engineer in the detailed design phase of  heavy industrial projects being  executed by consulting engineering firms using the STBQ work package (WP)  method. The STBQ method gets its  name from the STBQ success criteria where project success is defined as  Safe, on-Time, on-Budget, and with no  Quality deficiencies.  100% of the  project requirements are fulfilled in the execution of  WPs (the 100% rule) and  each WP has STBQ criteria of its own, and therefore by simpl e arithmetic if each WP meets its  success criteria then  the project will be successful.   In the STBQ method, the WP is the basis for all planning, monitoring, controlling,  and reporting on the project.   It is also specifically designed with many features r ecommended by other researchers  and intended to be integrated with the Advanced Work Packaging best practice developed by the Co nstruction  Industry Institute. The project engineer has two main responsibilities:  interdisciplinary coordination and overall  quality control.   This paper describes how these responsibilities are fulfilled at the WP level.   The paper also  describes the project engineer's role in the optional, but highly desirable additional goal to achieve project excellence  through knowledge capture, team building, and recognition for the purpose of improving the individuals, the project,  the culture, and the firm.    Keywords  Project engineer, work packages, engineering design project, consulting engineering, heavy industrial projects,  STBQ method    Introduction  The STBQ method of project engineering management is a work package (WP) based method intended for  successfully executing the detailed engineering and design phase of heavy industrial projects (Herbst, 2016).  The  project engineer (PE) play s a key role in the utilization of the STBQ method and this paper examines the various  aspects of that role.  Specifically, the paper describes the roles and responsibilities of the PE in the following major  groups of activities:  • Requirements analysis  • Initial planning  • Near-term look-ahead planning  • Commitment planning  • Monitoring, controlling, and reporting of progress  • Safety  • Quality control  • Change management  • Knowledge management  • Team motivation  The paper first provides a brief description of the STBQ method and its intended use by consulting engineering  firms.  Then each of the above major groups of activities will be described with a focus on the PE`s role.    Background and Overview of the STBQ Method  Purpose of the STBQ method  The STBQ method was developed to provide better success in the execution of the engineering phase of heavy  industrial projects. This led to the incorporation of many recommendations of other researchers and authors ",gmail.com,,,46.3144754,11.0480288
16,THE EFFECT OF IMPEDED INFORMATION SHARING ANDTRANSPARENCY ON POWER DIFFERENTIALS ANDORGANIZATIONAL DYSFUNCTION,@odu.edu,"Organization Structure, Communication Channel, Information Flow. ","The constituents of an organization have to interact with each other. The purpose of the interaction is to maintain flow  of information, to exchange knowledge, to share individual situational awareness, and to give rise to an organizational  shared awareness. The rate of information exchange is constrained by multiple factors and impacts the effectiveness  of the communication in an organization. For instance, e xtensive communication may not result in  informativeness  all the time.  Communication channels induce constraints between the interacting agents. These constraints can impede  information flow. Impeding information flow will result in  an accumulation of knowledge with a particular agent.  Consequently, the agent who possesses the knowledge becomes the hub for the communication channels in the  organization. The ensuing powerful position impedes sharing individual situational awareness. T his negatively  influences the formation of shared situational awareness. The accumulating node becomes the de facto repository of  the organization. In so doing, it generates further barriers in an auto reinforcing construct. The accumulating node  claims experience dominance, blocking interaction among the agents and thereby restricting the flow of knowledge.  This prevents refreshing individual and shared situational awareness and can engender serious problems for the  organization. In this paper, the conceptual model of this issue is outline d, and applied t o an incident of military  fratricide.     Keywords  Organization Structure, Communication Channel, Information Flow.    Introduction  One requirement to be considered an organization for a group of people is to  interact to achieve a goal. This applies  to all contexts. For instance, in military there exist political-military goals given to the commanders. The commanders  disseminate an intent to achieve the given goal. The mere dissemination  of the intent does not make the goal  achievable. The constituents have to  make decision s, maintain awareness, interact , and fulfill the allocated  responsibilities in the organization so that the organization proceeds towards achieving the goal.  All these are  constituent dependent, yet entangled activities which need information management. Through various communication  styles the information management is maintained in an organization.  Communication is a means of interaction which allows for the flow of information. Communication includes  the combined effect of many interacting constraints. These are thought processes, perceptual-motor biases, cognition  limitations, and social pragmatic factors. The impacts of these are manifested in the context of social interaction in an  organization. For example, the rate of information exchange is constrained by  some of these factors, and impact the  effectiveness of the communication in an organization.     To improve the communication effectiveness, the constituents in an organization seek temporal solution. An  example can be extensive communication. In the context of this paper, extensive communication emerges in two ways;  1) establishing communication link besides the required nodes in the organizational structure, 2) increasing the amount  of information that is distributed in the organization.   An organizational structure characterizes  the communication  links between the constituents of an  organization. As a result, communication maintains a distribution of knowledge, situation awareness, and information  with respect t o requirements. These parameters are cognitive enabler s for the information flow.  Excessive ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
17,MERGING SYSTEM ARCHITECTURE AND SOCIAL NETWORKANALYSIS TO BETTER UNDERSTAND EMERGENT NETWORKS OFSYSTEMS,@gmail.com,,"This paper proposes a research path to adapt social network analysis tools and systems architecture to understand  complex, emergent networks of systems,  implications for system design, and impact on family and system of  systems.  The Department of Defense (DoD) creates terabytes of data during the Joint Capability Integration and  Development System (JCIDS) process for individual systems; however, by devel oping and analyzing individual  systems in isolation, analysts can potentially miss valuable insights on synergies, redundancies, or dependencies  these systems have on the overall architecture.  Traditionally, a system-of-systems perspective provided these  insights for well -defined groupings of like systems within the DoD or systems that the department developed as  family-of-systems.  However, with increasing complexity, this analysis becomes difficult as nearly all DoD systems  are connected at some level.  Additionally, systems may be connected in a variety of ways: by Joint Capability Area,  system-to-system dependencies, activities, or mission areas.  As the network of systems becomes more complex, the  second and third order effects of changes to the archit ecture may not be readily apparent.  This paper proposes  applying social network analysis techniques to better understand and measure this complex architecture of  interdependencies.  Through these techniques, the DoD can better understand the architecture and impacts that new  systems or resourcing decisions have on the entire warfighting capability enterprise.     Key Words  Joint Capability Integration and Development System, Network Analysis, System Architecture     Introduction  Over time, systems have evolved t o the point where their internal complexity and the reliance on external systems  for operations have become difficult to understand and model with traditional systems engineering tools.  Systems  architecture addresses the internal complexity and includes tools and methods that have adapted to model modern  systems.  Additionally, systems engineers have recognized the need to understand external influences and have  developed methods to better understand systems of systems and family of systems.  At the enterprise level,  architecting the enterprise to deliver value has progressed to a point that the methods are readily accepted.   However, these tools and methods are not well suited for emergent networks of systems that may be developed over  time, by separate organizations, and without regard to the overall value proposition of the network.  These networks  of systems emerge similar to social networks.   This paper presents a review of the literature for systems engineering, system architecture, and enterprise  architecture to identify potential gaps in the literature and opportunities for  future research in this area.  It examines  both system of systems and family of systems to identify opportunities for applying these methods to a larger  network of systems.  It als o summarizes how the Department of Def ense (DoD) manages current capabilities and  develops future warfighting capabilities.  In the methodology section, the paper presets the foundational basis of the  emergent network of systems approach and the potential use of social network analysis methods.   The structure of the paper first presents opportunities within both the DoD and System Engineering realm  for understandin g emergent networks of systems.  The literature review focuses on systems engineering, systems   architecture, and the Joint Capability Integration Development System.  The next section identifies several potential  methodologies for managing and understanding networks of systems to include enterprise architecture, network  analysis, and social network  analyses.  Finally, it proposes a research path forward to identify social network  analysis methods that are applicable to networks of systems as a method to better understand key systems in the  network and essential links between these systems.   ",gmail.com,,,46.3144754,11.0480288
18,THEORETICAL RELATIONSHIP BETWEEN CHALEFF’S FOLLOWERBEHAVIORS AND BLAKE AND MOUTON’S MANAGERIAL GRID: ADISCUSSION,@ecu.edu,"Leader, Follower, Communication, Managerial Grid ","One of the most common problems in industry today is that, quite often, the lack of communication between leaders  and followers can result in a disorganized and inefficient hierarchy. An inability to effectively convey ideas can lead  to missed opportunities and possibly even endanger public safety. This is an issue of particular importance in the  field of engineering because of the role engineers play in leading the way for human innovation while keeping  public safety a priority. Since every engineer is a follower at some point in their career, it is important that a  follower be able to speak openly with their leader in order to provide different v iewpoints, and perhaps even uncover  design flaws overlooked by management. Interactions between leaders and followers can, however, be diverse  depending on the type of leader. There are several models that identify different types of leaders, as well as th ose  illustrating the characteristics of followers. Two of these models include the managerial grid by Robert Blake and  Jane Mouton, and follower behaviors by Ira Chaleff. This analysis will examine any theoretical relationship between  Blake and Mouton’s managerial grid and Chaleff’s follower behaviors.    Keywords  Leader, Follower, Communication, Managerial Grid    Introduction  Leadership in the workplace is something that has been studied and researched for decades. In order for a business  to be successful, it is imperative that workers be fully utilized . It is up to leaders to help guide workers in the right  direction.  Chaleff (2009), Dixon (2003)  and Kelley (1992) identify workers who follow leaders as the followership  component of the leadership triad.  One of the largest obstacles that leadership has to overcome, however, is the kind  of leader to which followers  positively respond. Dixon, Mercado and Knowles (20 13) allude to the importance of  this for younger employees entering a workforce dominated by experienced managers and peers.  Robert Blake and  Jane Mouton addressed this very question in their book The Managerial Grid (1964). This book introduces a style  leadership model that originally classified leaders into five different styles. The model has since expanded to include  two new leadership styles, as well as a new element.  While effective leadership is necessary, it is also important to note how exactly followers fit.  Chaleff has  proffered the importance of follower role s and how  followers will and must influence  leaders. Chaleff theorizes  critical follower behaviors and how nurture them in the work force as a means of developing more effective leaders.  The dimensions o f Chaleff’s courageous followership include the courage to assume responsibility, to serve, to  challenge, to participate in transformation, to take moral action, to speak to the hierarchy, and to liste n to followers  (Chaleff, 2009).  So how do the  followership behaviors and leadership styles influence th e hierarchy of a company? This  discussion compares and contrast s theoretical relationship between Chaleff’s follower behaviors and Blake and  Mouton’s managerial grid in an attempt to describe an environment where followers are able to more effectively  voice any questions or concerns regarding the  safety or efficacy of a workplace.  Some comparisons will highlight  ways in which leaders and followers are behaviorally similar, whereas some disparities will reveal reasons for the  current disconnect between workers and management.   ",ecu.edu,East Carolina University,United States,35.63285115,-77.48526724763182
19,LAYOUT IMPROVEMENT AT A MEDICAL DEVICE REFURBISHMENTBUSINESS,@stcloudstate.edu,"Facilities planning, layout design, space optimization, regulatory requirements. ","This project was conducted at a business involved in the refurbishment of second hand medical equipment for the  purpose of resale.  The existing shop floor was restricted  with machines and equipment and was hindering the  smooth flow of man and material. The scope of the project included: a) improving shop floor layout for optimum  utilization of s pace and reduction of congestion ; b) enhancing shop floor space to accomm odate additional  workstations; c) improving layout to rearrange work desks and d) ensuring all recommendations were in compliance  with the safety regulations. Additional goals of the project included rearranging the eyewash station and meeting the  safety standards. The project was limited to the shop floor only, and excluded the rest of the facility from its scope.  The project methodology involved reviewing the shop floor layout, analyzing the business requirements and  specifications pertaining to the equipment’ s, conducting a workload analysis and reviewing the OSHA & ANSI  regulations in order to recommend improvements to the existing layout. The recommendations  resulted in a five  percent saving of s pace on the shop floor; created  space for work stations a nd desks; optimally utilized  overhead  space for storage purposes and reduced travel time by almost 40 percent.     Keywords  Facilities planning, layout design, space optimization, regulatory requirements.    Introduction  This project was conducted at a business invo lved in the refurbishment of second hand medical equipment for the  purpose of resale.  The company catered to “ made-to-order” as well as generic products and included  assembly,  installation, testing and after-sales-services. As new machines replaced old ones and more equipment was added, the  shop floor became restrictive. The company required space to accommodate additional workstations and desks while  keeping the costs low and ensuring daily production activities  were not hampered . The paper describes the  methodology used by the project team and the recommendations made to improve the plant layout for optimum  utilization of space.    Problem Statement  The company was in the business of refurbishing medical equipment for re -sale. The existing shop floor was  restrictive with machines and equipment s. This was hindering the smooth flow of man and material and had an  adverse effect on productivity.    Nature and Significance of the Problem  On an average, plant s undergo major relayouts every 18 months, due to changes i n product design, materials and  processes (Meyers &  Stephens, 2013). Improper layout can result in non-value added work and reduced efficiency   (Ong, 1997). Thus, the smooth flow of man and material helps to not only increase productivity and minimize waste   but also utilize time more efficiently. This project showed how  efficiency improvements could be acheived  through  a modified shop floor layout.  ",stcloudstate.edu,Saint Cloud State University,United States,45.55139,-94.14833
20,IDENTIFYING THE FACTORS THAT INFLUENCE THE DESIGN OFPERFORMANCE MEASUREMENT SYSTEMS IN NOT-FOR-PROFITORGANIZATIONS,@utfpr.edu.br,"Not-for-profit organizations, performance measurement systems, design, conceptual framework, systematic literature ","Not-for-profit organizations have complex characteristics related to their social goals. Legitimacy, accountability, as  well as efficient and effective use of resources are examples of performance requirements from com munity,  government and regulatory bodies. Their particularities in terms of social demand make performance measurement a  difficult task. In this context, the purpose of this paper is to identify and to classify the factors that influence the  design of perf ormance measurement system in not -for-profit organizations.  The paper describes the results of a  systematic literature review used to synthesize a conceptual framework for studying the factors that influence the  design of performance measurement systems in not-for-profit organizations. The systematic literature review results  show that performance measurement systems in for -profit organizations are incomplete for supporting not -for-profit  organizations because characteristics such as accountability and intangible results are usually not properly  considered. It is noteworthy that in some cases in the literature, performance measurement systems were adaptations  derived from for-profit models, but some authors indicate that this type of adaptation is not recomm ended for other  not-for-profit organizations. The conceptual model presented in this paper can assist practitioners in developing  design processes, observing the role that the factors play and their influence in the design of not -for-profit  organizations’ performance measurement systems.    Keywords  Not-for-profit organizations, performance measurement systems, design, conceptual framework, systematic literature  review.    Introduction  Not-for-profit organizations (NFP) are organizations that meet the collectiv e needs of specific groups and depend on  funding and donations to run their  services. Usually, a NFP  has a financial restriction , that is,  its profit cannot be  distributed, so the focus is in its social mission and social value (Valentinov, 2011).   NFP include organizations like municipal, state and federal governments, university, char ities, museums,  foundations, religious institutions, hospitals, trade unions and civil right groups (Moxham, 2009; Valentinov, 2011) . ",utfpr.edu.br,Universidade Tecnológica Federal do Paraná,Brazil,-23.30729075,-51.11425781269135
21,M,Missing,,,Missing,,,46.3144754,11.0480288
22,SENSEMAKING IN A VALUE-BASED CONTEXT DUE TOREQUIREMENTS CREEP,@iastate.edu,"Value-Driven Design, Scope creep, Cynefin Framework, Large-Scale Complex Engineered system ","The design and the development of Large -Scale Complex Engineered Systems (LSCES) requires the involvement of  large numbers of people. Traditionally these LSCES are designed using requirements -driven systems engineering  approaches, where the preferences of the stakeholders are communicated using requirements. Usually, multiple  organizational levels and interaction between the different teams are required to elicit the requirements at different  levels. Most large -scale proj ects are subjected to scope creep. The scope creep can occur at any level of the  organization and cycle back to the highest level, thereby increasing the cost and time associated with the  development of LSCES. In this paper, the authors analyze the scope creep of requirements that may occur at various  levels of the organization to determine the value gap associated with this creep. The Cynefin framework, a sense- making framework, will be used to determine the state of the system and to provide guidance in decision -making to  minimize the value gap resulting from the requirements creep . Value-Driven Design (VDD) is used to capture the  value of the system and also the Value -gap associated with scope creep. This is calculated by the time and the cost  involved in communicating the requirements from one level to another. A sample organizational structure is used to  illustrate the effects  of requirements creep on V alue-gap. Two types of system, a  simple system and a complex  system are used to demonstrate the state of the system in the Cynefin Framework and understand the effects of the  requirement creep.    Keywords  Value-Driven Design, Scope creep, Cynefin Framework, Large-Scale Complex Engineered system    Introduction:  A Large-Scale Complex Engineered System (LSCES) is enormous in size and has a large number of entities or   sub-systems, thereby increasing complexity . The complexity within the system leads to various forms of risks,  uncertainties, and ambiguity involved in various stages of the design and development of a LSCES . With increase in  complexity and size, the c osts and time taken to develop LSCES is relatively huge (C. L. Bloebaum & McGow an,  2012; P. Collopy, Bloebaum, & Mesmer, 2012). The design and development of LSCES requires the involvement of  multiple stakeholders and organizational levels.   Organizations which develop the LSCES provide facilities for the design and development. These  organizations are made of different levels and are set up i n specific way depending on the nature of the company to  accomplish different goals,  such as the interactions between different departments for a particular product, cost  reduction and innovation. T he structure of an organization may differ based on the re gion, function and product   (Harris & Raviv, 2002) . The communication between the organizations  can be direct or indirect. D irect  communication refers to the communication between the two consecutive levels of the organization and indirect  communication refers to the communication between different levels ( Kwasa, Bloebaum, Mesmer, Kannan, &  Tibor, 2015). In a LSCES, the requirements of the stakeholders usually follow a top -down path, i.e. the requirements  are made at the higher level of the organization to lower levels ( Mantel, Meredith, & Shafer, 2010 ). The elicitation  of requirements is necessary to capture the preferences of the stakeholders to design and develop the LSCES. When  there is a change in a requirement in any level of the organization, or when new requirements are added on to the  LSCES, the requirement creep or the scope creep sets in (Sun, 2004).     ",iastate.edu,Iowa State University,United States,42.0279608,-93.64473746093857
23,CAPTURING ORGANIZATIONAL UNCERTAINTY IN A VALUE-BASEDSYSTEMS ENGINEERING FRAMEWORK,@iastate.edu,"Systems Engineering, Value -Driven Design, Multidisciplinary Design Optimization, Organization Design, ","Designing Large -Scale Engineered Systems (LSCESs) has evolved into an effort that demands large parent  organizations that are made up of several decision making teams. These organizations are often geographically  dispersed adding to the complexity of the development of these LSCESs.  These organizations have structures that  provide the platform  for the design process. These design processes affect the products on which they are applied.  Previous work on the design of LSCESs has demonstrated the improvements  in engineered systems design  where  Value-Driven Design ( VDD) is adopted by capturing stakeholders’ preferences in value functions when using  Multidisciplinary Design Optimization (MDO)  frameworks to design complex systems.  Along with these two areas,  Organization Design has recently been incorporated in the evaluation of deterministic systems to capture  stakeholders’ value of systems where both the product and organization characteristi cs are used to create the value  functions that are used to enumerate LSCESs’ value. This work explores uncertainty in organization structures as  part of the uncertainty that exists within organizations such as they do on engineered systems and the environm ents  in which they operate and interact. An example system with characteristics present in LSCESs is used to  demonstrate this application  of uncertainty. This paper demonstrates that uncertainty in organization structures do  affect the outcome of systems subjected to them.     Keywords  Systems Engineering, Value -Driven Design, Multidisciplinary Design Optimization, Organization Design,  Uncertainty    Introduction  Increasingly, engineered systems that are developed can be considered as Large- Scale Complex Engineered Systems  (LSCESs). These systems are characterized, in part, by the numbers of parts that make up the systems, the  complexity of the interactions that are present within the systems, the complexity of the interactions that these  engineered system s have with other LSCESs, the large costs associated with their design, development and  retirement as well as the extensive development time necessary to realize these systems (C. L. Bloebaum, Collopy, &  Hazelrigg, 2012) . LSCESs are designed to address the complex problems encountered in society today. This is  accomplished by designing these systems to interact with other systems such as aircraft carriers  needing to interact  with military aircraft which allow for the succes sful execution of objectives  to accomplish specific missions.   LSCESs are not only companions to other systems but also exist as parts of much larger systems. This is  demonstrated by the national power grid which is a LSCES and has other LSCESs as part of its infrastructure.  Engineered systems such as hydroelectric dams, geothermal power plants, nuclear plants and wind energy farms  form the part of the national power grid that is responsible for the generation of electricity . The success of these  LSCESs is d ependent on the conglomerate of hundreds or even thousands of people that form the decision making  units of the organizations as either individuals or teams to achieve a shared goals (Baligh, 2006). LSCESs can have  multiple organizations involved in their  development process es such as when sub- contractors are tasked with  development of certain portions of systems.   Organizations that develop LSCESs are designed to provide infrastructure that facilitates the design and  development of these systems.  Parts of this infrastructure in the organizations include collaborative efforts amongst  teams of decision makers and individual decision makers. The collaborative effort within and between organizations ",iastate.edu,Iowa State University,United States,42.0279608,-93.64473746093857
24,ETHICS AND ROBOTICS,@odu.edu,"Robotics, Machine Intelligence, Ethics ","New advancements in technolo gy offer both improvement in quality of life and unintended change in society.   Autonomous machines have rapidly increased their presence and use in the 21st century. Recently, these devices  have even exhibited the ability for self -improvement in the form of machine learning. Robots have yielded  satisfactory results through applications in information and systems security, environmental engineering, medical  care, and military warfare by performing tasks that surpass the capability of human beings, activitie s that appear to  be dangerous to humans, or simply jobs that are monotonous or time-consuming.    Although the field of robotics offers tremendous efficiency and the promise of quality work across a broad  spectrum of domains, a variety of ethical implications must be considered before wider adoption of the use of robots.  Thus, through machine learning, an embedding of moral values into the technologies that are produced should be  carefully considered.   This paper will describe machine learning algorithms and  performance evaluation measures for machine  learning solutions, the ethical impact of areas in which these solutions are implemented, and the progress of research  into the creation of moral machines.     Keywords  Robotics, Machine Intelligence, Ethics    Introduction  Advances in information technologies have given rise to many technological innovations that have paved the way for  the spread of robots in today's world. The magnitude of this phenomenon is exemplified  by the year 2010, during  which more than 12,000 robotic machines were employed by the US military, and companies such as Google created  computer-driven cars that were already on the road (Guterman, 2012). With this progression, it is only natural that  artificial intelligence will play a more prominent role in civil life, and that the world will witness a rampant growth of  robotics in the near future.  The Robot Institute of America (1979) states that a robot is “a reprogrammable, multifunctional  manipulator designed to move material, parts, tools, or s pecialized devices through various programmed motions for  the performance of a variety of tasks” (as cited in Hemal, 2011, p. 3.). Bekey (2005) gave a more detailed definition:   “a [machine that] must have sensors, processing ability that emulates some asp ects of cognition, and actuators.  Sensors are needed to obtain information from the environment. Reactive behaviors (like the stretch reflex in  humans) do not require any deep cognitive ability, but on -board intelligence is necessary if the robot is to per form  significant tasks autonomously, and actuation is needed to enable the robot to exert forces upon the environment.  Generally, these forces will result in motion of the entire robot or one of its elements (such as an arm, a leg, or a  wheel)” (p. 2).  By these definitions, it should be noted that a robot is not a computer that is equipped with the feature of  automatically ejecting a CD, a toaster, a coffee maker, or any other ordinary device. A robot is an autonomous  machine that senses, thinks, acts, and interacts with the external environment.  The notion of thinking means that the  machine is “able to process information from sensors and other sources, such as an internal set of rules, either  programmed or learned, and to make some decisions autonomously” (Lin et al., 2011, p.18) In addition, autonomy in  robots reflects their “capacity to operate in the real -world environment without any form of external control, once the  machine is activated and at least in some areas of operation, for extended periods of time” (Lin et al., 2011, p. 943).   ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
25,PERFORMANCE MEASUREMENT SYSTEMS IN NONPROFITORGANIZATION: A BIBLIOMETRIC ANALYSIS,@udesc.br,"Systematic Literature Review, Nonprofit Organizations, Bibliometric Analysis, Performance Measurement. ","The growing number of nonprofit organizations increases the competition for funding. As a result, some nonprofit  organizations are applying entrepreneurial strategies and business models to become more competitive and  more  transparent. Performance Measurement (PM) systems can be a solution not only for the management issues of such  organizations but also for their external control. The evolution of PM systems made available several tools and  instruments that are useful to for -profit companies, but  seem to be limited in their application to nonprofit  organizations. There are some unique features of these organizations that make the adaptation of for-profit PM systems  approaches to nonprofit organizations insufficient and difficult. This paper presen ts the findings from a systematic  literature review on PM systems in nonprofit organizations. Techniques such as bibliometric analysis are applied to  describe current research themes related to these subjects. Hence, the findings of this paper contribute t o a  comprehensive view of the current literature on PM systems and nonprofit organizations.     Keywords  Systematic Literature Review, Nonprofit Organizations, Bibliometric Analysis, Performance Measurement.    Introduction  A performance measurement (PM) syste m could be described as a set of processes that transforms mission, strategy  and organizational goals in key performance indicators and critical success factors to contribut e to organizational  actions (De Waal, 2007, p. 19).  Also, PM systems can be seen as a multi-dimensional set of performance measures   for planning and managing a business (Bourne, Neely, Mills, & Platts, 2003).   The majority of the tools and models for performance management have been developed in for -profit  companies (Ospina, Diaz & O’Sullivan, 2002). However, a PM system may be useful to nonprofit organizations as  well. For Austin (2000), the number of nonprofit organizations is increasing, especially because of the growing number  of complex social problems that needs to be addressed. Con sequently, there is a growing competition for funds. In  this context, notions of performance management and measurement can be a solution not only for the management  issues of such organizations, but also for their external control (Straub, Koopman & Mossel, 2010). Also, political ",udesc.br,Universidade do Estado de Santa Catarina,Brazil,-27.5857278,-48.50298683778315
26,"TEAM COACHING, KNOWLEDGE WORKERS AND VIRTUAL TEAMSIN THE CONTEXT OF FUNCTIONS, CONDITIONS ANDINTERVENTIONS",@gmail.com,"Coaching, virtual teams, distance learning, collaboration ","Peter Drucker created the term ""knowledge workers"" to designate workers that use information and their own  knowledge more so than any physical attribute.  Enginee rs are a prime example of knowledge workers and the  discipline of engineering management is built around the challenges faced in leading this type of worker. For  example, engineers frequently work within team structures that span functional areas and formal skill sets. For this  reason, effectively leading cross-discipline teams can be difficult. One -on-one mentoring or coaching fails to  consider the different facets that are present in integrated process teams. The nature of the work suggests a different  approach is needed to lead knowledge workers. An emerging trend to consider in this setting is group or team  coaching. Group coaching can foster trust and commitment, support effective problem solving and bring about a  heightened sense of ownership within the team, which also benefits the organization. Further study is needed to  determine if this approach significantly affects overall team productivity. How engineers, for example, learn outside  of the team and outside of the organization deserve to be conside red. More specifically, effectively leading  engineering teams needs to be analyzed further in the context of cross -functional collaboration. The key is to better  understand how engineering teams operate, how team coaching functions and how team coaching can apply to  knowledge workers.    Keywords  Coaching, virtual teams, distance learning, collaboration    Introduction  Organizations are turning to virtual teams to conduct project work and maintain day -to-day operations for a myriad  of reasons. Virtual teams are typically designed based on organizational initiatives such as cutting costs, a lack of  local talent, or even a response to demands for more flexible working conditions. Virtual teams also provide a model  for organizations to create and improve strategic partnerships with other organizations (Purvanova, 2013). Nearly  every industry can leverage virtual teams to improve and expand the operation of functions that do not require a  direct physical presence. For example, while it is difficult to imagine assembling automobiles by a team that is not  colocated, it is possible to see automotive supply chain or distribution teams meeting virtually to conduct business.   The current trend toward virtual teams has not necessarily resulted in a high percentage of successful teams. Work is  often directed toward virtual teams under the false assumption that virtual teams function in a manner similar to  their  colocated counterparts (Hackman & Wageman, 2005). This mindset has created an environment where virtual teams  are historically poorly managed, as a virtual team’s effectiveness requires far more than the traditional shared file  system and a conference bridge line. Specific challenges can be found in the virtual framework, where a lack of  synergy with other virtual teammates creates a culture where team members mistrust and avoid collaborating with  other team members (Johri, 2012). Virtual teams also suffer from an in ability solve problems also suffers due to a  lack of communication creating an overall lack of both accountability and sense of ownership.  Virtual teams have been found to be a cost -efficient means for organizations to increase their global  competitiveness. Organizations have seen significant savings through reduced travel expenses, meeting time, ",gmail.com,,,46.3144754,11.0480288
27,EVALUATION CRITERIA (CASE STUDY: A CONSTRUCTIONPROJECT-BASED COMPANY),@gmail.com,"Supplier evaluation criteria, c onstruction company, cause-effect analysis, Multiple Criteria Decision Making  ","This paper provides a framework for the criteria that can be considered to evaluate the performance of suppliers of  an international construction project -based company located in the United States . Through a comprehensive  literature review, capturing historical data from the case company, individual meeting with the case companies’  experts, and also holding several brain storming meetings with the experts in the area of supply chain, a list of  categorized criteria is  derived. The importance of the criteria and their relationship are asked from the experts via  questionnaires. The data obtained from questionnaires are statistically analyzed and the most significant criteria are  divided into several categories. Cause and effect analysis is conducted to investigate the interrelation effects among  criteria. After obtaining the weights of the criteria, a multiple criteria decision making methodology is proposed to  evaluate the potential suppliers for each project. The result is the list of weighted categorized criteria and a proposed  methodology to evaluate the suppliers.    Keywords  Supplier evaluation criteria, c onstruction company, cause-effect analysis, Multiple Criteria Decision Making   (MCDM). DEMATEL, ANP.    Introduction  Supply chain management (SCM) has become an important tool for increasing competitive advantages. Supplier  selection and evaluation has become a core competency for many world-class companies who want to stay ahead of  their competitors. S electing the best suppliers contains much more than scanning the price list, it depends on an  extensive variety of criteria which involve both quantitative and qualitative.  There are a variety of qualitative and  quantitative criteria for supplier evaluatio n and selection in the literature. Some papers in which supplier evaluation  criteria are investigated can be mentioned as: Wilson (1994), Swift (1995), Choi & Hartley (1996), Ghodsypour &  O'Brien (1998), Choy & Kee (2002), Jabbour & Jabbour (2009), and Xia & Wu (2007).   Lima Junior et al. (Lima, Osiro, & Carpinetti, 2014) reviewed the recently published papers which were  about the criteria for supplier evaluation and selection. They listed the most commonly used criteria for supplier  selection as presented as follows: 1) technical capability, 2) quality, 3) response to change, 4) cost/price, 5) financial  situation, 6) reputation, 7) easy of communication, 8) on time delivery, 9) relationship, 10) product performance,  11) after sale warranty, and 12) geographic location.  The supplier selection and evaluation process is considered as one of the most important components in the  supply chain management. Several methods have been developed for the supplier selection process. The most  commonly used methods can be listed as follows: the Analytic Hierarchy Process (AHP) (Chan, 2003), the Analytic  Network Process (Gencer & Gürpinar, 2007), Data Envelopment Analysis (DEA) (Noorizadeh, Mahdiloo, & Saen,  2012), and Mathematical Programming (MP) techniques (Chaudhry, Forst, & Zydiak, 1993).  In addition to the mentioned methods used to rank the suppliers, there are some methods in the literature  that have been used to weight the criteria. One of the most effective approaches in weighting the criteria is ANP  method which has been used in variety of research for obtaining the weight of the criteria for supplier evaluation.  Some examples include Sarkis & Talluri (2002), Bayazit (2006), and Gencer & Gurpinar (2007).   Copyright, American Society for Engineering Management, 2016 ",gmail.com,,,46.3144754,11.0480288
28,SOLAR POWER FORECASTING USING SUPPORT VECTORREGRESSION,@uncc.edu,"Solar power forecasting, support vector regression, weather variables. ","Generation and load balance is required in the economic scheduling of generating units in the smart grid. Variable  energy generations , particularly from wind and solar energy resources , are witnessing a rapid boost, and, it is  anticipated that with a certain level of their penetration, they  can become noteworthy sources of uncertainty. As in  the case of load demand, energy forecasting can also be used to mitigate some of the ch allenges that arise from the  uncertainty in the resource. While wind energy forecasting research is considered mature, solar energy forecasting is  witnessing a steadily growing attention from the research community. This paper presents a support vector  regression model to produce solar power forecasts on a rolling basis for 24 hours ahead over an entire year, to mimic  the practical business of energy forecasting. Twelve weather variables are considered from a high -quality  benchmark dataset and new variables  are extracted. The added value of the heat index and wind speed as additional  variables to the model is studied across different seasons. The support vector regression model performance is  compared with artificial neural networks and multiple linear regression models for energy forecasting.    Keywords  Solar power forecasting, support vector regression, weather variables.    Introduction  In a spectacular year for the U.S. s olar Photovoltaic (PV) industry, nearly 1 MW was installed almost every hour in  2015. The cumulative solar PV installed capacity in the United States has now surpassed 27 GW. Revenue generated  from installations of solar PV reached an estimated $22.6 billion in 2015, a 21% increase over 2014, and growth of  174% since 2011 (Economy Advanced Energy, 2016).  The wind and solar energy resources have created operational challenges for the electric power grid due to  the uncertainty involved in their output in the short term. The intermittency of these resources may adversely affect  the operation of the electric grid when the penetration level of these variable generations is high. Thus, wherever the  variable generation resources are used, it becomes highly desirable to maintain higher than normal operating  reserves and efficient energy storage systems to manage the power balance in the system. The operating reserves  that use fossil fuel generating units should be kept as low as possible to get the highest benefit from the deployment  of the variable generations. Therefore, the forecast of these renewable resources becomes a vital tool in the operation  of the power systems and electricity markets (Morales, Conejo, Madsen, Pinson, & Zugno, 2014).  As in wind power forecasting , solar power forecasting also consists of a variety of methods based on the  time horizon being forecasted, the data available to the forecaster and the particular application of the forecast. The  methods are broadly categorized according to the time horizon in which they generall y show value. Methods that are  common in solar forecasting include Numerical Weather Prediction (NWP) and Model Output Statistics (MOS) to  produce forecasts, as well as hybrid techniques t hat combine ensemble forecasts and Statistical Learning Methods  (Tuohy et al., 2015) . In solar power forecasting up to 2 -hours ahead, the most important input is the available  observations of solar power, while for longer horizons NWP model becomes crucial for accurate forecasts (Kleissl,  2013). Applying machine learning techniques directly to historical time -series of PV production associated with  NWP outcomes have been placed among the top models in the last global competitio n of energy forecasting,  GEFCom2014 (Hong et al., 2016) . Moreover, the empirical study in (Sayeef & West, 2014)  presents the errors of  the solar power forecasting follow a non -parametric distribution, therefore, using non -parametric methods such as  machine learning techniques can yield a better accuracy.  Support Vector Regression (SVR) is extended from support vector machines, a popular tool in machine  learning. Time-series predictions by using SVM for load forecasting are explored in (Sapankevych & Sankar, 2009) . ",uncc.edu,University of North Carolina at Charlotte,United States,35.311795,-80.741203
29,IMPROVEMENT PROCESS: A CASE STUDY,@gmail.com,"Value stream map, lean manufacturing, continuous improvement, value-adding, non-value-adding activities. ","In recent years, Value Stream Mapping (VSM) has  been the preferred approach to implement and support the lean  concepts. Lean manufacturing has been extensively applied in many industries for its effectiveness in continuously  improving productivity, product quality, and on- time delivery to customers. With this in mind VSM is the main tool  used to identify the opportunities for improvements , and helps organizations find non- value added elements in their  processes.   This paper presents a practical case study of lean implementation at a project -based construction company  located in the United States. The study begins with collecting information such as material flow, cycle time , and  changeover time. A current value-stream map is created to document activities on the production floor. The goal of  value stream map is to illustrate the potential opportunities that lie within the  value stream (i.e., reduced production  lead-time and lower work-in-process inventory).     Keywords  Value stream map, lean manufacturing, continuous improvement, value-adding, non-value-adding activities.    Introduction  Most of the world- class manufacturing industries have been faced with increasing amounts of pressure from  competitors and customers for the past few d ecades. Customers demands higher expectations from their purchases,  and manufacturers need to meet these expectations to stay ahead of their competitors by increasing a product’s  quality and reliability, reducing delivery time, and minimizing product costs , or a combination of them (George ,  2002). This has made the manufacturing companies to implement new production strategies to increase their  competitiveness in the global marketplace.    Recent practical process improvements show that lean manufacturing co ncept has been applied as one of  the initiatives by many major manufacturers in order to remain competitive in the global market. Since the generic  process management philosophy has been derived from the Toyota production system (Womack, Jones, & Roos   1990), many efforts are made in manufacturing industries by focusing on reduction of the wastes and improving  overall customer value. Meanwhile, the modern manufacturers have tried to strive for lean in order to reduce  inventory, production lead time, direct a nd indirect labor, storage requirements, quality costs and material costs  (Moore, 2004; Agarwal, Shankar, & Tiwari, 2006; Shah & Ward, 2007).  Value stream map concept has been applied extensively in many practical cases.  Venkataraman, Ramnath,  Kumar & Elanchezhian (2014) applied the lean manufacturing techniques in the crankshaft manufacturing system at  an automotive manufacturing plant located in south India to increase the export sales. The results of implementing  lean manufacturing system were reduction of manufacturing lead time by 40 %, reduction of defects, higher process  capability, and quick response to the customer demand.   Sundar, Balaji, & K umar (2014) investigated the implementation system sequence of lean elements in the  unstable business environment. The finding of this review was synthesized to develop a unified theory for  implementation of lean elements.   Rahani & Al-Ashraf (2012) developed a current and a future value stream map to design a lean process  flow through the elimination of the root causes of waste and as ultimate goal, improve the process. They  demonstrated the VSM technique and its application on a product.   Copyright, American Society for Engineering Management, 2016 ",gmail.com,,,46.3144754,11.0480288
30,INTRAPRENEURIAL PROCESS FOR INDUSTRIAL CORPORATIONSFORMING NEW VENTURES,@gmail.com,"Intrapreneur, New Venture, Process ","Corporations that foresee diminishing markets for their traditional goods and services must find new products or  markets to stay competitive.  Technology based industrial firms with production assets usually foster a culture of  continuous new product improvement and development, but this will not solve the problem of a diminishing m arket,  e.g. Defense.  Future prosperity of the entity may depend upon either entering or creating new markets.  The goal is  to develop new systems that leverage assets, corporate culture, and also, address the prevailing public context.   Key corporate asset s are mid -level engineers who have both technical and management savvy, and also,  understand the relationship between career and corporate success.  They are the ideal candidates to form groups of  intrapreneurs to use the resources of their corporate entit y to develop and implement innovative concepts.  This is  often done for new products anyway, but to enter new markets with a particular competitive context, or to develop  systems to serve emerging markets, a fresh, lean, new venture subdivision might be re quired.  This paper describes a  new process for industrial corporate engineers taking a concept, and determining if it is viable to propose to senior  leadership to form a new intrapreneural venture within the corporation.  Proposals involving consideration of  technical, managerial, social and political elements leveraging long -term system operation solutions are emphasized.   Since competitors realize similar opportunities, understanding the advantages and shortfalls of corporate culture is  critical.  .    Keywords  Intrapreneur, New Venture, Process    Introduction  In a comprehensive study of more than six million U.S. firms, Stubbart and Knight noted that only a tiny fraction of  firms live to age 40.  For example, for firms founded in 1976, only 10% survived 10 y ears later, leading them to  conclude that “Despite their size, their vast financial and human resources, average large firms do not ‘live’ as long  as ordinary Americans” (O’Reilly et. al, 2009).  Another statistic shows that one- third of the firms in the F ortune  500 in 1970 no longer existed in 1983.  Studies of organizational mortality have revealed that large firms have an  estimated residual life expectancy from 5.8 to 14.6 years.  Given large firms experience, financial capabilities, core  competencies, s trategic assets, etc. –  why aren’t large firms more successful?  And what have the firms that do  survive, do differently from the norm? (O’Reilly et. al, 2009).  The difference points to Intrapreneurship and the  ability to explore and capitalize on new ven tures that are strategically aligned with the company’s core  competencies.  Corporations experiencing diminishing traditional markets must reposition themselves to develop new  markets to stay competitive.  Forward -thinking corporations rely on internal entr epreneurial efforts to alter an  organization's status quo, harness the energies of talented renegades, and give sponsorship to promising businesses  that are unrelated to the company's cash cow (Tarkahashi, 2000).  Many mid -level engineers within such compa nies  are both technically and management savvy, and also understand the relationship between career and corporate  success.  They are ideal candidates to form groups of intrapreneurs to develop and implement innovative concepts  using the corporate resources of a large industrial entity in a fresh, lean new venture subdivision.  ""Look back at any ",gmail.com,,,46.3144754,11.0480288
31,FINANCIAL VIABILITY ANALYSIS OF AN ENGINEERING DESIGNTEAM,@vt.edu,"Cost management, student competitions, financial forecasting ","The Hybrid Electric Vehicle Team of Virginia Tech is an automotive engineering design team that competes in  Advanced Vehicle Technology Competitions, sponsored by the U.S. Department of Energy and managed by Argonne  National Laboratory. The team is comprised of graduate and undergraduate students who work together to reengineer  a production vehicle to reduce its environmental impact while maintaining its marketability. The current competition,  EcoCAR 3, which began in August 2014 and will conclude in June 2018, challenges each team to reengineer and  rebuild a 2016 Chevrolet Camaro. As a university affiliated student team  that relies exclusively on donations and  sponsorships, the Hybrid Electric Vehicle Team must properly manage its financial resources to realize its ambitious  goal of building a hybrid electric vehicle that meets numerous requirements and technical specifications. During the  first two years of EcoCAR 3, the Hybrid El ectric Vehicle Team spent $85,145, and the team is projected to spend  another $87,201 before the completion of EcoCAR 3. To understand the economic viability of the Hybrid Electric  Vehicle Team, a detailed record-keeping strategy was established during the first year of EcoCAR 3 to gather financial  data, which is then used to forecast future income and expense patterns. Using these projections, the Hybrid Electric  Vehicle Team concludes that the team will need to either reduce expenses or increase fundraising efforts to minimize  the EcoCAR 3 deificit. This cost management strategy is giving its leadership better insight into its economic future   and affording the team a chance to create mitigation strategies.    Keywords  Cost management, student competitions, financial forecasting    Introduction  Since 1994, the Hybrid Electric Vehicle Team (HEVT) of Virginia Tech has participated in Advanced Vehicle  Technology Competitions (AVTCs), a series of collegiate automotive engineering competitions sponsored by the U.S.  Department of Energy and managed by Argonne National Laboratory. The AVTC’s mission is to offer an unparalleled,  hands-on, real world experience to educate the next generation of automotive engineers. In these competitions,  students work together on univers ity teams to reengineer production vehicles to reduce the environmental impact  while maintaining the vehicle integrity.  HEVT is currently competing in EcoCAR 3, the latest AVTC, which challenges 16 university teams to  reengineer a Chevrolet Camaro with the goal of reducing its petroleum energy use and greenhouse gas emissions while  maintaining the safety, performance, and consumer appeal of the vehicle. The competition began in the fall of 2014  and will conclude in the spring of 2018. HEVT is well -known at Virginia Tech, due to its sustained success and  considerable alumni history; over 600 undergraduate and 28 graduate students have participated during its  22-year  history.  As AVTCs grow in scope and technical complexity, formal project management techniques  become more  critical to ensure success. At the beginning of EcoCAR 3, the HEVT Project Manger (PM) implemented a cost  management strategy that is designed to gather detailed financial data with the goal of understanding team financial  viability so that risk mitigation strategies can be implemented as necessary. This paper details the team cost ",vt.edu,Virginia Tech,United States,37.22192675,-80.42728184013652
32,PERFORMANCE MEASURES,@oregonstate.edu,,"One of the issues in putting the Triple Bottom Line (TBL) into practice is the measurement of each of its three  categories: social, environmental and economic. The social aspect of TBL lacks a uniform co nceptual framework  for assessment –  i.e. identification and measurement of social impact. In the literature, there are relatively few  articles that discuss how to evaluate the social impact of projects. In this paper, a uniform system of measures is  suggested to help scholars and practitioners complete effective benchmarking. The study includes a holistic  comparison between different social impact performance measurement systems discussed in the literature. This  study is a first step toward building an inte grated performance measurement system that utilizes the advantages of  the primary models developed in the literature. A path is identified for scholars to better understand social impact  performance measurement systems to build on for future research. Addi tionally, implementation guidelines are  provided for the practitioner seeking to implement a social impact performance measurement system in an effective  and efficient way.  This work investigates a strategy for an organization’s social assessment by identifying causal relationships between  different social impact performance measures categories, using Decision Making Trial Evaluation Laboratory  method (DEMATEL).     Introduction   The Triple Bottom Line (TBL) is a framework to assess organizational performance  through economic,  environmental, and social impacts (Norman & MacDonald, 2004). As such, the TBL acts as a support for achieving  sustainability goals (Hall, 2011). While there are several defined tools to measure the economic and environmental  impacts, social impact is often trivialized. Social impact refers to any intended or unintended change to the quality  of life at the individual, community, or societal level resulting from the actions of an organization ( Norman &  MacDonald, 2004). Potential drivers o f social impact include project development, policy change, organizational  growth, and nonprofit initiatives. Various studies have stated that it is difficult to measure the social impact of an  organization since the definition of performance measures is l imited (Ullmann, 1985; Burdge, 1987; Grieco,  Michelini, Iasevoli, 2015). The lack of performance measures has been addressed by informal and industry -specific  studies performed by independent organizations. As a result, there is variation in measurement pr actice across  industries (Franks & Vanclay, 2013). By synthesizing current performance measures found in the literature, this  study aims to create an updated and more cohesive set of measures to be applied in practice. The Decision Making  Trial Evaluation Laboratory method (DEMATEL) is utilized to identify causal relationships between distinct  categories of social impact performance measures.   It is assumed that environmental impact assessment will be conducted separately by organizations. As  such, environmental impacts of organizations will be considered only through changes to the health of community  members. Similarly, while social and economic impacts are interrelated, economic performance measures will only  be considered in terms of their social consequences. This paper will focus solely on measuring the social impact of  organizations.      Copyright, American Society for Engineering Management, 2016 ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
33,ANALYTICS IN POST-GRANT PATENT REVIEW: POSSIBILITIES ANDCHALLENGES (PRELIMINARY REPORT),@uncc.edu,"patent management, litigation, text analytics, semantic search ","Recent analysis of litigation outcomes suggests that nearly half of the patents litigated to judgment were held invalid.  Commonly available patent search software is predominantly keyword based and takes a “one-size-fits-all” approach  leaving much to be desired from a practitioner’s perspective. We discuss opportunities for using text mining and  information retrieval in the domain of patent litigation. We focus on post- grant inter partes review process, where a  company can challenge the validity of an issued patent in order, for example, to protect its product from being  viewed as infringing on the patent in question. We discuss both possibilities and obstacles to assistance with such a  challenge using a text analytic solution. A range of issues need to be overcome for semantic search and analytic  solutions to be of value, ranging from text normalization, support for semantic and faceted search, to predictive  analytics. In this context, we evaluate our novel and top performing semantic search solution.  For experiments, we  use data from the database USPTO Final Decisions of the Patent Trial and Appeal Board. Our experiments and  analysis point to limitations of generic semantic search and text analysis tools. We conclude by presenting some  research ideas that might help overcome these deficiencies, such as interactive, semantic search, support for a multi - stage approach that distinguishes between a divergent and convergent mode of operation and textual entailment.     Keywords  patent management, litigation, text analytics, semantic search    Introduction  With patent data being publicly available and practitioners trained to think in terms of search, a number of solutions  exist in the market that pr ovide patent research and analysis services (Outsell, 2014). However, the potential of  multistage semantic search is yet to be fully realized in this domain, particularly in high value areas   of patent  litigation and licensing work. Commonly available patent search software is predominantly keyword-based and takes  a “one -size-fits-all” approach for practitioners across areas as diverse as prosecution, licensing, portfolio  management or litigation. A popular litigation support tool, LexMachina.com, now part of LexisNexis shows several  relevant statistics, but is yet to integrate semantic features that could more closely align with the work flow of  litigation practitioners in the field. Recent analysis of litigation outcomes suggest that ""nearly half of all patents  litigated to judgment were held invalid"" (Allison et al. 2014). Furthermore, the need for more thorough research and  preparation of quality patents is perhaps as strong as ever: US Patent quality appears to be lagging international peers  and the USPTO initiated its quality improvement initiative with pilots announced in May 2016. Semantic search will  potentially help quality improvement in select pilot areas, for example - determining whether similar claims are being  treated dissimilarly in different art units.   The purpose of this paper is to illustrate the gap between practitioner requirements and current search  technology in select tasks, for example to understand licensing opportunities and provide litigation support. Given  the known limitations of commercial tools, we will focus on emerging technologies of semantic search and text  analytics. For our tests of semantic search we use a collection of patents explicitly mentioned in decisions of the ",uncc.edu,University of North Carolina at Charlotte,United States,35.311795,-80.741203
34,APPLYING COMPLEXITY SCIENCE TO ANALYZE ORGANIZATIONALRESILIENCE,@systems-geometry.com,"Resilience, Organizational Resilience, Complex Adaptive Systems, Complexity Science , Environmental Turbulence, ","The growth of technology and innovation has allowed organizations to globalize their operations at increasing  rates.   While providing firms with expanded partnership opportunities, it has also left them more vulnerable to disasters given  their increased exposure along key elements of the supply chain.  For example, Intel claimed a $1 billion loss in sales  due to a reduction in computer manufacturing after floods in Thailand caused a shortage in hard drives needed to build  the machines.  In another situation, a fire in a semiconductor plant in New Mexico caused a firm’s major customer to  claim $400 million in lost potential reven ue.  Today’s organizations, with the complex relationships they maintain  with suppliers and customers, exhibit characteristics of c omplex adaptive systems (CAS).  Maintaining performance  in such an environment requires resilience in the expanded organization.  Analytical methods developed among researchers in complexity science can provide insight into the nature  of resiliency in a global organization.  This provides managers with critical information on where to focus their  mitigation activity to reduce the  impact of adverse events.  This paper explores techniques used in CAS and their  application to organizational resiliency to reduce vulnerabilities and maintain performance despite environmental  shocks.    Keywords  Resilience, Organizational Resilience, Complex Adaptive Systems, Complexity Science , Environmental Turbulence,  Disaster.    Introduction  Environmental changes and their impact on biological organisms, humankind, structures, systems, networks and  businesses have been topics of research study for years (Baum & Oliver, 1991; Bhamra, Dani, &  Burnard, 2011;  Holling, 1973; Kantur & Iseri -Say, 2015; Perrow, 1984; Richardson, 2002; Rochlin, La Porte, & Roberts, 1984) .  Change is inevitable (Coulson-Thomas, 2009).  What’s important is how organizations plan for and adapt to different  types of change which in turn alters the impact on the organization itself (Acar & Winfrey, 1994) .  There are three  categories of changes affecting organizations that have been studied  (Ross, 2010).  First - there are changes that are  everyday common changes but very important due to the incremental impact that occurs over time (Cooke & Rohleder,  2006; Perrow, 1984; Petersen, Boer, & Gertsen, 2004; Rijpma, 1997).  Second – there are the disruptive changes that  impact a critical component important to the organization (Chun Wei, 2008; Ivanov, Sokolov, & Dolgui, 2014; La  Porte, 2006; Roberts & Bea, 2001).  Third – there are changes that are so impactful, they transform the organization   (Comfort, Sungu, Johnson, & Dunn, 2001; Lengnick- Hall & Beck, 2005; Ross, 2010) .  In all cases of change, the  organization must adapt and be resilient to environmental forces  (i.e., economic shock, economic downturn,  competition, information security breach)  in order to survive.  To do this, the organization must have the ability to  interpret situations, devise new ways to deal with the situations and mobilize the resources needed  (Lengnick-Hall &  Beck, 2005; Richtner & Lofsten, 2014).  In this world of increasing technology and globalization - the world has become more complex.   Organizations have become more dependent on their global supplier network and environmental change is occurring  at increasing rates - making it more diffi cult for organizations to be resilient to the varying environmental forces.   Global expansion in supplier networks have enabled organizations to develop efficiencies during normal operations.  ",systems-geometry.com,,,46.3144754,11.0480288
35,ASSESSING MANAGEMENT STRATEGIES FOR INTERMEDIATEDREGIONAL FOOD SUPPLY NETWORKS,@iastate.edu,"regional food supply chain management, disintermediation, agent-based modeling,  reinforcement learning ","Consumers are increasingly seeking fresh and healthy food that has been sustainably produced by regional farmers.   However, most consumers also value convenience and efficiency and prefer to purchase food from retailers and  restaurants, rather than farmers’ markets.  Likewise, many farmers would rather focus their time and efforts on food  production, leaving logistics and marketing to others. Regional food hubs can bridge this gap by providing  aggregation, warehousing, transportation, and marketing services for these farmers, enabling them to reach larger  markets through sales to wholesale and retail customers.  These services can be invaluable to s mall and midsized  farmers, who often cannot perform such activities cost -effectively.  However, once food hub m anagers have helped  to establish connections between farmers and customers, they often find themselves cut out of the regional food  supply network when the farmers decide to sell their products directly to the customers, thereby avoiding the food  hub’s service fees.  While this can have short -term financial benefits for the farmers, widespread disintermediation  can eventually lead to food hub failure, which can disrupt the entire regional food system.  To avoid this, food hub  managers must develop and imple ment policies that will support long -term and mutually beneficial relationships  with their suppliers and customers.  This paper describes an agent -based model of an intermediated regional food  supply network in Iowa.  The model is designed to serve as a decision support tool for food hub managers, allowing  them to simulate the effects of various supply chain management strategies on long -term organizational and system  success.    Keywords  regional food supply chain management, disintermediation, agent-based modeling,  reinforcement learning    Introduction and Background  Consumers are increasingly demanding regionally- produced food due to its perceived quality and healthfulness, as  well as its ability to assuage the environmental and social sustainability concerns that consumers may have about  production practices in  conventional food supply chains . Adams and Adams (2011)  found that m any c onsumers  value these benefits enough to pay higher prices for regionally produced  food than they would pay for the same  conventionally-produced items. However, the authors also noted  a lack of convenient access can be a significant  barrier for consumers. In an effort t o make regionally produced  food more accessible to consumers , a number of  facilities known as r egional food hubs have been developed throughout the U.S. over the past decade. A regional  food hub aggregates, markets, and distributes food from small -scale and midsized producers  to customers that are  located in the same  region, facilitating a connection between them  (Barham, Tropp, Enterline, Farbman, Fisk, &  Kiraly, 2012) . Some r egional food hubs sell directly to consumers , while others  sell wholesale to retailers and  restaurants, where consumers can access local food conveniently. These w holesale markets can be very challenging  for small and midsized producers to reach on their own, because they lack sufficient volumes  and adequate  processing, storage, and transportation infrastructure. Such producers can greatly benefit from a food hub’s ability to  provide aggregation, warehousing, and transportation services. In addition to logistics, food hubs often provide  producers with marketing se rvices, which can  help them to grow their sales and gain more customers. In contrast  with conventional food distributors , r egional food hubs view their producers as strategic supply chain partners  instead of merely interchangeable suppliers (Stevenson & Pirog, 2013) . A core part of their mission is to  ensure that  suppliers are treated fairly and any business decisions are made with the welfare of all partners in mind.  ",iastate.edu,Iowa State University,United States,42.0279608,-93.64473746093857
36,SYSTEMIC ANALYSIS OF COMPLEX SYSTEM GOVERNANCE FORACQUISITION,@odu.edu,"Complex System Governance, Systems Theory, Management Cybernetics, Acquisition ","The purpose of this paper is to explore Complex System Governance (CSG) issues related to systemic analysis of  acquisition systems.  CSG is an emerging field focused on the design, execution, and evolution of the functions  necessary to provide continued system performance (stability) in the midst of incessant turbulence and increasing  complexity.   Integral to this field is the necessity to engage systems to ad dress behavior or performance that is  inconsistent with that which is desired.  Systemic analysis for CSG serves to examine a system to discover  fundamental system issues (e.g. acquisition).  Arguably, system acquisition has an unremarkable record of success,  ranging from missing cost, schedule, or performance expectations to outright failure.  However, although acquisition  has been a continual subject of reform, little emphasis has been placed on a more rigorous systemic exploration of  the field.  This systemic analysis is aimed at uncovering deeper levels of aberrant behavior/performance as a function  of a deficient underlying governance system.  To examine systemic analysis of CSG for acquisition, this paper  pursues three primary objectives.  First , a bri ef introduction to the acquisition problem domain and CSG are  provided.  Second, a perspective of systems -based pathologies for CSG is developed.  Third, an approach to  systemic analysis for CSG is developed (the M- Path Method).  This method is introduced as an approach to  ‘systemic analysis’ through the identification of pathologies (deviations from healthy system functioning) in CSG.   The paper concludes with directions for future development of systemic analysis for CSG in acquisition.      Keywords  Complex System Governance, Systems Theory, Management Cybernetics, Acquisition    Introduction  Traditional acquisition processes have been under increasing pressure to address missed budgets, delayed deliveries  and expensive canceled systems that appear to represent a new normal for complex programs.  There have been  numerous investigations conducted attempting to determine the underlying factors that resulted in unsuccessful  efforts (Berteau, Levy, Ben-Ari, & Moore, 2011; Francis, 2008, 2009; Rascona, Barkakati, & Solis, 2008).  Unfortunately, there is not a definitive response that can conslusively offer an explanation or provide a remeady.   The acquisition endeavors that have experienced success based on usability, budget and delivery schedule are the  rarity and are often the subject of study in the hopes of discovering the critical essence that contributes to success  and can be generalized to advance future acquisition programs chances for success (Boudreau, 2007; O'Rourke,  2014). However, to date there is not an accepted articulation of the ‘recipe’ for ensuring performance in the  acquisition of systems.  Rather than rehash prior approaches or viewpoints, complex system governance (CSG) is  examined as a value adding alternative to examine acquisition systems.  The hope is that the CSG perspective might  provide new insights to an all too familiar problem domain (Keating et al., 2015).  CSG is an emerging field  grounded in Management Cybernetics (science of effective system organization) and Systems Theory (principles  governing behavior of all systems). CSG has posited nine metasystemic functions required for effective governance,  which will be briefly examined in the next section.    The problems facing practitioners dealing with complex systems appear to be intractable.  The domain of  the acquisition practitioner is marked by conditions characterized by:   ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
37,MAINTAINING A CORRECTIVE ACTION PROGRAM,@utc.edu,"Corrective action, root cause analysis, causal factor, value-added activities, continuous improvement. ","The role of corrective action in continual improvement of an organization is to identify a problem, make an  immediate correction to fix it for now, and make sure the finding does not occur again.  The goal of implementing  corrective action is to identify and address the root cause of a problem to prevent reoccurrence.   Organizations that  have the willingness to learn from their past mistakes and experiences will, in turn, realize that it only adds value  which is vital to the future of the business.  Organizations that wish to keep this competitive advantage must maintain  a robust and satisfactory Corrective Action Program (CAP).   The adequacy of the CAP program will be strongly  dependent on the inputs, the investigation processes, and the implementation of the steps.  A CAP is part of the core  business of organizations aspiring to become world class leaders in their industry.   The purpose of this paper  is to discuss the  CAP model and evaluate the general eff ectiveness of the model  presented.  CAP is intended to measure and assure that weaknesses are revealed and addressed correctly, so that the  program can be improved to support an organization’s objectives, safety, quality, reliability, and regulatory  compliance.  These improvements will create a learning culture to help change the fundamental belief about  integrity, accountability, commitment, and professionalism.   This paper is one method of increasing an appreciation  of this vital program area and is a viable example for the assessment of the CAP.    Keywords  Corrective action, root cause analysis, causal factor, value-added activities, continuous improvement.    Introduction  Corrective action can be implemented at any point within a project, or when a specific task or tasks have taken the  project in a direction that is in conflict with the pre -determined outcome of project management.  This new direction  should be documented, and upon execution, turn the project to better align with the goals, expectations, and  ultimate  results laid out in the management plan.  Intervention as soon as possible is more effective, as it can involve a more  minor correction rather than later, which can require a more significant alteration.  For this reason, expedient  intervention can result in less financial and time expenditures.     For example, a principal deficiency that led to both the Columbia Space Shuttle disaster and Davis -Besse  Nuclear Power Station vessel head corrosion incident was failure to evaluate the causal factors t horoughly and  implement effective corrective actions. (Vanden Heuvel, et. al., 2008)  Due to the lack of proper response actions,  the result in both of these cases was catastrophic.  Lessons learned in the investigations following events clearly  dictate the need of maintaining a robust corrective action program that meets the following requirements:  • Identifies, documents, evaluates, and trends problems to ensure the causal factors (including root causes)   and significance of each problem are understood,  • Develops, tracks, and implements timely corrective actions to resolve the identified finding, and  • Verifies completion and reviews the effectiveness of the completed corrective actions to ensure that they   successfully resolve and prevent recurrence of the same and similar findings.   The thoroughness and effectiveness of a CAP to resolve and prevent recurrence of identified findings may  directly impact a plethora of different areas including: the environment, health and safety of the site/organizations,  the public, performance of organizations, cost effectiveness of operations, and cost avoidance resulting from the  repeat violations. (Anderson and Fagerhaug, 2006)       ",utc.edu,University of Tennessee at Chattanooga,United States,35.0459,-85.2953
38,PART KITTING PROCESS FOR MATERIAL FLOW EFFICIENCY INENGINE PRE-ASSEMBLY OPERATIONS,@ferris.edu,"Part kitting, sequencing, product variation.   ","Increased product variation, varying line speed, product complexity, and increasing focus on lean manufacturing  have led to challenges in the delivery and replenishment  of parts in manufacturing processes. Part kitting and  sequencing, with a main focus on efficiency in the material flow process, is a good option for manufacturing and  assembly operations. This study explores the application of kitting design to improve efficiency in engine pre - assembly operations at an assembly plant.  The study examines the  planning process involved in the design of a  kitting system and the steps of implementation while developing a fully operational  system. The benefits of kitting  are reviewed, along with measurements of the impact on assembly operations at the plant.     Keywords  Part kitting, sequencing, product variation.      Introduction  The purpose of  this study was to find a practical and cost -effective solution to a problem faced by manufacturing  operations at an engine assembly plant.  The launch of a new engine variation at the  assembly plant created a space  issue in the engine line pre -assembly area.  The issue specifically regard ed parts storage and presentation to support  the assembly operation.  With three engine types already  in production at the time, adding a fourth meant a 25%  increase in part content and variation for  lineside delivery.  At the time, the line configuration was at 90% capacity  in the lineside delivery area before launching the forth engine variant.  This meant that the current logistics concepts  for part replenishment would not support the new engine and an alternative solution was required.  Typically in  manufacturing, given similar situations, the assembly line is simply expanded to create the space requir ed to support  production.  However, in an effort to avoid the high investment of line expansion, the goal was to seek alternatives  that provided a cost-effective answer to the problem at hand.   As the material flow engineer, with responsibility for the engine line, the team was challenged with the task  of developing a system that would allow for the new engine launch without the cost of physical line expansion or  an  increase in overhead cost  due to a greater headcount.  This was a unique challenge that required a unique solution.   On one hand, the risk associated with downtime for the production line because of missing parts was the ultimate  driver behind a ro bust in-house logistics process.  On the other hand, the cost o f holding inventory was the driver  behind lean manufacturing and logistics processes resulting in a ‘just-in-time’ (JIT) approach for part replenishment.   Any logistics alternatives that did not require line expansion involved moving parts , and moving parts away from  the point of use introduced a risk to the JIT system.  The following study reviews  the design and implementation of a part kitting process to support production   of a mid -sized sedan (the Passat)  at the Volkswagen Chattanooga  assembly facility.  The kitting process was  designed as an alternative to costly line expansion in the implementation of a new engine variant —1.8L four  cylinder Turbo Stratified Injec tion (TSI).  The project and part kitting process was cross-disciplined and  multifaceted.  Successful implementation of the kitting process  required experience and knowledge in  the areas of   manufacturing, logistics, management, operations, industrial engineering, mechanical engineering, and  finance.  The  project was of particular importance to the success of a new engine model launch at the plant.   The plant started production in 2012 and grew to a production estimated at 120,000 vehicles per year.  Production models prior to the 1.8L engine launch consisted of 2.5L five cylinder  and 3.6L six cylinder gas variants, ",ferris.edu,Ferris State University,United States,43.6828521,-85.48361918322209
39,REVERSE BULLWHIP EFFECT IN PRICING FOR DOMINANTRETAILER IN A STACKELBERG GAME,@uncc.edu,"Game theory, pricing, stackelberg games,bullwhip effect in pricing. ","Bullwhip effect in pricing (BP) refers to the amplified variability of prices in a supply chain. When the amplification  takes place from the upstream towards the downstream of a supply chain this is referred as the reverse bul lwhip  effect in pricing (RBP) and similarly, if the direction of amplification is from downstream to upstream of a supply  chain, this phenomenon is referred as the forward bullwhip effect in pricing (FBP). This paper analyzes  the  occurrence of BP in case o f a dominant retailer and contrasts it to the case of a dominant wholesaler. Here, the  dominant party or the stakelberg leader (the retailer or the wholesaler) sets their respective price first. After that, the  other supply chains players (followers) make pricing and order quanti ty decisions. Th e results indicate that  occurrence of BP depend s on the demand function, the supply chain structure , and the structure of the pricing game.  More specifically, RBP occurs for isoelastic demand and FBP occurs for linear demand, and no RBP or FBP occurs  for negative exponential demand.    Keywords  Game theory, pricing, stackelberg games,bullwhip effect in pricing.    Introduction  Bullwhip effect refers to information distortion of order quantity and inventory decisions from the downstream  towards the upstream of a supply chain (Lee, Padmanabhan, & Whang , 1997). One of the main causes of bullwhip  effect in order quanity is the fluctuation of prices ( Lee, Padmanabhan, & Whang, 2004 ; Bhattacharya and  Bandyopadhyay, 2011). While fluctuation of prices is an expected outcome of cost changes and promotions, the rate  of the price fluctuations may not be constant across various stages of supply chain, i.e. sometimes, the fluctuation of  price may be amplified towards the upstream or downstream supply chain. Özelkan and Çakanyıldırım (2009) refer  this phenomenon as ‘Reverse Bullwhip effect in Price (RBP)’ if variability of downstream price is amplified. They  relate the variability of price with the amplifying (1> ) cost pass-throughs (i.e. changes of price with respect to cost.  e.g. dc dw dc dp dw dp ,, etc.). In this paper, we differentiate the case where price variability amplification is instead  towards the upstream, and we refer this case as the ‘Forward Bullwhip ef fect in Price (FBP)’, relating it with the  absorbing ( 1< ) cost pass-through .   There are numerous empirical examples of amplification of price variability towards downstream or  upstream supply chain stages (i.e. RBP and FBP respectively ). Exihibit 1 shows prices of beef for more than 30  years and prices of fresh orange for 22 years. It is clearly visible that standard deviation of retail price is higher than  that of the wholesale price and the farm price, which implies occurance of RBP in related beef and fresh orange  markets. On the other hand, in the U.S. coffee market, Leibtag, Nakamura, Nakamura, and Zerom (2007)  found that  a 10% increase in cost leads to a 3% increase in retail price, which means, in this case, the price variability in  downstream supply chain is decreasing. Similarly, Bonnet, Dubois, Villas Boas, and Klapper (2013)  observed one  third absorption in fluctuation of retail coffee prices than the fluctuation of raw coffee price in German coffee  market. Borenstein and Camer on (1992)  analyzed oil prices and found the standard deviation of spot market,  terminal and retail gasoline prices as 5.74, 4.12 and 2.91, respectively [Exhibit 2]. Hence, variability of retail prices  seems to reduce for oil prices. These are some examples of FBP in coffee and oil markets.      ",uncc.edu,University of North Carolina at Charlotte,United States,35.311795,-80.741203
40,M,Missing,,,Missing,,,46.3144754,11.0480288
41,THE CIRCULAR ECONOMY OF WASTEWATER: SUPPLYPRIORITIZATION OF RECLAIMED WATER FOR POWER PLANTCOOLING,@uncc.edu,"Wastewater, recycling, reclaimed water, power plant cooling, network optimization, WEAP demand priorities. ","A circular economy is based on the principal of closing resource loops and reducing waste through reuse and  recycling.   Drivers of th is model in waste water systems include increased  pressures on limited water and energy  resources, environmental protection and cost considerations. This study constitutes the first part of overall research  that aims at examining the energy and cost implica tions of potable water source substitution using reclaimed water  (highly treated wastewater) from the 5 main wastewater treatment plants (WWTP s) in Mecklenburg County, North  Carolina.  The objective  of the  study involves prioritizing  potential reclaimed water distribution for power plant   cooling, considering multiple, but limited, reclaimed water sources  (the WWTPs), and multiple demand sites  (the  power plants). Based on cost minimization, considering cost per unit length of pipe and  piping distance from the  WWTPs relative to power plants , a n etwork optimization model, formulated as a transportation model with the  objective function being to minimize piping cost, was found to be a simple methodology for supply prioritization, in  cases where supply was limite d.  The procedure followed could be easily replicated and more beneficial  for larger  networks.      Keywords  Wastewater, recycling, reclaimed water, power plant cooling, network optimization, WEAP demand priorities.    Introduction  Energy production requires w ater in several processes such as plant cooling, hydropower generation, and raw  materials extraction (Rio Carrillo & Frei, 2009).  Wastewater treatment plants (WWTPs) can serve as water  resources for thermoelectric (steam driven) power plants when reclaimed water, which is highly treated wastewater  effluent, is used for power plant cooling.  This in turn can reduce the impacts of freshwater withdrawal for  thermoelectric power plant cooling, currently the greatest fresh water withdrawal activity in the U.S (  Exhibit 1). By considering wastewater as a supply source for thermoelectric cooling, the value of wastewater can be  viewed in terms of a circular economy , based on the  closed-loop circulation of water that allows for repeated use  while retaining full value as defined by Stuchtey (2015).    Exhibit 1: Freshwater Withdrawals by Sector.                            Image Source: http://www.globalchange.gov/browse/multimedia/freshwater -withdrawals-sector. Data from Kenny et al. ",uncc.edu,University of North Carolina at Charlotte,United States,35.311795,-80.741203
42,QUALITY MANAGEMENT OF THE CIVIL ENGINEERING DESIGNSERVICES FOR SMALL AND MEDIUM PROJECTS,@uaa.alaska.edu,"Cost of Quality, Civil Engineering Design Services, Quality Management System. ","The quality for engineering design service will affect the final outcomes of the developed systems.  This  research surveys the engineering design service of small and medium projects and verifies the cost of quality model  by using activity based accounting methods. A case study evaluated an Civil Engineering Design Services (CEDS)  corporation to reduce the firms cost of quality (CoQ).  The Feigenbaum quality cost revealed that the ratio of median  value of the Firms prevention, appraisal, and failure cost as compared to the total budget are 1.0%, 2.7%, and 5.2%  respectively. This case also revealed that the Firms current CoQ is not currently running at an optimum level.  The  failure rate was determined to be 5.2% of the overall budget with appraisal cost at 2.7% of the overall budget.   Although there was not a statistical significance showing that an increased appraisal cost would decrease the failure  cost the data did suggest that it would occur. The optimized model of the CoQ system predicts that if the appraisal  cost were increased to 4.5% of the overall budget the failure cost could be reduced to 0.5% of the overall budget.   This change in the Quality Management System (QMS) system would provide an overall reduction in the Co Q by  2.8%, of the job budget.  The predicted CoQ of the optimized system has a median value of 6.1% of the total job  budget.    Keywords  Cost of Quality, Civil Engineering Design Services, Quality Management System.    Introduction  The engineering design servi ces for small or medium projects may lack a mature Quality Management System  (QMS) to manage their project quality.  This case study focuses on one service line within a firm.  The Civil  Engineering Design Services (CEDS) of this firm typically involves products whose end result is used by the general  public. These products may include airports, bridges, buildings, dams, levees, roads, storm sewer system, sanitary  sewer systems, water systems, and others. Within the CEDS workload product development, submittals may vary  from preliminary planning documents to the full design packages ready for construction.  This product includes  design packages used to construct public facilities within Alaska.  The Firm, as it shall be referred to through the  remainder of t he report, is a small corporation licensed in the State of Alaska.  The Firm consists of a  multidisciplinary staff including engineers, scientists, planners, drafters, and other support staff.  The workload of the  Firm consists of planning, permitting, des ign, and construction management services. The design work may be  further separated into civil, environmental, mechanical, and structural design services. The Firm is considered a  “Small Enterprise” with a full time staff of less than 50 employees and annu al revenues of less than $11 Million.   The facilities specific to this case study include roads and bridges. A design package is a set of construction  documents developed by the CEDS team. These documents typical consist of design reports, plan and specifi cation  which a contractor uses to construct a facility.    Literature Survey  Today the quality movement is booming with annual quality spending nearing $3 Billion dollars in the United States.  (Sloma-Willams, 2004)  Marzouk et al (2012) suggests that the imp lementation of QMS lean principles could  improve utilization of the design process by 40%.  The modern view of optimized CoQ presented by Juran and  Gryna (1988) shows that optimum conformance is at zero defects. This modern model is shown as Exhibit 1. ",uaa.alaska.edu,University of Alaska - Anchorage,United States,61.19037895,-149.82232541071522
43,"NEW PROCESS IMPLEMENTATION: A MATTER OF CONGRUENCYBETWEEN LEARNING, KNOWLEDGE, AND PRACTICE",@oregonstate.edu,"Process Improvement, Engineering Management  ","New process implementations require coordinated efforts from all levels of operation. From the moment new process  training begins to implemented practice, organizations expect to transition from current to future processes without  large fluctuations in performance. However, i n reality not all improvement efforts succeed. The way employees  perceive and behave in an organization (organizational culture), impact the success of improvement efforts. In this  research, the authors present a measure to  integrate measures of culture into daily and weekly data collection for  regular assessment of an organization. The result of this study is a conceptual measure that incorporates organizational  culture and can be used by engineering managers to better plan new process implementations to complement current  roadmaps. The authors hope to energize the engineering management community to pursue research  in integrating  measures of culture and climate into regular measures of an organization’s status.     Keywords  Process Improvement, Engineering Management     Introduction & Background  An organization that seeks to innovate and continuously evolve with the market must have a culture that embraces  and encourages continuous process improvement  (Schneider, Gunnarson, & Niles -Jolly, 1994) . If a culture that  encourages continuous improvement does  not exist, organizations will need to change their  culture. Organizational  culture is defined in this research as the shared behavior, assumptions, and underlying values of an organization, which  are established over time (Schein, 1990, 2004) , and as such cannot be changed overnight. A dditionally, an  organization’s climate is defined in this research as  the day to day experiences that create the “feeling in the air”,   which is a manifestation of the  culture (Reichers & Scheider, 1990; Schneider et al., 19 94). Furthermore,  organizational culture and climate have dynamic interactions where each affect and receive feedback from the other   (Reichers & Scheider, 1990). Thus, one way to change organizational culture is through the management of its climate.  Management in this research follows the logic of Deming (1993), in order to manage we must cont rol, in order to  control, we must measure , and in order to measure, we must understand. Thus, we must understand and measure  climate before it can be managed.   Scholars and practitioners from the fields of psychology (Schein, 1990, 2004), sociology (Ouchi & Wilkins,  1985; Swales & Rogers, 1995) , management  (Unger-Aviram & Erez, 2015) , anthropology (Mulhare, 1999) ,  economics (O’Reilly & Chatman, 1996), and engineering management (Harper & Utley, 2001), to name a few,  have  sought to understand and measure organizational culture. Qualitative and quantitative research methods used by these  scholars and practitioners rely on ethnographic studies, interviews, questionnaires, and surveys. These current research  methods are meant for organizations with a stable environment, thus they require long intervals between observations.  Not only do current methods have long intervals between observations but also consume a large amount of resources  (time and manpower), and produce delayed r esults. When an organization goes through culture and climate change ,  as part of a process improvement effort , the large intervals of current methods  are not desirable. Due to the day -to-",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
44,A CONCEPTUAL FRAMEWORK ON THE ROLE OFCOMMUNICATION COMPETENCY IN NEW PROCESSIMPLEMENTATIONS EFFORTS,@oregonstate.edu,"Communication competency, General Systems Theory, learning-by-doing ","Competent communication within new process implementation  efforts is a necessary, but often a lacking element in  the successful transformation from an organization’s current to future processes. A systemic approach to information  transfer across and in between all levels of operations (e.g. employee training and leadership roles, machine processes,  and organizational culture) is required to understand factors impacting effective communication during new process  implementation efforts.  In this research, the authors present a proposed conceptual model  that captures the role  communication plays in new process implementation efforts. This conceptual model is illustrated through a case study  on Boeing Portland and the ongoin g efforts to adopt a True Lean initiative known as Boeing Portland Production  System (BPPS). In this case study, communication emerged as a relevant factor in the BPPS initiative. As BPPS is a  learning-by-doing process, employee understanding through compe tent communication is critical in progressing  through continuous improvement milestones. Therefore, by applying General Systems Theory concepts to understand  the effects of the communication system on employee training, the gap between what employees think they know and  what they actually know is investigated for communication factors that impact employee learning. This work seeks to  present a conceptual model for ongoing research into the system of communication and provide a practical tool for  Engineering Managers involved in employee training initiatives.      Keywords  Communication competency, General Systems Theory, learning-by-doing    Introduction  Boeing Portland is a subsidiary of the Boeing Company that produces flight critical, complex machined hardware. In  recent years, Boeing Portland has substantially improved their production process in their ability to adopt a strong  process improvement culture. These initiatives developed to meet the needs of Boeing Portland in improving product  flow and meeting delivery and quality expectations  through the adoption of True Lean principles. The main goal of  Boeing Portland is to develop a “one system, one voice” organizational culture where all of the employees  speak the  same organizational language, understand the same systemic metrics  of each work station,  and maintain a healthy  balance between the customer needs and the employee needs . As defined by Boeing Portland, t he idealized culture  will theoretically be reached when individual teams are 1) functional by themselves, 2) the teams apply systematic  problem solving to address issues,  3) the work they complete is continuously improved, 4) the company’s goals and  targets are achieved, and 5) the existing  culture is driving the change.  The organizational initiative to attain this  idealized culture is known as the Boeing Portland Production System.     Research was conducted with the Boeing Portland site in an effort to understand the Boeing Portland  Production System implement ation strategy and the organizational culture as a whole. W hile the new process  implementation has been in progress for three years , pressure has increased to improve the efficiency of the training  and the speed of team maturity and stage progression . Through an understanding of the Boeing Portland culture and ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
45,LEADING FROM A DISTANCE: AN EXPLORATORY SYSTEMATICREVIEW,@mhcinc.net,"E-leadership, leadership, virtual teams, geographically-distributed teams, dispersed teams ","The demand for effective l eadership of virtual  or geographically -dispersed teams continues to increase with the  globalization of industry. As such, understanding the dynamics of leadership in virtual team s has become an  increasingly important category of leadership research.  Following a systematic review format based on a thematic  synthesis methodology the intent of this research is to provide insight and guidance to practitioners faced with virtual  team leadership challenges. Some of th e findings identify that leadership adaptation and style flexibility based on  behavioral complexity theory and situational leadersh ip are a consistent theme. It i s also identified that managing  communication and viewing communication as a surrogate mechani sm for personality and other leadership elements  not present in virtual teams presented an important consideration.  Finally, while there seems to be an expectation in  virtual teams that members understand their roles and should be able to work independent ly, the social element of  teams should not be neglected.  Leaders need to find mechanisms to encourage social interaction and increase team  cohesion.  Finally, team dispersion adds a layer of context complexity to the already complex and conditional  leadership body of research    Keywords  E-leadership, leadership, virtual teams, geographically-distributed teams, dispersed teams    Introduction  Global hyper -competitiveness and the ever -present business need to reduce cost while creating value along with  improvements in technology are primary growth drivers for virtual or geographically -distributed teams. Throughout  this paper , the terms virtual teams (VTs), geographically -distributed teams and dispersed teams are used  interchangeably.  According to one source “mo re than 60% of tasks at Global 2000 companies will eventually be  accomplished by distributed teams” (Connaughton & Shuffler, 2007, p. 389).  Eighty percent of companies surveyed  in a similar timeframe  believed that the number of virtual teams would continu e to grow (Perry, 2008).  A 2008  survey indicated that 80% of corporations with more than 10,000 employees use virtual teams (i4cp, 2008).   More  recently, a Society for Human Resource Management (SHRM) study (2012) suggested that approximately 66% of  multinational organizations use virtual teams.        Despite the appeal, technology -mediated communication and geographic dispersion create challenges that may  threaten effectiveness, defined for purposes of this discussion as both team member satisfaction and  the  accomplishment of predetermined team goals. With the advent of the virtual team structure in the 1990s,  underperformance became an issue particularly as related to (a) lack of communication within the VT, (b) absence of  trust development, (c) excessive length of projects, and (d) excessive expenses (Karpiscak, 2007).  “. . .[C]hallenges  faced by a global team are placed into three categories: (1) geographical, (2) cultural, and (3) organizational . . .  (Eriksson et al., 2002, p. 54).  An OnPoint Consult ing study across industries highlighted the impact of VT  challenges. The study found that 27 percent of the virtual teams surveyed were not fully performing. Reinforcing  OnPoint’s findings “...only 18 percent of the seventy global business virtual teams as sessed [in an MIT study] were  found to be highly successful” (Lepsinger, n.d, para. 7). In a Sieman’s Enterprise (2012) study, only 44% of 320  respondents felt that virtual teams were as productive as face- to-face teams.  43% of the respondents in the same  ",mhcinc.net,,,46.3144754,11.0480288
46,CATEGORIZATION OF SOCIAL VALUE CREATED WITHIN U.S.NONPROFIT ORGANIZATIONS: A CASE STUDY IN OREGON,@oregonstate.edu,Social return on investment; nonprofit; categorization; social value ,"Social value is impacted by the actions of organizations and government agencies. However, few track or forecast the  social value impacts of these action s, making it difficult to effectively apply resources and document social  gains.  This is especially true in the third  sector, composed of nonprofit organizations . Social return on investment (SROI)  monetizes social benefits and costs associated with a project in an effort to counter this shortfall. By including SROI  in a financial management plan, nonprofits are able to analyze strategies which maximize financial and social benefits.  SROI is calculated as the quotient of present value of benefits and cost of an investment. In order to perform this  calculation, social benefits must be defined, then converted into a dollar value. This is difficult because it requires an  initial segregation of financial returns from social returns. Additionally, many current strategies monetize social  impacts using perceived market prices which are largely imperfect. There are currently no guidelines for these  calculations which results in inconsistent SROIs. This makes it difficult to interpret and compar e SROIs within and  between nonprofit organizations. This paper will provide a conceptual model for categorizing nonprofit organizations  by the social value they create, followed by a discussion of  future work for developing valuation techniques for  monetizing the value created within each category , and the implications of SROI to engineering management  profession in general.    Keywords  Social return on investment; nonprofit; categorization; social value    Introduction  The third sector is a branch of the economy composed of nonprofit organizations that provide services to fix problems  ignored by the public and private sectors (Levitt, 1973). Non profits dedicate the majority of their resources towards  improving the lives of individuals and those within the community (Emerson & Cabaj, 2000). This leads to ambitious  projects with multiple, sometimes incompatible goals (McGill & Wooten, 1975). In many cases, this is due to the  large number of stakeh olders that are affected by non profit services, such  as the board of directors, donors, and  customers (Keating & Frumkin, 2003). This can make performance analysis d ifficult since it must be analyz ed from  each stakeholder perspective (Nicholls et al., 2012). This requires a larger time commitment and resources compared  to the private sector who has limited stakeholders (Park, 2016). Additionall y, the services provided by nonprofits are  generally less standardized than those provided within the privat e sector. While this allows non profits to customize  their approaches to better satisfy their customers’ needs, it also increases the difficulty in analyzing performance.  Nonprofits receive funding from foundations, corporations, federated funders, and individuals , and are  exempt to taxes under the Internal Revenue  Service (IRS) (Keating & Frumkin, 2003). Additionally, donations to  nonprofits are tax -deductible for donors. Combined, this increases nonprofit accountability to efficiently use their  donations to make the world a better place  (Pollak & Lampkin, 2001). This requires U.S. nonprofits to make annual  financial statements (IRS Form-990) public record. As the main financial reporting tool for nonprofits, Form-990 has  developed public criticisms. An investigation by the Independent Sector  in the latter part of the 1990’s found  inaccuracies within Form-990 data (Pollak & Lampkin, 2001). These inaccuracies stemmed from IRS input errors as  well as missing entries, vague program descriptions, and arithmetic erro rs in data provided by the nonpr ofits. Since  this investigation, the National Center for Charitable Statistics (NCCS) has worked closely with the IRS to revise the  Form-990 to increase the quantity and quality of  nonprofit information available to the public (Keating & Frumkin,  2003). Unlike the private sector, non profits must balance performing to social goals in addition to economic goals  (Mook et al., 2015). According to Quality 990, a website developed by NCCS to improve IRS Form-990 reporting, a ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
47,SYSTEMATIC FRAMEWORK TO CYBER INCIDENTSCENARIO RISK ANALYSIS,@odu.edu,"Scenario risk analysis, cyber incidents, risk management, systems engineering, scenario analysis ","Scenario risk analysis of cyber incidents are often published in the form of natural language narratives in many types  of publications, including engineering and technology, management, and economics. Though narrative -form of  scenarios may be convenient and effective for informational purposes, describing such complex scenarios as cyber  incidents only using natural language presents particular limitations: usability across disciplines, side -by-side  comparison among supposedly disti nct analysis, and re- usability. This paper describes a systematic framework to  model cyber incidents to overcome these particular limitations of narrative descriptions using tools and techniques  from systems engineering, risk analysis, and decision analysi s. This can be useful for engineering managers who are  conducting a multidisciplinary risk analysis of past or future cyber scenarios.    Keywords  Scenario risk analysis, cyber incidents, risk management, systems engineering, scenario analysis    Introduction  Scenario commonly pertains to a narrative description of what could possibly happen within a specific context or  domain, such as in a play, movie, opera, etc. From a systems perspective, scenarios can be described as a collection  of possible states of the system and its subsystems at a particular time or through time, and can be codified using  natural language or some other modeling  language. In risk analysis, a scenario pertains to a narrative description of  what could possibly happen wi thin the domain of interest, particularly descriptive of risk events, i.e. things that did  (or can) go wrong. As an example, possible scenarios of future earth include being scorched by solar flares, hit by an  asteroid, reigned by AI, or ravaged by severe climate change, the latter being more likely than the other.  Nowadays, cyber incidents are commonly analyzed using scenarios. Publications such as Data breach  digest. Scenarios from the field  (Verizon, 2016), Managing cyber crime accumulation risk  (RMS, 2016), and other  similar publications describe various cyber incident scenarios that maybe useful for engineering managers analyzing  cyber risks at their respective organizations. Consider this scenario:    “A loose affiliation of hackers located in several U.S. cities forms a g ang to reinvest some of the  gains from their previous operations to mount a more structured and coordinated campaign of  data theft. During their planning they discover that there are three quite different zero-day  vulnerabilities for sale on grey markets on the dark web. These have been discovered by  unscrupulous hackers who have carried out fuzzing tests on various systems and each is offering  to sell the technique they have discovered for around $25,000 in bitcoin. The gang quickly realise  that these three rare vulnerabilities in combination could provide a method to extract confidential  data from secure networks across many companies. They realise that they have a finite time  window before these vulnerabilities are publicized and fixed by their target victims” – RMS  (2016) p. 28.   ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
48,AN ANALYTICAL STUDY OF HAZARDS AND RISKS IN THESHIPBUILDING INDUSTRY,@msstate.edu,"Hazard Evaluation Checklist, PHA Worksheet, Risk Score, Workers, and Safety Issue ","The shipbuilding industry is one of the heavy production industr ies, and because of the kinds of materials,  equipment, actions, processes, and conditions shipbuilding involves, th ere is a heightened probability for the  occurrence of accidents. Shipbuilding is associated with numerous risks and hazardous wastes that have the potential  to negatively affect environmental safety and health. The purpose of this paper is to identify and evaluate workplace  hazards in the shipbuilding industry using Khulna Shipyard as a case study. Khulna Shipyard, located in Bangladesh,  is considered as a  heavy ship construction and repair yard. In an effort to improve the decision making process  relevant to risk control and mitigation, a Preliminary Hazard Analysis (PHA) will be used to develop an initial listing  of potential hazards and hazardous events that affect workers’ health and safety. Following the initial listing of the  hazards, the paper presents a hazard evaluation worksheet (PHA worksheet), based on a systematic approach, which  is designed to help the shipyard take corrective actions.     Keywords  Hazard Evaluation Checklist, PHA Worksheet, Risk Score, Workers, and Safety Issue    Introduction  Bangladesh is a well -known maritime nation, entitled to 12 nautical miles of territorial sea and 700 rivers that flow  from the adjacent countries constituting inland waterways of 15000 mile s. Due to congenial topographical position,  water transports play a sig nificant role for Bangladeshi trade and commerce. According to present statistics, more  than 10,000 inland and coastal ships have been plying all over the country, which transport around 90% of total oil  product, 70% of cargo and 35% of passengers (Iqbal, Zakaria & Hossain, 2011).  Owing to favorable facilities, all  inland ships are manufactured and renovated in local shipyards. For instance, local shipyards can design and  manufacture ship up to 3500 deadweight (DWT) that fulfill the demand of local market and l ately, few local  shipyards achieved the competence to fabricate the ships of 10000 DWT (Iqbal, Zakaria & Hossain , 2011).  Bangladesh also exporting ships to Denmark, Mozambique, Germany, the Netherlands and Finland for more than a  decade and in 2008, was declared to be a shipbuilding nation with high international standards. With more than 200  shipyards & marine workshops in Bangladesh, a large workforce is required to work in production areas under  difficult conditions while  handling hazardous material s. Most of these production areas, which include welding,  painting, blasting, and fiberglass production have a direct effect on workers’ health. For example, exposure to  volatile organic compounds (VOCs) and fumes generating from burning base metal, as well as a substantial  generation of NO x gases during the welding and cutting processes can cause severe and chronic health problems  (Celebi & Alarcin, 2010). In recent years, research pertaining to health and safety issues of shipyard workers has  been flourished. While som e studies (Coggon & Palmer , 2016; Selikoff & Hammond , 1978; Kilburn, Warshaw &  Thorton, 1985) were conducted on how process outcomes (fumes, spark, asbestos) adversely impact on the health of  the shipyard workers, other s (Cherniack, Brammer, Lundstrom, Meyer, Morse, Nealy, & Fu 2004; Gillibrand, Ntani  & Coggon, 2016; Malharbe & Mandin , 2007) focused on the consequences of environmental factors (dust, noise,  vibration, VOC) on the shipyard workforce.   The shipyard environment demands constant caution to con trol or mitigate  the hazard s inherent in the  production processes. Thus, it is necessary to identify and manage any potential hazards, hazardous situations using ",msstate.edu,Mississippi State University,United States,33.4386876,-88.79432320417112
49,TECHNOLOGY R&D OUTCOME,@nasa.gov,"Portfolio risk, project risk, BBN, sensitivity analysis, object-oriented BBN, tornado diagram. ","The NASA Aeronautics Research Mission Directorate (ARMD) vision falls into six strategic thrusts that are aimed to  support the challenges of Next Generation Air Transportation System (NextGen). In order to achieve the goals of the  ARMD vision, the Airspace Operations and Safety Program (AOSP) is committed to developing and delivering new  technologies. To meet the challenges of constrained resources and timely technologies delivery, program portfolio  risk assessment is critical for communication and decision -making. This paper describes how Bayesian Belief  Network (BBN) is applied to assess the probability of a technology meeting the expected outcome. The network takes  into account the different risk factors of technology development and implementation phases. The use of BBNs allows  for all technologies of projects in a program portfolio to be separately examined and compared. In addition, the  technology interaction effects are modeled through the application of object-oriented BBNs. The paper discusses the  development of simplified project risk BBNs and presents various risk results. The results presented include the  probability of project risks not meeting success criteria, the risk drivers under uncertainty via sensitivity analysis, and  what-if analysis. Finally, the paper shows how program portfolio risk can be assessed using risk results from BBNs  of projects in the portfolio.     Keywords  Portfolio risk, project risk, BBN, sensitivity analysis, object-oriented BBN, tornado diagram.    Introduction  With NASA’s overall goal of transforming aviation, NASA ARMD has the vision to benefit today’s air transportation  system, aviation industry, and the passengers and businesses who rely on aviation every day. The ARMD aeronautical  research and development (R&D) encompasses a broad range of technologies to meet future needs of the aviation  community, or the outcomes of the six Strategic Thrusts, as identified in NASA Aeronautics Strategic Implementation  Plan (SIP) (NASA Aeronautics Research Mission Directorate, 2014). NASA ARMD technologies aim at increasing  the capacity and improving the efficiency, safety, and environmental compatibility of the national airspace system   (NAS). AOSP supports ARMD particularly through providing relevant technologies contributing to Thrusts 1, 5, and  6. The ultimate goal of AOSP is to mature its technologies from low technology readiness level (TRL) (NASA, 2012)  to high TRL for a successful technology transfer and implementation on operational systems in the NAS. To achieve  this goal with constrained resources, AOSP management team provides leadership to set strategic visions, build a  portfolio, allocate proper funding to technology projects or sub-projects in the portfolio, and guide the R&D execution.  Consequently, portfolio assessment is a critical component of program management activities. Program portfolio  management requires insight in technology (sub)projects from various perspective, such as benefit, cost and risk. This  paper focuses on portfolio assessment from a risk perspective.   It should be noted that for NASA AOSP, the risk management has been implemented at sub- project level,  project level and program level. At program level, the desire is to perform a portfolio risk assessment with a consistent  and transparent analysis process across the technology projects to provide insight for key decision -making and  communication. To this end, the Bayesian Belief Network (BBN) approach was chosen to model individual sub - project risks and used in portfolio assessment. For the sake of simplicity, sub -project is simply referred to project in  BBN discussion throughout this paper. The paper explains why and how the BBN was utilized to provide a visual  model of various risks and their relationships within a project and/or across projects. Various results from the project  BBN analysis are presented, including probabilistic estimates of concerned risks, risk drivers from sensitivity analysis,  and exploratory results from what -if scenarios. Finally, portfolio risk is assessed using comparative results from  Copyright, American Society for Engineering Management, 2016 ",nasa.gov,,,46.3144754,11.0480288
50,ENERGY SUSTAINABILITY OF TURKEY IN THE CASE OF LNG,@odu.edu,"Natural Gas, LNG, Turkey, LNG Terminal Extention, Natural Gas Supply ","Energy is both vital and strategic element for a nation to sustain its fundamental activities like security, logistics,  heating, etc. Countries sustain their energy demands through internal or external sources. In the case of not being  able to maintain energy dem ands from their internal sources, they would need to import their requirements.  Whenever they need to buy raw materials, they have to build terminals to process the raw material into the required  form. The dependency on the imports may cause the importing country to weaken its advantage in international  conflicts, unemployment, and welfare. Therefore, countries aim to mitigate dependence to one country and seek for  alternate countries. To keep the energy sustainable, they should not be dependent on a sole supplier country. A state  chooses to increase its number of providers; it might cause economic advantages or disadvantages regarding the type  of material. Besides, particular type of resources requires specific terminals, facilities, and technology to process the  material to be ready to consume. Consequently, decision-makers should employ a holistic approach that should  comprehend all of the aspects of the situation. We study the case of importing and building a terminal of Liquefied  Natural Gas (LNG) to define criteria in maintaining the energy sustainability and profitability of Turkey.    Keywords  Natural Gas, LNG, Turkey, LNG Terminal Extention, Natural Gas Supply    Introduction  Arab Spring has changed the maps, governments, and social and business dynamics in the Middle East and North  Africa. Local militias or terrorist  groups have taken over the control in some part of Iraq, Syria and still have been  struggling to take control in some North African countries like Libya. The turmoil in Syria has caused extra  politic  conflicts between countries. Turkey’s political position is opposed to Iran and Russia, which are leading natural gas  suppliers of the country. The politic al tension between Turkey and Russia had increased when Turkey downed a  Russian jet near Syria border. Russia immediately imposed an economic sanction to Turkey such as cutting imports.  Experience with Russia and Iran force Turkey to diversify its natural gas supplier s to secure its energy demand and  have a political advantage against those count ries. Therefore, Turkey has sought out to find new suppliers. The  country has already made agreements with Algeria and Nigeria to import LNG. Turkey also purchases from Qatar  but in a small amount. Qatar and Turkey have been discussing the details of long-term LNG trade agreement.   In the last 25 years, Turkey’s consumption of natural gas has increased. Turkey imports almost 99% of its  demand from Russia, Iran, Azerbaijan, Algeria, Nigeria, Qatar and spot market. Turkey has long- term contracts with  those countries except Qatar, which is in -progress. According to British Petrol ( BP) Statistical Review of Energy ,  countries that has the most reserves respectively are Iran, Russian Federation, Qatar, Turkmenistan,  U.S., Saudi  Arabia, and so on ( British Petrol [ BP], 2015).  Turkey purchases natural gas from Russia, Iran, and Azerbaijan  through pipelines.  Turkey also buys natural gas in liquid form from Algeria, Nigeria, Trinidad & Tobago, and Qatar.   LNG Trade of Turkey in 2014 is displayed in Exhibit 1. ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
51,MULTI-FACTOR PERFORMANCE MEASURES AND THE THEORY OFCONSTRAINTS: AN ANALYSIS OF THE CURRENT RESEARCH STATE OFTHE ART,@gmail.com,"Multi-factor Performance Measures, Theory of Constraints, Scheduling, Simulation ","Planning and measurements are key to  managing any modern process. When a variety of constraints and external  factors are involved, the resulting process is hard to manage due to the complex interactions. The purpose of this paper  is to review the current state-of-the-art research in order to 1) determine the latest concepts in multi-factor performance  measures, 2) determine the opportunities for future research in using multiple  performance criteria and the Theory of  Constraints, and 3) the feasibility of using these concepts to develop a simulation to address performance management  in a highly-sensitive multi-objective environment.  To meet these objectives, two analyses of current research in the area were conducted. The first analysis was  on multi-factor performance measures . The critical measures were chosen from Sink and Tuttle: productivity,  efficiency, effectiveness, quality, and budgetability. The second analysis  was on the use of the Theory of Constraints  principles, performance measures , and scheduling. The results showed little research on either performance  measurements for operations with multiple factors that define s uccess or the use of Theory of Constraints principles  to refine performance measures . This study concludes that the lack of studies in these areas  indicates that further  research on of multi-factor performance criteria and the Theory of Constraints is warranted and that a simulation could  be developed as a predictive tool for management to understand interactions in a complex system.   Keywords  Multi-factor Performance Measures, Theory of Constraints, Scheduling, Simulation  Introduction  Ours is an age of evolution and one of the drivers of evolution is technology. As technology has evolved, we find  ourselves in a world filled with data. Technology has also resulted in increasingly complex products. One only has to  look at today’s consumer products to see the complexity. As the interaction between components have increased, the  various components form an increasingly complex system of systems. Similarly, business systems have also increased  in complexity. As bus iness systems become more complex, business leaders increasingly rely on performance  measurement systems to provide them with the information necessary to manage their business. W. Edwards Deming  states that “management is prediction” (Deming, 2000) . Leaders have used metrics to help predict for centuries. As  early as 1494, in his Summa de Arithmetica, Geometrica, Proportioni, et Proportionalita, Luca Pacioli presented basic  financial measures as a means to evaluate business performance. From the period of Pacioli well into the 18th century,  the primary focus of businesses was centered on the calculation of profit and loss of their ventures  (Euske & Zander,  2005). In the late 19th century, competition forced companies to be more efficient and this drove companies to consider  factors other than financial measures. The introduction of using both financial and nonfinancial measures offered the  opportunity to capture multiple aspects of performance (Euske & Zander, 2005).  An example of a current performance management system  (PMS) that uses financial and nonfinancial  measures is the popular Balanced Scorecard. Using groupings of key performance indicators, the Balanced Scorecard  uses four specific perspectives based on the vision and strategy: financial, customer, learning and growth, and internal  business processes (Kaplan & Norton, 1992) . However, in  the complex systems of today, one must ask, “Is that  enough?” A critical analysis of the Balanced Scorecard reveals that it is primarily a measurement system more suited  to top-level management decision support and that the Balanced Scorecard fails to include a system analysis function  better suited to support lower level decision makers. This same analysis also indicated that although the Sink and ",gmail.com,,,46.3144754,11.0480288
52,A SYSTEMIC VIEW ON GAME THEORY: MODELING A GAME OFBATTLE OF THE SEXES USING ARCHETYPES AS PROOF OFCONCEPT,@oregonstate.edu,"Game theory, system dynamics, battle of the sexes, success to the successful, archetypes ","Game theory studies the strategies of two to n players who must make inter -related decisions. It does this by setting  players in a competitive game environment where they must analyze their own and the other player(s)’s strategies to  determine which move offers the greatest self -payoff. Due to its applicability, game theory concepts have been  applied to a wide variety of fields, primarily economics but also biology an d political science. Within game theory,  there has been a push to better model the dynamic strategies and outcomes of games. The current tools used to model  game theory are best suited towards modeling dynamic behavior of simple, unitary systems. However, games  between multiple players with their own goals lead to complex, pluralist or even complex, coercive systems which  current tools are not able to accurately model. The objective of this research is to investigate the applicability of  system dynamics to model game theory behaviors. This study modelled a finitely repeated two player rendition of the  ‘Battle of the Sexes’ game with two Nash equilibria using system dynamics. The Success to the Successful system  archetype was used to model the behavior of thi s game. A stock and flow model was created from this archetype that  was able to achieve the same results expected from traditional game theory models. To better represent real human  behavior, an initial learning phase was incorporated to the model to illustrate how players reach Nash equilibrium.     Keywords  Game theory, system dynamics, battle of the sexes, success to the successful, archetypes    Introduction  Game theory studies decision making strategies in a competitive environment ( Mendelson, 2004). Speci fically, it  studies the interactions between entities (or players) in a fictional scenario (or game) where each entity attempts to  maximize its own outcome (or payoff) given the strategy of the other player(s). Historically, game theory has been  used to de velop economic models which predict the behaviors of agents who interact with each other within an  economic environment. Major economic applications of game theory include predicting oligopoly behavior using a  Cournot competition model, Stackelberg competi tion model, and Bertrand competition model (Romp, 1997). Game  theory has also been used to develop economic models of market competition and international policy coordination  (Romp, 1997). It has also been used to model consumer behavior (Mailath & Samuels on, 2006). Recently , game  theory has become multidisciplinary given  its general applicability. It has seen recent applications in such fields as  biology and political science (Mendelson, 2004).   Game theory can be separated into two distinct branches: non -cooperative and cooperative games (Romp,  1997). Non-cooperative games models the individual actions and preferences of each player (Leyton -Brown, 2008).  In non-cooperative games, players are not allowed to discuss strategies with each other prior to making  his/her own  move (Mendelson, 2004). Cooperative games allow for communication among players and models how coalitions  form and payoffs are distributed between those players (Leyton-Brown, 2008). Each of these game types can be  further broken down into sta tic and dynamic games. Static games model a single event where players play the game  according to discrete strategies to achieve discrete payoffs. Romp (1997) states that players make their move  independently and without prior knowledge of the other player’s past choices. Dynamic games model two scenarios. ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
53,PROPOSED METHODOLOGY FOR PROCESS IMPROVEMENT ATOREGON STATE UNIVERSITY ATHLETIC DEPARTMENT TEAMTRAVEL OPERATIONS,@oregonstate.edu,"Systems Thinking, Critical Systems Thinking, Total Systems Intervention, Local Systems Intervention ","Oregon State University (OSU) Athletic Department oversees 18 sports units, each v arying in terms of technical and  cultural aspects.  Team travel arrangement methods within OSU Athletic Department lack standardization between  sports units.  Combined with a tight budget, this can lead to travel process sacrifices in comfort, time , safety, and  overall well-being.  The lack of  standardized processes oftentimes leads to  inconsistencies when choosing mode of  transport and lodging, and high levels of variability with regards to cost and record keeping.  The outcome of this  paper is the FIR (Familiarization, Intervention, and Refinement) methodology to assist OSU Athletic Department in  improving current team travel operations.  The methodology follows the philosophy of Critical Systems Thinking and  its commitments of critical awareness, improvement and pluralism .  Conflict and incommensurable variabilities  between sports units will be addressed by considering the perspectives of all stakeholders and seeking a consensus  prior to implementation.  This research is relevant to the Engineering Management community as it will create a  practical implementation of Critical Systems Thinking within a sports organization.    Keywords  Systems Thinking, Critical Systems Thinking, Total Systems Intervention, Local Systems Intervention    Introduction  Oregon State University (OSU) Athletic Department supervises 18 sports units (also known as athletic programs or  sports teams), comprised of roughly 500 student -athletes and over 200 staff members.  The sports units differ  in (1)  technical aspects such as size, gender, amount of support staff, budget, sports season and schedules and (2) cultural  aspects such as values, preferences and philosophies.  The budget for Team Travel  operations is scarce, and there is   no standardized process set in place for expenditures.  This provides flexibility for travel schedulers, however it does   not guarantee that resources are used effectively.  Additionally, for many teams the Team Travel budget is a significant  portion of their overall budget, meaning that resources needed elsewhere in the overall budget commonly comes from  the Team Travel budget.  This leads to sacrifices being made that affect comfort, safety, performance and overall well- being of those partaking in Team Travel operations. Examples of sacrifices include:   1. Increased time and effort for travel schedulers to find innovative solutions to minimize costs.   2. Use of vans rather than buses, despite being less safe and more difficult to “herd” when driving in large cities.  3. Driving for long time periods rather than flying.  4. Leaving early or arriving late due to plane times, affecting student academic attendance and performance.  Sports units have a hard time standardizing T eam Travel operations because of their dissimilar identities and  incommensurability in technical and cultural aspects.  This lack of standardization affects time consuming tasks (such  as creation of agendas and record keeping), which places the focus on logistics rather than the performance of student- athletes.  Increased standardization would address less-than-ideal payment methods, such as coaches having to carry  large sums of cash to pay for food, or using personal credit cards and needing to go through a lengthy reimbursement  process.  Finally, standardization would assist in combining the voices of all sports units into a central bargaining  power that would generate better prices and customer service from transport ation and lodging services. Currently the ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
54,PROBABILISTIC SOLAR POWER FORECASTING THROUGH TREEENSEMBLE METHODS,@gmail.com,"Solar power forecasting, probabilistic forecasting, pinb all loss, quantile regression, gradient boosting, random forest, ","Solar power forecasting has attracted more and more interests due to the increasing penetration of the solar power  into the grid. Utilities use s olar power forecasts for system operation and planning purpose. However, there exist  great uncertainties on the availability of solar power supply due to the dynamic weather situation. As a result, utilities  often have hard time to use point forecasting results to support their operational and planning decision making. I n  such case, probabilistic forecasting, which can be scenario -based and/or in the form of quantile, density function, or  intervals, can provide more comprehensive information  about the future. In this paper , we present our investigations  on several popular machine learning techniques , namely, gradient boosting tree , random forest, and extremely  randomized tree. The results show (1) gradient boosting tree and extremely randomized tree perform better in  predicting high percentiles while the three methods have  similar performance for predicting low percentiles; (2) the  relative importance of the explanatory variables varies while predicting different quantiles.     Keywords  Solar power forecasting, probabilistic forecasting, pinb all loss, quantile regression, gradient boosting, random forest,  extremely randomized tree    Introduction  Solar power forecasting is the process of predicting  the electricity generated from solar power. With the increasing  penetration of solar power into the grid, sola r power forecasting has attracted more and more interests from the  industry and the academy. Based on the resolution of the forecasts, solar power forecasting problem can be  categorized into point forecast ing and the probabilistic forecast ing. The first one provides a single pr edicted value  for every step into the forecast horizon, while the latter one provides predicted intervals for every step into the  forecast horizon.   The availability of solar power is mostly driven by solar irradiance together with air density variables su ch  as temperature, humidity, and etc.  Previous research has focused on building point forecast system based on these  independent variables and a variety of methods have been explored including artificial neural network(ANN),  ARIMA, and k-NN (Inman, Pedro, & Coimbra, 2013).   Due to the great uncertainties in these weather conditions, utilities often have hard time using point  forecasting results for their operational a nd planning decision making.  Probabilistic forecasting, which can be  scenario-based and/or in the form of quantile, density function, or intervals, can provide more comprehensive  information about the forecasting variability at each future time point and facilitate management decision .  Probabilistic solar power forecasting w as firstly promoted in Global Energy Forecasting Competition 2014  (GEFCom2014) (Hong et al., 2016) . Many modelling techniques have been utilized  in this competition and showed  promising performance. Data mining models become popular choices among the winning teams, and they have been  able to extend those models like gradient boosting, k-NN and random forest  for probabilistic forecasting purposes  (Nagy, Barta, Kazi, Borbély, & Simon, 2016)(Huang & Perry, 2015). ",gmail.com,,,46.3144754,11.0480288
55,ECONOMIC FEASIBILITY OF RENEWABLE ENERGY POWEREDSTREET LIGHTING: A CASE STUDY,@d.umn.edu,"Energy Efficiency, Renewable Energy, Streetlights, LED ","Some major issues associated with the current use of fossil fuel include concerns about energy security, climate change  and negative impacts on the environment. In addition, there are concerns related to the supply and cost of conventional  energy sources due to limited resources and widely fluctuating prices. These issues can be addressed by the utilization  of renewable energy which has minimal impact on the environment and which represents an unlimited resource. This  research investigates the replacement  of lighting systems powered primarily by conventional energy  sources with  renewable energy technologies using a case study in the United States . An analysis was conducted to determine the  economic feasibility of replacing conventional streetlights with ener gy efficient lighting and renewable energy  systems in the city of Duluth, Minnesota. This analysis compared four alternatives over a 25-year period with the goal  of establishing the alternatives that represent the best opportunity for energy efficiency and cost savings. Results of  this study provides insights for engineering managers on the economic impact of various alternatives when considering  energy efficient options and renewable energy sources for streetlights.    Keywords  Energy Efficiency, Renewable Energy, Streetlights, LED    Introduction  Streetlights are necessary to illuminate roads and pedestrian sidewalks but they can represent  a significant share of  utility bills. In cities, streetlights  can be a significant financial burden and can account for close to one third of the  electricity cost (Kaleem, Ahmad, & Lee, 2014). This cost can be reduced considerably by the use of more efficient  lighting. For instance, a light emitting diode (LED) can increase energy efficiency by 31% when compared to a high- pressure sodium (HPS) light bulb (Huang, Lee, Jeng, & Hsieh, 2012). Furthermore, some LED models for streetlights  have life span of 10 years compared to four to five years for the high-pressure sodium bulbs (Luckow & Hirsch, 2013).  These characteristics make LED an attractive option for any city. A variety of options including efficient lighting and  renewable energy use can be used  by cities to increase their energy efficiency, reduce greenhouse gas emissions and  save money.   The economic and environmental impact of using efficient lighting and/or renewable energy  systems  compared to conventional lighting systems has been documented in previous studies. Luckow and Hirsch (2013)  compared the performance of induction, LED and HPS streetlights in Minneapolis, MN and evaluated several factors  including energy consumption, operating costs and light quality. The study found that the significantly higher cost of  LEDs was partially offset by savings from energy efficiency and reduced maintenance. The LEDs also had better light  output. Chung, Ho, Hui, and Mai (2005) reported on light bulbs containing long lasting dimmable systems controlled  by electronic remotes. This dimmable system implementation conducted in China affected 8 ,000 lights and resulted  in a 27% energy savings with an unchanged amount of crime reports and roadside accidents.  A study by Georges &  Slaoui (2011) on the conversion of conventional highway lights  to hybrid wind/solar energy systems emphasized the  need to reduce carbon emission levels in the city resulting from conventional energy sources. Kwon, Weidemann, and  Cinnamon (2008) reported on a solar and wind hybrid light pole project in Duluth, Minnesota and concluded that the  hybrid light pole is ideal for rural applications where a power grid is lacking or impractical.   This paper considers the implementation of small solar and wind systems  along with more efficient lighting  in the city of Duluth, Minnesota. According to Bull (2001), the two most abundant and clean technologies of the ",d.umn.edu,University of Minnesota - Duluth,United States,46.8203898,-92.08527412081386
56,CRITICAL SYSTEM THINKING ON ENTERPRISE ARCHITECTURE,@oregonstate.edu,"Enterprise architecture, critical system thinking,SSM. ","Enterprise architecture is the blueprint that helps organizations to achieve their functional goals while balancing  various perspectives as a complex system. Enterprise architecture must also match the enterprise’s weltanschauung,  purposes, and context. Therefore, robust enterprise architecture is built by considering 1) all enterprise components  (departments and branches); 2) components changing behavior and structures over time; 3) their interdependence  within the enterprise; and 4) their  alignment with the enterprise weltanschauung, purposes, and context. Problems  arise in implementation stage of enterprise architecture as there is lack of collaboration when in the designing stage.  The objective of this research is to apply Soft Systems Methodology on enterprise architecture, to investigate the  applicability of critical system thinking principles into modeling enterprise architectures. Using creative design of  methods, the weltanschauung, context, and boundaries of the enterprise architecture are clarified.  The resulting  approach presents a first step towards a systemic approach for engineering and management of a reliable enterprise  architectures.     Keywords  Enterprise architecture, critical system thinking,SSM.    Introduction  Globalization removed traditional constraints such as distance, culture and technology. As those boundaries fade,  organizations start facing more complexity in terms of continuous change in the enviorenment having considerable  impacts on oganizations. For this reason, a clear, easy to follow structure is needed for organizations to manage their  arising complexity they are facing. Enterprise is a type of organization as it consists on social entities with  identifiable boundaries and goals (Robbins, 1987). Enterprise architecture, with several definitions, can be  practically defined as “a consistent and coherent set of design principles” (Nakakawa, Bommel and Proper, 2011). It  provides a viewpoint to study the enterprise , builds bridges between enterprise branches,  and guides the  transformation of the enterprise. e nterprise architecture has been applied to different areas such as IT and business  strategy, providing positive outcomes towards communication and management. Various forms of enterprise  architecture, from the Zachman Framework, the Open Group Architecture Framework (TOGAF), to the Federal  Enterprise Architecture Framework (FEAF),  have been developed to provide different points of view towards an  enterprise.  However, limitations of Enterprise architecture exist a t the implementation stage. Enterprise architecture is  designed by enterprise architects who might not be familiar with the purpose and goals of all stakeholders involved.  Different perspectives need to be taken into account in order to guarantee the implementation and effectiveness of  such architecture. Mezzanotte and Dehlinger (2012) summarized that problems involved in the implementation stage  include low tolerance for change, misunderstanding, power distribution and distrust. The key issue leading to th ese  problems is the lack of collaborations , both internal and external,  during the designing stage of e nterprise  architecture.  System with complex  human interactions is described as a human activity system by Checkland (1981).  Since enterprise architecture is aiming to address problems in human activity systems, Soft Systems Methodology  can be applied to better analyze the complexity of the system. This paper introduces a systemic approach for  enterprise architecture. Enterprise architecture will be analyzed using soft systems methodologies, studying the  Weltanschauung, purposes, and context in the creation and adjustment stage of e nterprise architecture. Business part ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
57,BAS AND CYBER SECURITY: A MULTIPLE DISCIPLINEPERSPECTIVE,@uncc.edu,"Cybersecurity, BAS, Facilities Management ","The traditional view of facility management (FM) previously included task s which were more of a physical nature in  terms of the respons ibility of the building. Newer FM roles now include maintaining critical operations through the  use of information technology (IT) and automated controls. Therefore, these changing operations now include new  concerns for facilities personnel, who today  typically must work in conjunction wi th subcontractors and IT staff for  installation as well as operations. Facilities personnel are now also becoming aware of the needs to secure university  (or any campus setting) administrative and facilities data. This awa reness has created an additional layer of concern  and responsibility for facilities management personnel. Although publications often address cyber -threats to campus  operations, rarely have these topics included what the specific concerns might be for faci lity managers regarding the  potential threats. Additionally, rarely has the discussion addressed the role of the stakeholders with regards to  the  mitigation of those concerns.    Keywords  Cybersecurity, BAS, Facilities Management    Introduction  Facilities Management (FM) is the term used for duties associated with managing the operations of a building. FM   often exists with segmented personnel roles due to the multitude of responsibilities for a facility. This segmentation is  what also makes the capabilities  of building automation so appealing  because it provides a control system for  operations. Building automation improves communication and operations, but it is only possible through network  access – thus introducing concerns for cybersecurity due to new channels opened for an attack. New construction and  renovation projects frequently include networked building automation systems (BAS) which are typically contracted  work for the installations. The project team consists of the designer, installer, and sometimes a building commissioning  agent (C xA) and the facilities manager.  Although cybersecurity has been a discussion point for years, rarely is it  outlined for a project  team in terms of specific responsibilities. T his study aimed to establish a  perceived list of  concerns and responsibilities and the team roles concerning protection from a cyber attack . A survey instrument was  utilized to collect the perceptions from four stakeholder  groups who most typically have a role in either the initial  installation or the management after installation of the system . The four groups included : information technology,  facilities and operations, BAS installers, BAS consultants or Commissioning Agents (CxA). Although the current  relevance of cyber security is well known, the importance of this study is to determine more specifically, the perceived  concerns for the stakeholders and their personal responsibilities in terms of risk mitigation.     Background  The existence of a cyber threat on an intelligent building is imminent, regardless of the purpose of the attack. Part of  the attraction to attackers is the capability of malicious agents to attack anonymously (Fisk, 2012). In a facilities  campus setting, the networked systems are larger and therefore have more available entry points. The organizational  mission often includes the need to provide access to the network. This is especially true in the case of higher education  where autonomy and shared information is necessary. However, this mission is counter intuitive to many of the efforts  to stop cyber-attacks. Additionally, numerous student and faculty personal computers and smartphones are brought to ",uncc.edu,University of North Carolina at Charlotte,United States,35.311795,-80.741203
58,INCREASING COMPETENCY IN HUMAN ACTIVITY SYSTEMS USINGBOUNDARY CRITIQUE,@oregonstate.edu,"CX Tool ©, stakeholder’s congruence, Conflict prevention, complex socio -technical systems, boundary critique, ","The CX Tool © is a critical systems thinking approach that assists in evaluating the co mpetency of human activity  systems by creating congruency between what is Known (Thinking) and what is done (Doing). The CX Tool ©  is  composed of a four -tier structure capable of tackling complex and complicated human activities in a socio -technical  systems. In the CX Tool ©, complexity of a system is dependent on the number of stakeholders involved. Currently,  the CX Tool© does not provide a methodology to distinguish between the stakeholders’ perspectives, nor a process to  guide stakeholders to reach consensus when congruency is low. In this research, the authors’ aim is to develop a  methodological application capable of assisting CX Tool© to select stakeholders and reach consensus under a critical  systems thinking perspectives. The boundary critique system s approach is utilized as the theoretical foundation to  understand how conflicts between stakeholders can be stabilized, leading to relegation of certain stakeholder groups  and addressing the issues that concern them. The resulting methodological applicati on can benefit the engineering  management community by gaining a pragmatic systemic approach to assist consensus building in socio -technical  systems.    Keywords  CX Tool ©, stakeholder’s congruence, Conflict prevention, complex socio -technical systems, boundary critique,  Human activity systems.    Introduction  Human activity systems (Checkland, 1981) and socio -technical systems (Mason and Mitroff, 1993) are composed of  multiple stakeholders, each with individual beliefs, expectations, and motivations. Interactio ns between stakeholders  can result in complex problem situations due to uncertainty in human behavior as every stakeholder might adopt a  different perspective.  Furthermore, stakeholders may not be always aware of their different perspectives which  reinforce the complexity of the problem situation. Consensus between stakeholders about their individual beliefs,  expectations, and motivations is crucial for systems competency. As an example, consider the Ford explorer rollover  incidents (Felder and Collopy, 2012). When investigations about the incidents began, it was initially believed that  either the Firestone tires or the ford engineered parts were defective.  Later, it was found that the Ford explorer  rollovers were due to the complex interaction between the human drivers, Firestone tires, and the Ford suspension  system. In this case, the three groups of stakeholders were involved (Ford engineers, Firestone manufacturers, and  the human drivers), each with different beliefs, expectations, and motivations that resulted in a lack of system  competency: Ford Explorer rollovers.   Flumerfelt & Calvo -Amodio (2012) have proposed the CX Tool © to diagnose and analyze system’s  competency, based on evaluating the congruence levels between two spheres: organizational thinki ng and  performance management. Calvo -Amodio & Flumerfelt (2015) presented the notion of tiers within the CX Tool ©. ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
59,A UNIQUE APPLICATION OF A DISTRIBUTION AND INVENTORYPROBLEM WITH SET-UP COSTS MODEL,@fsw.edu,"Decision-Making, Distribution, Inventory, Supply Chain, Heuristic Algorithm, Route Clearance Teams ","In the armed services, one of the biggest threats to combat forces is the enemy's use of Improvised Explosive Devices  (IEDs). One effective counter to t hese IEDs is the use of engineer Route Clearance Teams (RCTs). The availability  of these teams is limited and commanders are always faced with the question of which units and routes should be  allocated to these teams. This study proposes an approach to sol ving this scenario by modeling this as an instance of  a Distribution and Inventory Problem with S et-up costs (DIPS), a subclass of the Vehicle Routing Problem (VRP) -   the distribution of goods by a fleet of vehicles from a central location to a group of distributed customers. In a  distribution problem with set-up costs, the model takes into account the additional factors of multi -depots, demands  spread over several time windows, and the possibility of inven tory at distribution centers en  route, including set -up  costs. Such distribution problems could  then be solved through many methods such as the implementation of a  metaheuristic algorithm based on a combination of cross entropy and Lagrangian relaxation.     Keywords  Decision-Making, Distribution, Inventory, Supply Chain, Heuristic Algorithm, Route Clearance Teams    Introduction  Every day, people make routine decisions based on the goal of finding the best or optimal solution to specific  situations: What is the optimal or shortest route to work? How should I invest to get the optimal or greatest return for  my money? Similarly, businesses devote a considerable amount of time and effort to determining the optimal strategy  for accomplishing day to day tasks: What is the optimal or most cost efficient way to produ ce products? What is the  optimal or most time efficient way to schedule jobs? Thus, the field of mathematical optimization, the process of  determining the optimal solution to a particular problem from an allowed set within the constraints of the situation,   has significant practical applications. Within the field of optimization, combinatorial optimization problems are those  problems in which the set of feasible solutions to the problem is discrete, or at least countably infinite. The set of all  possible feasible solutions is called the solution space, and finding the optimal solution to the problem usually involves  a search through the solution space. Examples of combinatorial optimization problems include the traveling salesman  problem, timetabling and sche duling problems, network flow problems, portfolio optimization problems, minimum  spanning tree problems, knapsack problems, and distribution problems.   With its practical importance to both industry and the military, over the last fifty years much study ha s been  devoted to transportation problems and in particular to the Vehicle Routing Problem (VRP). In the VRP and its  variations, a fleet of vehicles service to a number of customers. These problems have shown to be interesting both  practically and theoretically. Practically, they arise in the daily logistics of providing services to customers and range  from gas tankers providing fuel to gas stations to the scheduling of buses and their routes.   Theoretically, VRPs  have been proven to be NP -hard (Lenstra & Ri nnooy, 1981)  and some of the most  challenging combinatorial optimization problems being studied today. The implication of being NP -hard is that no  solution methods exist that execute in polynomial time. Recent developments in computer processing and improved  algorithms have made vast improvements in the search for solutions, and there are many off the shelf programs  available today for solutions to optimize practical VRPs. These successes, however, are still limited. The largest VRPs  currently shown to be solved using exact methods are limited to around 50 customers, and larger instances can only  be solved in particular cases to optimality (Toth & Vigo, D., 2002). These limitations have led to the development of  many heuristic algorithms. Heuristic algorithms trade accuracy (finding the exact optimal solution) for speed (the  ability to find a “good"" solution quickly). ",fsw.edu,Florida SouthWestern State College,United States,26.5516,-81.8893
60,SYSTEM DYNAMICS PATTERNS RELATED TO ENERGY SYSTEMSUSTAINABILITY,@uta.edu,"Sustainability, energy sustainability, energy source, energy system, system dynamics, causal model. ","Challenges related to energy systems include an increasing demand for energy as well as growing concerns about  environmental impacts related to some energy sources. These challenges drive the need to consider the sustainability  of energy sources  as well as energy systems . Sustainability includes three major components: social development,  economic development and environmental protection. These three “pillars” of sustainability are mutually reinforcing.  Sustainability of energy sources and associated energy systems should be considered throughout their lifecycle. A  better understanding of energy system sustainability can help decision makers understand the effects of choices made  related to energy systems. A systems thinking approach can be applied to understand and evaluate energy system  sustainability. System dynamics is used to model important factors and factor relationships. The paper presents  examples of system dynamics causal loop patterns that can be used to consider sustainability across different  individual energy systems. These patterns can facilitate the development of system dynamics models.    Keywords  Sustainability, energy sustainability, energy source, energy system, system dynamics, causal model.    Introduction  The World Commission on Environment and Development (WCED) (1987) defines sustainable development as  “development that meets the needs of the present without compromising the ability of future generations to meet  their own needs”. The United Nations (UN) (2005) identifies the three major components of sustainability to include  social development, economic development and environmental protection. Both integration and a reasonable  weighting is desired among the factors associated to these pillars so that they are “interdependent and mutually  reinforcing” (UN, 2005). Sustainability can be achieved when there is an acceptable balance between the core  sustainability dimensions (O’Neil-Carrillo et al., 2008).  Social development focuses on aspects such as fair  resource access, empowerment, accessibility,  participation, cultural identity, and institutional stability. Key social indicators are related to economic self - sufficiency, equity, health, and social cohesion (Organization for Economic Cooperation and Development [OECD],  2009). Respect for public rights, freedom to express opposition and the right to be heard during the decision making  process are important goals are associated to this pillar. Economic development considers sustainable production and  consumption (World Society for the Protection of Animals [WSPA], 2014) and suggests an equitable and efficient  allocation of resources (TRUiST, 2013). Economic development also addresses economic security. Environmental  protection considers environmental impacts and ecological concerns. This pillar relates to the conservation and  protection of the natural environment and ecosystems. Environmental challenges can result from population growth,  increasing consumption patterns, and negative impacts from technology, among other concerns (Wackernagel &  Rees, 1996; Meadows, Meadows, & Randers, 1992).  Accessible as well as reasonably priced energy helps to ensure a strong economy and society. However,  there are limits to the amounts of traditional nonrenewable energy reso urces (e.g., fossil fuels) that are available for  human use. There is evidence of significant negative impacts to the environment, and in some cases, humankind, due  to the use of particular energy sources. Given concerns about these impacts, many countries  have begun to focus on  more use of renewable energy  alternatives. However, even with renewable energy sources, there needs to be ",uta.edu,University of Texas at Arlington,United States,32.728471299999995,-97.11202127009975
61,M,Missing,,,Missing,,,46.3144754,11.0480288
62,CALIBRATION OF BUILDING ENERGY MODEL TO MEASUREDTHERMAL RESPONSE OF A RETAIL OFFICE BANK,@uncc.edu,"Building Energy Modeling, Model Calibration, Green Building ","We present a method of energy model calibration that is adapted from our previously developed energy model  optimization work and is based on matching the “thermal response” of space temperatures. This is significant because  most energy model calibration methods are based on matching utility records alone that are not directly representative  of thermal loads, leaving room for err or in the “calibrated” model.   The “thermal response” method ensures a  calibrated energy model used for design optimization produces reasonably good results. The developed method of  model calibration is relevant to projects for which the management and operation of an existing building asset is to be  optimized, such as a retrofit project or a standard building design to be replicated. A case study example of such a  project is presented. Hourly temperature and energy values were collected from an existing bu ilding to serve as the  basis of design for other buildings. An energy model of this building was developed and calibrated to these measured  values. This was done by searching for the set of model parameter values that minimized the error between modeled  and measured temperatures. The calibrated energy model may serve as a trustworthy means to explore building design  optimization and fault detection and diagnosis.    Keywords  Building Energy Modeling, Model Calibration, Green Building    Introduction  In buildings, thermal comfort conditions are sustained primarily by energy-intensive equipment. In 2012, operational  energy in the commercial building sector accounted for 19% of total energy consumption in the United States (Annual  Energy Review, 2011). Major determining factors of heating, cooling, and lighting loads that must be met by HVAC  and electrical light systems are climate, internal eq uipment, building use type, occupancy schedule profile, and the  design of the building enclosure. Of these factors, building enclosure design is perhaps the most freely managed by  the design team. B uilding enclosure design includes building orientation, floor plan/footprint shape, window  placement and size, material selection, etc. The building itself is a system of energy (in the form of sensible heat and  radiation – including light) conducting, tra nsmitting, and storing elements (walls, windows, and building mass,  respectively) that interface between the exterior and interior environments. Because the design team has great control  over these load-determining aspects of design, it is appropriate that they be investigated as a first tier energy efficiency  strategy.   To this end, r obust and validated modeling tools exist for the performance evaluation of building designs .  Such models are used to explore the performance of design alternatives. In addition, the application of optimization  algorithms to the problem of energy efficient building design optimization has received much attention in recent years.  However, such practices are only valuable if the discrepancy between predictions and reality is acceptably small. The  building energy simulation program used in this work, EnergyPlus, has been validated by (Witte, Henninger, Glazer,  & Crawley, 2001). However, building energy models are based on input parameters that are difficult to determine, as  (Eisenhower, O'Neill, Fonoberov, & Mezić, 2011)  have shown. This work addresses the issue of underdetermined  parameters by developing a method by which key parameters of  an energy model are calibrated such that the model  predicts measured values, specifically zone temperatures, reasonably well for a given set of weather inputs.  After  model calibration, design exploration and optimization can proceed with greater confidence in the results.       ",uncc.edu,University of North Carolina at Charlotte,United States,35.311795,-80.741203
63,STUDIES ON RISK DECISIONS FOR INTERNATIONALLYCONTRACTING PROJECTS BASED ON BEHAVIORAL DECISIONTHEORY,@nerin.com,"Project management,  project strategic planning, risk decision, behavioral decision theory. ","Project risks have been proposed to be divided into four categories, namely, strategic risks, financial risks,  operational risks and hazard risks for projects under the international competitive environment. Through systematical  studies on internationally contracting project risk decisions based on Behavioral Decision Theory (BDT), key factors  related to the behavior preferences of decision makers have b een induced to expand and improve existing risk  decision models, improving the efficiency and effect iveness of risk decision-making. Such systematical studies have  resulted in some interesting findings which may be helpful for guiding the project management in strategic and  operational planning.  Firstly, a reasonable guide has helped owners determine the appropriate contracting project management  mode based on the specific context  and requirements of the project in order to better improve the decision -making  efficiency and effect iveness, and effectively control project risks by properly staffing the organizational behaviors.  Secondly, the risk quantification and proper financing decision making basis has been proposed on internationally  contracting project costs estimate to meet the financing requirements of international banks, so as to help the  contractor assist the owners for financing. Thirdly, the clearly defined methods for calculating the expected returns  and risks on the risk decision on contracting p roject in properly packaging planning and procurement strategies have  been systematically studied , so as to effectively instruct the contractors for the project subcontracting and its  packaging planning and risk controlling.     Keywords  Project management,  project strategic planning, risk decision, behavioral decision theory.    Introduction  Risk classification standard is not absolute, generally  project risks are proposed to be classified into strategic risks,  financial risks, operational risk s and hazard ri sks. (1) Strategic r isks refer to the impact s of uncertainty on the main  contractor to achieve the strategic development goals and implementation of development programs, where the main  contractor must comply with the requirements of the shareholders  to select the appropriate project management   modes by combin ing the market context and improving the contractor core competitive edge, seizing development  opportunities to the aspects of market losses by avoiding the risk factors.  (2) Financial r isks are also c alled  commercial risks or economic risks, which r efer to  the impacts on the contractor's cash flow  caused by the  uncertainties including changes in interest rates  and exchange rates, as well as fluctuations of raw material s or  product price, and policies for credit and financing, as well as the impacts on the contract or's financial goals caused  by the financial management behaviors.  (3) Operational risks r efer to the impacts involved in company operational  targets and its implementation process caused by th e uncertain factors such as planning and supply chain  management, as well as operations planning and rational allocation of resources, configuration and flow of key  personnel, subcontracting strategies and legal compliance, supervision and inspections. (4) Disaster risks refer to the ",nerin.com,,,46.3144754,11.0480288
64,STRUCTURE AND PROCESSES,Missing,"Project Management Office, PMO, Engineering Projects, Multi-Project Management. ","The Project Man agement Office (PMO) is an organizational entity that maintains the standard of projects and may  also provide resourcing to support the project management process across an organization.  The PMO can  be  involved in portfolio or program management as well a s strategic project development and management.   Traditionally associated with IT (Information Technology) projects, the PMO approach has now been adopted in  other applications and industries.  However, there remain  a lack a frameworks to properly describe the specification  and functioning of the PMO.  Therefore, this paper will provide insights from an exploratory study of the PMO  approach to organizing and managing projects, including the results of a literature study related to engineering  projects that will be used to develop a framework to describe the potential role, structure and processes of the PMO.   This framework will be of use to practitioners looking to design and establish a PMO in order to improve the  efficiency and effectiveness of the project management process. The findings will be used to develop a research  agenda to inform future studies needed to further understand the theoretical basis of the PMO as well as the  practical implications of implementing an organizational PMO.    Keywords  Project Management Office, PMO, Engineering Projects, Multi-Project Management.    Introduction  The Project Management Office or PMO is an organizational entity that is created in order to standardize how  projects are undertaken and to generate efficiencies throu gh so called ‘economies of repetition’ (Davies and Brady,  2000).   Such repetition of projects allows lessons to be learnt and best practice to be established, thereby leading to  a more efficient and effective delivery of projects.  Moreover, it is possibl e to develop common approaches,  systems, management tools and methodologies to drive forward performance improvements in terms of project  delivery (according to schedule, budget and quality parameters).  The PMO leverages knowledge and resources,  including people, finance and supporting infrastructure so that a focused service and organizational capability is  available to support projects from the early concept ual stage through to commissioning, project delivery and finally  project closure, i.e. across the full project lifecycle.    Many organizations have implemented PMOs to varying levels of success and there are a range of different  types of PMOs.  Originally established in the IT (Information Technology) sector, PMOs have now been set up in a  range of industries that have requirements for technology and engineering projects, such as telecommunications ,  aerospace and construction.  PMOs also support the development of strategic projects and have been viewed as an  enabler to ensure strategic alignment of projects with corporate strategy.  A recent survey of the project  management profession by the PMI (Project Management Institute, 2014) found that somewhat alarmingly less than  half (42%) of organizations report a high level of alignment between projects and organizational strategy.  PMOs  are ideally placed to improve this level of alignment and especially for technology and engineering projects where  there can often be significant levels of technical and management uncertainty (Philbin, 2015) and such projects can  also be subject to many challenges as reported by the Standish Group (1995 and 2009).  Implementing a new PMO structure requires a good understanding of the underlying features to be  developed and it is therefore useful to be able to draw on supporting management frameworks.  Consequently, some  PMO approaches have been developed along with definitions.  Indeed the Project Management Body of Knowledge  (PMBOK) defines the PMO as “An organization structure that standardizes the project -related governance  processes and facilitates the sharing of resources, methodologies, tools, and techniques” ( PMI, 2013).  This  definition is accompanied by the categorization of PMOs in terms of three types: supportive (1), c ontrolling (2) and  directive (3).  Nevertheless there are still a lack a frameworks to properly describe the specification and functioning  of the PMO.  Therefore, this paper will provide insights from an exploratory study of the PMO approach to  Copyright, American Society for Engineering Management, 2016 ",Missing,,,46.3144754,11.0480288
65,MANUFACTURING EXECUTION SYSTEM EFFICIENCY WITHSIMULATION-BASED DECISION SUPPORT,@gmail.com,"Manufacturing Execution System, Enterprise Resource Planning, Plant systems , Business process optimization, ","Business units inclusive of large, medium and small-scale entities traditionally conducts activities based on business  processes. Globalization has resulted in the gradual introduction of various automation systems at various levels of  the business enterprise, specifically focussed on capturing essential busines s activities across the entity. These  systems, inclusive of Enterprise Resource Planning (ERP), Manufacturing Execution Systems (MES) and Plant  systems has been adopted by larger corporates in executing and optimizing business functions. These large  multinationals are described as complex entities with complex business structures inclusive of business processes. The  quantification of automation, escalations and critical variables of these business processes has not been effectively  conducted. A “systems thinking” approach adds the complexity of integrating all enterprise functions but creates a  framework for evaluating the limitations and synergies so as to optimize these processes. This research focuses on the  development and configuration of a simulation model for modelling enterprise maturity via business processes.   This research approach includes hierarchical layout and segregation of these business processes, exploring  these enterprise operations adopting business process tools, techniques,  and methodologies aligned with a system  thinking approach. A simulation framework is configured and tested. The results prove that a simulation model  potentially benefits a complex organisation specific to evaluating time taken to conduct business processes. The results  indicate that interdependent processes can be modelled together with determining impacts of multiple critical variables  in reducing business process time.    Keywords  Manufacturing Execution System, Enterprise Resource Planning, Plant systems , Business process optimization,  Systems thinking.    Introduction/Background Context  Business Processes (BP’s) forms a critical component of multinationals and large businesses in general. Enterprise  activities are described by Marianne & Gregory; Paradiso & Cruickshank; Harmon as a collection of unified enterprise  functions, adding value to process inputs, concurrently transforming these inputs into outputs. The process age has  resulted in large business entities adopting several enterprise automation syste ms, in optimizing business functions  (KO; Ryan; Stephen & Wah, 2009). A system is defined as a collection of several types of machinery or elements that  collaborate to yield an output ( Kasser & Mackley, 2008). Enterprise systems are described as a group of  machinery  or elements adapted to support the business functions of any Business Unit (BU) typically comprising ERP, MES and  Plant Systems (PS). These technologies can work independently comprising elements presenting enterprise managers  with a myriad of b enefits, and can also collaborate so as to enhance performance service level of business systems .  Interdependencies or synergies of these systems offer more leverage opportunities to BU’s, which includes improved  Turnaround Time (TAT); improved analytic ca pabilities; present holistic BP views; enhance operational efficiency;  expedited business decision making.    Review of literature provides limited research in effectively quantifying BP’s of MES/ERP/PS technologies  collectively. Gove & Uzdzinski established that no singular integrated concepts have been adopted in assessing system  optimization from design deployment, development stage to the operational utilization of business systems. This  research seeks to develop and configure a framework to quantifying and optimizing business activities of these  technologies as a singular integrated entity relative to TAT. Rechtin established that synergies between elements  making up a system produces a system-level output, and adds value as a unit rather than the yield it provides as an  independent entity. Ko & Wah argued for BU’s to remain competitive in this era of organizational competitiveness,  Copyright, American Society for Engineering Management, 2016 ",gmail.com,,,46.3144754,11.0480288
66,M,Missing,,,Missing,,,46.3144754,11.0480288
67,INDUSTRY: TECHNOLOGY VALUE ANALYSIS,@pdx.edu,"Technology Assessment, Technology Roadmapping, Robotics Technology. ","The electric power utilities as important social infrastructures should be operated stably without any failure in supply  of electricity. For stable operation, it is necessary to input huge amount of resource and investment throughout power  generation, transmission and distribution facilities. Particularly, constant inspection and maintenance of the facilities  requires highly skilled manpower and advanced technologies. In spite of endless efforts, the electric power industry  is facing serious challenges from social, economic and environmental problems. In this regard, a number of robotic  systems have been tested and applied for inspection and maintenance in nuclear power plants and high voltage power  transmission lines. The Electric Power Research Institute ( EPRI) which conducts research, development and  demonstration (RD&D) relating to the generation, delivery and use of electricity for the benefit of the public wants to  centralize the R&D capability of robotics technologies which are dispersed by each divisi on in order to prevent  duplicated investments and manage its R&D capability effectively. A hierarchical decision model was developed and  applied to evaluate robotics technologies so that multiple benefits can be identified.    Keywords  Technology Assessment, Technology Roadmapping, Robotics Technology.    Introduction  Electricity is fundamental public commodity for human life and sustaining economy. Therefore, the electric power  utilities as an important part of the infrastructure should be operated without any  failure in supply of electricity. For  example, the Northeast blackout of 2003 which was an unexpected power outage throughout Midwestern and  Northeastern United States and some Canadian regions caused countless amount of financial and social loss ( U.S.- Canada Power System Outage Task Force: Final Report on Implementation of Recommendations, 2006). Therefore,  for stable operation, it is necessary to input huge amount of resource and investment throughout power generation,  transmission and distribution facilities. Particularly, constant inspection and maintenance of the facilities require  highly skilled manpower and advanced technologies.   However, in spite of endless efforts, the electric power industry is facing serious challenges from social,  economic and environmental problems. In general, the working environment of electric pow er facilities includes  hazardous conditions such as high voltage, high temperature, high density of electromagnetic field and radiation.  Therefore, alternative technologies which are available to carry out various tasks under these hazardous conditions,  instead of human workforce, are indispensable (Park, Lee, Cho, & Oh, 2012; Parker & Draper, 1998) . In addition,  operation of electric power facilities is facing ever -intensifying shortage of manpower due to aging population and  retirement of skilled people. According to Allen, electric power industries aware the seriousness of aging population  and shortage manpower due to changing jobs or retirement of skilled professionals (Allan, 2012; C.-C. Liu & Frank  Wayno, 2008). Furthermore, strengthening of human safety related regulation enforces more efforts for preventing  industrial accident and reinforcing safety technologies on electric power companies. Robotics technologies have been  regarded as one of promising alternative technologies.  Copyright, American Society for Engineering Management, 2016 ",pdx.edu,Portland State University,United States,45.51181205,-122.68493059820187
68,A HIERARCHICAL STATISTICAL ENGINEERING MODELINGMETHODOLOGY,@odu.edu,"Bayesian causal networks, Statistical Engineering ","In the ASEM-IAC 2015, Cotter (2015) proposed a systemic joint deterministic -stochastic dynamic causal Bayesian  statistical engineering model that addres sed the knowledge gap needed to integrate deterministic mathematical  engineering models within a stochastic framework.   However, Cotter did not specify the modeling methodology  through which statistical engineering models could be developed, diagnosed, and applied to predict systemic mission  performance.  This paper updates research into the development a hierarchical statistical engineering modeling  methodology and sets forth the initial theoretical foundation for the methodology.    Keywords  Bayesian causal networks, Statistical Engineering    Introduction  The primary objective in developing a general statistical engineering methodology is to facilitate the construction of  hierarchical models of partially observable causal-stochastic socio-technical systems in order to better understand and  predict the effects of subsystem, module, and component design or improvement interventions on systemic mission  performance.  Cotter (2015) addressed the problem integrating deterministic engineering models as sys tem dynamic  causal components  within general linear models (GLM) by representing them as functional causal  hierarchical  Bayesian networks within a state-space framework to model joint deterministic-stochastic dynamic causal effects.  He  proposed that the X controllable and Z noncontrollable covariate input variables become endogenous variables of the  form    xi = fi(pai, uxi) i = 1 to k predictors (1)  zj = fj(paj, uzj) j = 1 to l covariates    where fi(•) and f j(•) take on any linear or nonlinear and constant, temporal,  instantaneous, or short-term inflection  inducing physical or stochastic model that accurately represents the dynamics of the process, pai and paj are the  endogenous parents of x and z respectively whose functional form and current values determine the a priori Bayesian  state of each xi and zi respectively, and uxi and uzj are the unobserved structural and random errors associated with each  xi predictor and zj covariate respectively (notation taken from Pearl, 2009).  The random component of each uxi and  each uzi is not restricted to being N(0, σ 2) distributed.  Deterministic physical models are incorporated in their  functional form as x controllable and z noncontrollable input variables with respective uxi and uzj error terms to reflect  structural and random lack of fit.  With this functional notation, the GLM becomes    Min YTotal = f(w′(Ypred – T))          (2)  s.t.  Y = F(pai, uxi)β + F(paj, uzj)γ + ε  LX ≤ F(pai, uxi) ≤ UX  possibly                   LZ ≤ F(paj, uzj) ≤ UZ    whereYTotal is the vector or matrix of systemic performance variables, f(w′(•)) is a vector or matrix of norma lized  weighting functions that admit tradeoffs among the (Ypred – T) differences, and T is the vector or matrix of identified  systemic mission performance targets.  F(•) is a matrix of functional relationships of the X predictors and Z within  and cross layer covariates respectively (generalization of Sain, Furrer, and Cressie (2011) alternat e formulation of a  multivariate Markov random field) to the Ypred variables performance levels.  Where the functional relationship has  Copyright, American Society for Engineering Management, 2016 ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
69,AN APPLICATION OF VISUAL MANAGEMENT TO REDUCE WASTESIN IN-HOUSE MATERIAL HANDLING SYSTEMS IN SMALL ANDMEDIUM-SIZED ORGANIZATIONS,@oregonstate.edu,"Waste Reduction, Material Handling, Visual Management ","Small and medium -sized enterprises (SME s) that are characterized by high -mix and low -volume operations, often  present high product variability, which can result in unstructured ma terial movement processes  that are prone to  material handling wastes. These sub -optimal material handling systems are oftentimes a result of an SME’s limited  resources, whether financial or human. In this research, the authors theorize that visual management presents an  efficient and effective approach to reduce wastes in  in-house material handling  operations with high product  variability. Visual management is useful to improve organizational structure and material flows, while remaining  cost-effective and simplistic . However, the guidelines to aid companies in developing a visual management system  that is tailored for their organization are few and far in between. In this research, the authors propose a tool  capable  of reducing material handling wastes in  SMEs through utilizing a  visual management system. The methodology is  illustrated via a case study organization , and the results show an ad -hoc visual management system capable of  increasing the efficiency of in -house material movements. The resulting frame work is expected to assist SME   managers in reducing in-house material handling wastes using visual management systems.    Keywords  Waste Reduction, Material Handling, Visual Management    Introduction  SMEs (small  and medium-sized enterprises) are often faced w ith many challenges when competing with multi - national organizations. These challenges commonly stem from an inadequate supply of resources - both financial and  human. A survey of 100 SMEs showed the most agreed upon statement was, “it is difficult to compe te with large  companies since we operate w ith limited financial resources”  (Weinrauch, Mann, Robinson, & Pharr, 1991) . The  consequences of the lack of resources creates a scenario where SMEs are inclined to implement sub -optimal  solutions to issues that arise in their workplace (Weinrauch et al., 1991) , and material handling systems are not an  exception. These sub -optimal solutions are often  low-cost, inefficient systems that are more prone to waste .  Additionally, SMEs have a disadvantage with the amount of workers and managers that can contribute to problem  solving and innovation. Human resource limitation leads to issues when attempting to manage new/existing  technology (Eden, Levitas, & Martinez, 1997) . These two considerations are often cited as th e major obstacles faced  by SMEs;  and t here is a lack of formal research done to address these specific challenges that SMEs face when  attempting process improvements, especially in material handling systems (Lee, Lim, & Tan, 1999).   In SMEs, material handling and transportation costs consume a large amount of organizational resources,  and are considered non -value added activities (Goldsby & Martichenko, 2005) . Since the 1950’s, the Toyota  Production System (TPS) has provided methods for eliminating these non -value added activities and their 7  associated wastes  (Womack & Jones, 2003) . However, in applying these methods to other industries some have  found that the original 7 wastes that were identified in the TPS were not directly transferable to their industry   (Bauch, 200 4). For example, Goldsby and Martichenko (2005) suggested the original 7 wastes lack the level of  detail and granularity to adequately describe a logistics systems. Instead, they proposed a list of alternate wastes that  are tailored to logistics activities . Logistics wastes are more comm only rooted in transportation and inventory than  the manufacturing direction that TPS had been designed for. Furthermore, a correlation exists between material ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
70,SYSTEMATIC REVIEW OF THE IMPLEMENTATION OF KANOMODEL IN THE HEALTHCARE INDUSTRY,@mst.edu,"Customer satisfaction, healthcare, Kano model, service quality, systematic literature review ","As customers’ perception of quality continues to be the key determinant of success in any industry, organizations are  intensively focused on customer satisfaction for service quality improvement, business growth, and sustainability.  Historically, the Kano model has been used in many industries to elicit customers’ service quality requirements and  to categorize attributes that add value to the service and improve customer satisfaction. However, the implementation  of the Kano model in the healthcare industry is still in its infancy and the customer needs related to the various  healthcare services remain ambiguous.   This article identifies and categorizes studies of service quality improvement that employed the Kano model  in the healthcare industry. The research method used is a systematic literature review from databases related to quality  improvement in the healthcare sector published in the periodicals from 2002 to 2015.  The principal findings of Kano  model implementations, practices, and integrated quality methodology approaches identified allow healthcare  providers to comprehend customer needs related to the service quality and develop sustainable improvement  strategies. This article intends to propel further research in quality improvement of the healthcare sector.     Keywords  Customer satisfaction, healthcare, Kano model, service quality, systematic literature review    Introduction  While the healthcare industry is one of the largest and fastest-growing industries of the world, healthcare continues to  be a grave socio -economic problem facing many nations today. Healthcare providers and medical professionals  constantly face numerous challenges in understanding and meeting the needs, requirements , and expectations of the  customers, which leads to poor quality of care, lower patient satisfaction, and excessive medical costs. According to  the U.S. Department of Health and Human Services  (DHHS), the total national health ex penditure was $2.9 trillion  dollars in 2013 with a growth rate of 3.6% compared to the previous year.  Customers’ perception of quality is a significant determinant of success of a healthcare organization,  considerably impacting customer satisfaction, service consumption , and customer loyalty. With the rising costs of  providing healthcare and intensified competition in the industry, healthcare providers are focusing on the improvement  of patient satisfaction as a method of managing costs and enhancing service quality (Bond and Thomas, 1992). Patient  satisfaction is an intricate combination of perceived needs, expectations, and the overall experience of healthcare; and  its measurement has often been challenging for healthcare providers due to its complexity (Smith, 1992).  Traditionally, most studies assumed a linear relationship between product or service quality and customer  satisfaction; suggesting that the more the quality level increases, the more customer satisfaction is attained (Huiskonen  and Pirttila, 1998). However, the researchers observed that the linear relationship may be inaccurate for understanding  customer satisfaction and different aspects of quality. Herzberg’s two factor theory was initially developed to identify  the factors influencing motivation levels of the employees in the workplace (Bloemer and Kasper, 1995). Kano,  Seraku, Takahaski, and Tsuji (1984) extrapolated Herzberg’s two factor theory and developed a model to identify the  quality attributes that influence customer satisfaction, and suggested a non -linear relationship between them. The  Kano model can be used to identify and classify the quality attributes based on their impact on customer satisfaction  into must-be, attractive, one -dimensional, reverse, and indifferent categories. Must-be attributes correspond to th e  basic requirements of the product or service quality, and absence of these attributes leads to extreme customer  dissatisfaction. Customer satisfaction is directly proportional to the level of performance for  one-dimensional  attributes. The presence of at tractive attributes leads to extreme customer satisfaction , but the absence leads to no  Copyright, American Society for Engineering Management, 2016 ",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
71,MANUFACTURABILITY ASSESSMENT METHODOLOGY,@iser.msstate.edu,"Manufacturability, Assessment, Metric, Life Cycle ","There is a need for a practical, yet comprehensive method of reviewing a product design in order to ascertain the  level of difficulty and risk inherent in its manufacturability. The development of the Manufacturability Assessment  Methodology (MAM) is aimed at addressing this need.  It is an approach that em ploys a series of judgments  conducted by subject matter experts in an effort to understand the key elements of the product design and their  impact on various aspects of manufacturing.  By applying this type of evaluation, it provides a means to quantify the  inherent risk of a product’s manufacturability.    For typical industrial applications, the benefits of using this methodology include savings in both cost and  time, which serves to enhance the competitiveness of a business and contribute to its general economic  development.  Similarly, the Department of Defense (DoD) Engineered Resilient Systems (ERS) program seeks to  utilize this type of tool along with others to provide the capability to rapidly design, develop, test, and build trusted,  flexible, and resilient systems.  The U.S. Army Engineer Research and Development Center (ERDC) has developed  other software tools, as well as a systems engin eering framework and workflow, that supports system development  from DoD Pre-Milestone A analysis throughout the life cycle of the system.    The resulting outcome of the methodology would include a metric (i.e. manufacturability score) along with  identified risk areas for improvement of the manufacturability of a given design.  This paper provides an  introductory approach to the design of such a methodology.    Keywords  Manufacturability, Assessment, Metric, Life Cycle    Introduction  Development of a product design is a significant undertaking and even more so with larger, more complex designs.   There are many factors that need to be taken into account but an  overarching driver of product design is often cost.    It is generally acknowledged that the majority of costs that will be incurred in the product life cycle are committed  or locked in during the early phases of product design (Anderson, 2014).  It is the design that will largely stipulate  such items as material, labor, and machine requirements, all of which  are associated with manufacturability. It is  commonly understood that the manufacturability of a particular product or design is one of the major life cycle cost  drivers (Anderson, 2014) .  Therefore, the ability to assess the manufacturability of a produc t design early in the  product life cycle is beneficial to the overall product cost.    Based on the literature surveyed, there is no generally accepted assessment methodology and resulting  metric for evaluating the manufacturability of a product design. In the authors’ experience, companies tend to either  forego this evaluation, develop an internal metric, or use whatever other industry accepted tools that are available.   Such companies may experience major quality problems, cost overruns, significant delays in the release of the  product design, and major customer/warranty issues as a result of areas of oversight within the design.   Companies  that commit to use design evaluation tools do so because they recognize the need to assess a design’s  manufacturability early in the design development cycle.  However, their metric or tools may or may not investigate  all the key criteria within the scope of manufacturability.   The objective of this paper  is to introduce a framework  Copyright, American Society for Engineering Management, 2016 ",iser.msstate.edu,Mississippi State University,United States,33.4555,-88.7904
72,EMPIRICAL STUDY OF PUBLIC SECTOR PROJECTS,@bui.edu.pk,"Top Management Support, Project Performance, Public Sector, Project, Pakistan. ","The performance of public sector projects is comparatively low. Top management support is one of the critical  success factors that adversely affect performance of projects in organizations. A large number of studies have been  conducted on top management support as a single dimensional construct in the field of engineering management and  project management. However, a very few studies have explored top management support as a multi -dimensional  construct. The objective of this study is to investi gate the relationship between multiple dimensions of top  management support and project performance. This quantitative study employ ed random sampling techniques on  cross sectional data collected through an online survey from public sector projects. The respondents were the project  managers and project directors working on public sector projects in Pakistan. Explanatory an d confirmatory factor  analyses were employed to test the validity of the construct.  For  testing of research hypotheses, co rrelation and  regression analyse s were conducted. Findings indicate that all dimensions of top management support have   significant positive influence on project performance in the public sector  of Pakistan. The study provides   implications for academicians and practitioners in policy formulation, to gain apt support from top management and  improve project performance. The study provides directions for future research.    Keywords  Top Management Support, Project Performance, Public Sector, Project, Pakistan.    Introduction  The performance of projects in the public sector of Pakistan remained grossly lacking (Gera, 2007). As evident from  lowest budgetary allocation in South Asia region,  education and health sectors have not been given sufficient  priority by the government  of Pakistan ( Pakistan-European Community, 2007).  To improve the welfare of people  through active involvement of top management and sta ble policy intervention, there is a need to emphasize on the  development of public sector in Pakistan  (State Bank of Pakistan, 2005 ). One of the biggest challenges is slow  progress in social development that impac t on future stability, security and economic prosperity of Pakistan  (Pakistan-European Community, 2007).  In the last one decade, the government of Pakistan has increased spending  on development projects of public sector.   Project performance of the public sector  in Pakistan is declining for last many years, which  needs to be  improved particularly in education a nd health sectors of Pakistan (Asian Development Bank, 2009) .  Top  management support is one of  the most critical factors for  implementation of projects in Pakistan (Awan, Raouf,  Ahmad, & Sparks, 2009 ).  Top management refers to the individuals working on higher level positions in  organizations. Top management needs to support project activities and project teams, which is an emerging trend in  developing countries like Pakistan ( Haque & Anwar, 2012 ).  To provide strong support during implementation of  projects in public sector organizations, top management should take the leadership role ( Talib, Rahman, & Qureshi,  2011).  Top management commitment is essent ial for improving project performance in Pakistan. Senior  management should provide support, authority, finance, and resources to the project managers for successful  accomplishment of projects (Shah, Bokhari, Hassan, Shah, & Shah, 2011).   Asian Development Bank (ADB) reported that  only 8% out of 24 public sector  projects were successful  with 58% judge d as partly successful and balance of 33% were unsuccessful projects (Ahmed & bin Mohamad,  2014). The objective of project implementation in the public sector is to improve the liv ings of people by delivering  basic facilities of education, health, sanitation and supply of clean water ( Planning Commission, 2011)  but the  management of projects is considered one of the weakest areas in Pakistan (Mahmood, 2001). Planning Commission  (2011) emphasized on the performance of projects in public sector of Pakistan, in term of planned vs actual time  Copyright, American Society for Engineering Management, 2016",bui.edu.pk,,,46.3144754,11.0480288
73,AND A CASE STUDY ON HOME AFFORDABILITY,@mst.edu,"Risk, threshold, target, home-affordability calculators ","Risk is often measured in terms of variability in outcomes. Hence, a highly variable  outcome is often described as  risky. In this paper, we use threshol ds, also called targets, to characterize risks. Targets have been widely studied in  the literature on risk, especially in the cont ext of the two popular metrics: downside risk and semi-variance.  Downside risk is generally measured as the probability of revenues falling below a target or costs rising above a  target. Semi-variance captures the component of variance below the target revenue (or above the target cost). Going  too close to the target can be alluring, and yet even approaching the target can be dangerous, because accompanying  it is the danger of falling off a cliff. This paper  will focus on a nalyzing behaviors associated with hovering around  the target. Such behaviors can be seen in numerous consumer and industrial activities, e.g., selling a house, changing  raw materials of a product, introducing a new drug into the medicine market, offering financial schemes to  consumers etc. Oftentimes, such behaviors are known to be fraught with danger , and yet vested interests prevent  their risks from be coming well -known.  The risks associated to these behaviors usually become apparent after  disaster has struck, but at times their root causes can remain unknown even in the aftermath of a disaster.     Keywords  Risk, threshold, target, home-affordability calculators    Introduction  Risk is often quantified in terms of randomness in the outcomes of events. Therefore, if a highly variable event  occurs, it is considered to be risky. In general, but not always, risk is treated as an undesirable trait. An exception to  this is entrepreneurial risk , which is considered to be positive , although it also assumes the probability of failure .  Undesirable outcomes are tho se that are related to loss of money and/or life . This paper seeks to analyze risk from  the perspective of targets. In the jargon used in the world of financial risk, a target is a pre -set value for net revenues  or net costs. In terms of revenues, the common understanding is that if revenues fall below the target, one enters a  risky territory. Similarly, in the context of costs, if costs rise above a target, one enters risky territory. These targets  have to be chosen carefully. The target is sometimes obvious, but at times it is not. There are many well -known  instances of situations where targets were blatantly violated to enter risky territory, but the consequ ences became  apparent later. The motivation for entering risk territory is usually short -term profits, bu t the long -run im pact is  generally negative. The goal of this paper is to show that although targets are known  to producers or service - providers, consumers (e.g., users of products and other stakeholders) are often misled into believing that either those  targets do not exi st, or that operating on the target, which is dangerous, is perfectly acceptable and in fact in the  customer’s best interest. The paper will provide multiple illustrative examples from industry for such scenarios and  one case study from real-estate finance where customers are encouraged to be at the target.    Targets are useful in risk analysis  because for settings where outcomes are highly variable they provide   much-needed thresholds that should not be crossed. Consider the returns from a stock. The target for a stock can be  usually set depending on the risk appetite of the buyer.  The target will typically be a price for the stock below which  the buyer will not be interes ted in keeping the stock. Now consider an event such as an individual buying a home  loan, approval of a new drug by FDA, a space shuttle takin g off, or changing the raw materials in a product.  Oftentimes, the variability here is not known. It is in such conditions that identifying targets is really crucial. The  different analyses in this paper will threaded by the role played by targets –  in making risk visible in the analysis,  even if it not always measurable.     Threshold or target risk is often measured in terms of two metrics: (i) downside risk, i.e., probability of  revenues falling bel ow a pre -set target and (ii) semi -variance, which is the component of the variance that falls  Copyright, American Society for Engineering Management, 2016 ",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
74,ADOPTION OF CO2 HEAT PUMP WATER HEATER: A CASE FROMPACIFIC NW USA,@pdx.edu,Technology Roadmap; Technology Adoption; Energy Efficiency; Heat Pump Water Heater ,"The quest of gadgets for homes that would reduce electricity bills, avoid kilowatt-hrs of electricity generation and minimize carbon  foot print has sparked innovation of many energy efficient technologies. CO2 Heat Pump Water Heater (HPWH) is one such  innovation led by Japan, the tech giant of the Pacific Rim. The product was developed over the last decade and many countries  have configured the technology to local needs due to the benign nature and high efficiency potential of the technology. Howev er,  CO2 HPWH is yet to enter the Pacific Northwest market. Hence, detailed roadmap needs to be outlined f or replacement of the  existing less efficient water heaters and integration of the technology in the region.    Keywords  Technology Roadmap; Technology Adoption; Energy Efficiency; Heat Pump Water Heater    Introduction  Electric resistance water heaters are used in majority of the Pacific Northwest habitats. Electric water heaters are  inefficient, costly and carbon intensive. Hence, high efficiency water heaters designed for the NW climate would save  enormous energy. Energy efficient water heaters have the prospect of saving nearly 500 aMW by 2029 that is capable  of powering 381,500 homes each year (Northwest Energy Efficiency Alliance (NEEA), 2014).  Carbon dioxide (CO2)  has low Global Warming Potential (GWP) when compared to other refrigerants, has zero ozone depletion prospective,  and is not flammable. As a Party to the Montreal Protocol, the U.S. must decrease Hydrochlorofluorocarbons (HCFC)  consumption and production gradually, leading to a complete HCFC phase-out by the year 2030. Moreover, the new  efficiency standard by DOE, effective from April 16 th, 2015 requires large-capacity electric water heaters to possess  200% efficiency, a standard that can only be achieved by HPWH  (Gary, 2015).    Literature Review  The term “Roadmap” was first coined during the mid-nineteenth century to denote roads in a particular area. However,  it found new connotation when it was used in conjunction with technology. Motorola CEO, Robert Galvin identified  and invented Technology Roadmapping in the 1970s as a tool that recognizes the different capabilities required today  to achieve a certain technology in the future (Richey & Grinnell, 2004). Since its inception, the concept has been  adopted, adapted and implemented in many different organizations through the years to achieve specific objectives.  During the past two decades, besides qualitative techniques quantitative data collection method has enhanced the  quality of roadmaps. Analytic models have added dependability and flexibility of TRM. TRM has been found to cater  different objectives in various organizations. However, irrespective of the diversity of entities, TRM helps to ensure  that technology strategy is in harmony with business strategy in all forms of organizations (Wells, Phaal, Farrukh, &  Probert, 2004). Mostly, roadmaps are intended for identifying technologies that would aid in developing products to  meet the unmet customer needs identified through market drivers for a specified planning horizon extended in the  future. The needed research and collaboration for the technology development are identified in the roadmap process  and the logical link between each layers finally shows the map to destination. Roadmapping of sustainable biodiesel  or wind energy in the PacNW unveiled the technologies needed to produce the different products in thes e industries  to meet the demands of the markets (Blair, 2009)(Daim, Amer, & Brenden, 2012). In pharmaceutical industries,  Copyright, American Society for Engineering Management, 2016 ",pdx.edu,Portland State University,United States,45.51181205,-122.68493059820187
75,AN ECONOMIC FRAMEWORK,@ttu.edu,"Additive manufacturing, decision factors, economic framework ","Over the last two decades the term Additive Manufacturing (AM) has been developed, mainly originating from  machines initially developed for rapid prototyping applications. Recent publications name AM as a potential  paradigm shift in manufacturing.  AM has been proven to increase manufacturing efficiency and generate an  enhanced customer value through shorter lead times and a higher level of demand. While a general increase in  number of AM machinery is predicted, the question inevitably arises whether certain processes are suitable for AM  and if the potential benefit pays off the necessity of higher costs. The goal of this research is to identify important  factors that determine the economic suitability of AM for manufacturing processes. The relevant factors are  identified through a literature review of existing research which describes economic factors of AM and their  inherent impact. From these facto rs an economic framework can be drawn, which supports the decision whether a  manufacturing process should be designed traditionall y or by utilizing AM. The frame work is designed in an open  way, so that an application in many scenarios seems possible. This study seeks to increase the understanding of  economic factors of AM to allow engineering managers to make the best economical decision with respect to AM  techniques.    Keywords  Additive manufacturing, decision factors, economic framework    Introduction  The term “additive manufacturing” initially referred to 3D printing that was used  to manufacture relatively simple  parts, and to rapid prototyping techniques. The term, however, grew over the last several years, and now describes  the direct manufacturing layer by layer to produce final parts, even with very complex shapes and product details.  (Gibson, Rosen, & Stucker, 2010; Campbell, Bourell, & Gibson, 2012) There are several  things that differentiate  additive manufacturing from conventional manufacturing:   1. Additive Manufacturing  (AM) does not require any substantial process planning. There is no planning  needed for pre-treatment, no production of support structure, no facility design.   2. AM is an additive procedure. Layer by layer, the material is being added, wh ereas in conventional  procedures like drilling and milling, material is taken away to get to the final product.   3. AM produces one solid part, whereas conventional production methods produce several parts due to  production restraints, which then need to be assembled.   4. AM further allows for the use of different material in just one machine, whereas conventional production  methods usually focus on using one material per production task. Using different materials is especially of  value for complex parts.   5. AM does not require and molds or special tools, unlike the application of conventional production methods.  This is a significant advantage of AM and only possible because the machine can print the support structure  of a different material while printing the main  part layer by layer. The support structure can be washed  away after the part is finished. (Baumers, Dickens, Tuck, & Hague, 2014)  Consequently, AM offers a new way to produce and construct parts, mainly described as freedom  fabrication. (Gibson, Rosen, & Stucker, 2010) It has been identified in recent studies, that AM can increase  efficiency, enhance customer value and allow for higher flexibility. (Khajavi, Partanen, & Holmstroem, 2013) With  all the named and explained advantages of AM, it can  be predicted that the number of AM machinery will increase  in the near future. This development may be especially seen in production environments with small batch sizes,  Copyright, American Society for Engineering Management, 2016 ",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
76,AN EXAMINATION OF DECISIONS MAKER’S INTRINSICCHARACTERISTICS AND THEIR EFFECTS ON THE VALUATION OFSUPPLIER ATTRIBUTES PROCUREMENT,@tamu.edu,"Supplier attributes, supplier selection, rational decision making,  ","The current era of globalization and outsourcing has increased complexity in the competitive market among  manufacturers and distributers. Sourcing decisions have taken on a more strategic role; this requires professionals that  can make decisions that balance numerous competing demands. Very little work exists that examines how s ourcing  professionals make these decisions or whether their background or intrinsic preferences and biases affect these  decisions.  This work hypothesizes that supplier selection criteria (and the perceived value associated with those  criteria) for decisio n makers are formulated by a decision maker’s attitude towards risk management and his/ her  cognitive ability. This paper advances a methodology to find relationships among the perceived value of selection  criteria for decision makers, their rationality, r isk preferences, cognitive ability, and demographic and professional  background. Data from industry professionals representing manufacturers and distributors are shown detailing the  relationships among selection criteria, the perceived value of alternative attributes, risk attitudes, and cognitive  capabilities. Correlations and comparisons are made on the basis of industry (distributor versus manufacturer), role,  and experience. Distinct differences are shown for manufactures as compared to distributors; cognitive ability is  significantly correlated with rationality with respect to attribute valuation. Experience and education are shown to  have significant relationships with risk attitudes.    Keywords  Supplier attributes, supplier selection, rational decision making,     Introduction  Due to a highly globalized economy and fiercely competitive marketplace, companies are looking for  opportunities to reduce cost and provide a  greater variety of products and services to their customers in order to  gain/retain the market share (Nepal et al. 2014). Furthermore, depending upon the type of firm (e.g., manufacturer or  distributor), the purchasing spend can vary anywhere from 50-80% of their annual sales. Therefore, supplier selection  decisions are some of the most critical decisions an enterprise has to make (Kaufmann et al. 2014). The globalization  effect has also shifted the competition from “between the firms” to “between the supply chains” (Monczka et al. 2015).  Understandably so, there is abundant coverage of supplier selection problem in the existing purchasing and supplier  selection literature. Prior researchers have adopted a variety of models to deal with this problem ranging from a simple  weighted score model to a linear programming model ( Nepal et al. 2009), total cost of ownership (Micheli 2008),  Analytic Hierarchical Process (Büyüközkan and Görener 2015), group decision making approach assigning weights  to both the attributes and the experts (Chen 2015), and to utility driven approach (Ulutas et al. 2016), to name a few.   Risk considerations and a decision maker’s rationality during the supplier selection process is another area  that has drawn tremendous attention in the prior literature. Maccrimmon and Wehrung (1985)  have developed 16  measures of “propensity” to risk.  These 16 measures were derived from three fields of study: economics (financial  decisions made by the managers), psychology (attitude of the decision makers), and the theory of risks or utility  function based measures (i.e., measures from standardized risky situations such as “sure payoffs” versus “winning or  losing a gamble”). Based on their empirical study of over 500 executives, the authors found that there was no  correlation among the three different types of risk measures, suggesting th at there is no uniformity across decision  makers about what is considered risky. In other words, it clearly illustrates that risk can’t be measured with one  Copyright, American Society for Engineering Management, 2016 ",tamu.edu,Texas A&M University - College Station,United States,30.6108618,-96.35206061388457
77,STORYTELLING IN PREFERENCE COMMUNICATION,@uah.edu,"Storytelling; Preference Communication, Requirements, Systems Engineering, Lifecycle processes. ","This research explores the benefits of using storytelling in Systems Engineering by understanding the characteristics  that make it an effective tool in the communication process.  An effective tool in Systems Engineering would be one  that improves the efficiency and performance of the Systems Engineering process and product.  T his process is  extremely important to the success of a system  and it is unfortunately quite hard to understand. Current methods of  communicating are limited by the means in which information is expressed and distributed.  Document -centric  communication methods, with an emphasis on requirements and constraints, dramatically limit the stakeholder's and  engineer's ability to express their desires.  The current practice of engineering, which uses these communication  practices, has been plagued by cost overruns and schedule delays.  In addition to projects being delayed and over  budget, misunderstandings between stakeholders, caused by potentially different backgrounds and preferences, can  result in the development of a system that does not reflect the stakeholders’ desires and needs. P reference  communication is key for systems and design engineers where benefits can be seen by adopting storytelling methods  such as the dramatic curve and drama triangle. In this paper, personal preferences regarding the way people prefer to  hear stories are explored with the future aim of understanding which storytelling type is more effective. An initial  research plan and a preliminary understanding for how different types of storytelling impact preferences and beliefs  of both the engineers and the stakeholders is investigated.     Keywords  Storytelling; Preference Communication, Requirements, Systems Engineering, Lifecycle processes.    Introduction  Communication between groups of people occurs in multiple ways via different media. This communication is usually  tailored to the situation and the environment where it occurs, which affects the importance placed on it. From general  observations and experiences of  the authors, c ommunication in the engineering world consists mainly of numbers,  diagrams, and formulas; the human aspect of communication in engineering is often neglected with the main focus  placed on the more technical and functional aspects regarding the system. This results in engineering communication  being very dry and lacking in emotions, which are intrinsic to human nature. It often escapes the engineers’ mind that  this activity is supposed to be a cultural activity  (Griffin, 2016) as it involves team work  among many different  individuals coming from diverse disciplines. It therefore becomes necessary, in the engineering community, to find a  common language that can be used and be fully understood by people with backgrounds other than engineering. The  science that analyses  these interactions between everyone , and everything,  involved in a system , is systems  engineering, whose goal is to make sure everything runs smoothly through the use of good communication.  Systems engineering is a “management technology” that deals with the multidisciplinary interactions found  within a system; branches that are of interest to the systems engineers span from science  and organization, to the  humans behind the development of a system (Sage, 2000) . There are many faults in the systems engineering field ,  partially due to reliance on people.  People can be hard to read and complicated to interact with due to each one’s set  of personal beliefs, preferences, and biases.  However, people also bring creativity and experiences that computers are  not capable of recreating, and therefore people are not viewed as bad components in systems engineering, but as ones  not being used in the best manner.  The humans in systems engineering are the artists of the system.  Any type of  engineering is, even if it is not often see n as such, a form of art and needs freedom to create un iquely functional  systems without carbon copying past ones. In order to allow such freedom, the initial statement from the stakeholder,  the “beneficiary of the system” (Ulrich & Reynolds, 2010),  should be detailed enough for the stakeholder to get her  message across to the engineering team  during the requirements phase, but vague enough to let the engineers create  Copyright, American Society for Engineering Management, 2016",uah.edu,University of Alabama at Huntsville,United States,34.7252,-86.6405
78,,@globalnpsolutions.com,"Virtual teams, leadership, communication, innovation. ","Lean innovation is built on several principles:  identifying customer value, mapping the value stream, eliminating  waste, responding to customer pull, and pursuing perfection.  Improving new product project execution can increase  productivity, improve time -to-market, and save development costs.  However, successful new product project  implementation is highly dependent on the teams doing the innovation work.  In today’s world, we rarely have the luxury of working in the same office, must less the same time zone, as  our team mates.  Additionally, selecting individual team members who have the right skill sets (e.g. marketing,  engineering, or operations) and the right cultural aptitude (collaborative mindset, creativity, and flexibility) is what  drives successful teamwork.  Most organizations today fail to recognize skills and cultural competencies when  selecting individuals for challenging innovation work.  These lean teams struggle with communication, work practices,  and even leadership.  The result?  Low quality new products are commercialized that suffer significant schedule delays,  chaotic market launches, and excessive budgets.  Five tools are necessary fo r successful engineering managers to work with dispersed, or virtual, teams  engaged in lean innovation projects.  Best practice tools focus on people and processes, including:  1. Initiation and structure,  2. Communication practices,  3. Team protocols,  4. Knowledge management, and  5. Leadership for virtual teams.  Keywords  Virtual teams, leadership, communication, innovation.    Introduction  Lean innovation requires that teams are constantly learning in order to meet customer needs, increase the productivity  of new product development efforts, and continuously improve products and services.  Eliminating waste is the key  philosophy in lean manufacturing, and lean innovation must also remove inefficiencies in the idea-to-launch cycle for  new products.  Most new product failur es can be traced to poor alignment of customer needs and wants with  development goals.  Poor marketing results in failed product launches as customers do not want or need the product  or feature set.  Failed innovation is even more prevalent when companies try to implant a locally -developed product  in another region.  Customers around the world are different, and therefore, innovation teams must address these  differences in order to be successful.  Virtual innovation teams allow firms to access diverse talent and local market  knowledge to increase acceptance rates of new products in the international marketplace.  Moreover, virtual teams  often provide cost-savings to the company.  Most research to date has focused on the failures of virtual teams.  Some of the most frequently cited reasons  for failure of dispersed teams include distance (Hoegl & Proserpio, 2004 ; Mortensen, 2015), lack of coordination  (Connaughton & Shuffler, 2007;  Ferrazzi, 2014), and poor communication (Lurey & Raisinghani, 2001 ; Smith &  Blanck, 2002).  However, with about 80% of people reporting that they work frequently or always in remote teams  (New Way to Work Index) , we have to learn to be successful in accomplishing project goals with dispersed team  members.  Lean innovation can flourish only if virtual teams capitalize on diversity, increase cohesion, and  communicate effectively.   Globally dispersed teams can drive success in innovation by improving creativity, accessing talent, and  increasing diversity.  Well-managed virtual teams, in fact, can have higher success rates than traditionally co-located  teams (Siebdrat, Hoegl, & Ernst, 2009).  However, to achi eve cost savings, access to local market knowledge, and  utilize expert technical talent, dispersed teams must be carefully selected a nd managed.  Both team members and  Copyright, American Society for Engineering Management, 2016 ",globalnpsolutions.com,,,46.3144754,11.0480288
79,SINGLE FAMILY HOUSES IN USA,@stcloudstate.edu,"Forecast, Regression, Analysis, Single, New, Houses, USA, Market. ","Forecasting the market demand is a very critical step in planning all kinds of business including construction  business. This study was conducted to develop a robust regression model that  enables construction companies  predicting the demand of new single family houses in the USA. The study identified each of inflation rate, mortgage  rate, GDP, p ersonal consumption, unemployment rate, and population as independent variables that may affect the  market demand of new single family houses. The data were collected over 21 years, evaluated, and sorted according  the nature of the relationship between e ach independent variable and the market  demand of new single family  houses. The data reflected double conversion in relationship between GDP, p ersonal consumption, and popula tion  and the market demand due to the financial  crises and the beginning of the recovery after it. The d ummy variables  technique was used to identify the periods of before the financial crisis, during the financial crises, and after it. The   dummy variables have been added to the model to handle the fluctuation in these data sets. The study concluded that  the unemployment rate and t he personal consumption are the most important factors that affect the market demand  of new single family houses in the USA. A regression model was developed to be used to predict the market.    Keywords  Forecast, Regression, Analysis, Single, New, Houses, USA, Market.    Introduction  Forecasting is the science of predicting the future. It invades  all aspects of daily decisions of the personal life a nd  business life. In business, f orecasting is a starting point of planning all kinds of  business. Forecasting is needed in  different aspects of business, such as cash flow, number of personnel, raw material prices, sales, etc.  It is used as a  technique to estimate a certain value of an uncontrollable variables to be used in planning the business.   There are different kinds of forecasting such as qualitative forecasting that depends on bringing together in  a logical, unbiased, and in systematic way all information that are related to the factor of interest. This method is  based on educated opinions of appropriate persons. The quantitative forecasting is another way of forecasting that   relies on historical data to make predictions with the help of mathematical models. Quantitative forecasting is  further can be classified into two types, the statistical method and the deterministic method. The statistical method  focuses on patterns and patterns changes. Regression models and exponential smoothing are classified as statistical  method. On the other hand, the deterministic method is a mathematical model in which outcomes are determined  through known relationships between the forecasted factor and the influencing factors, without random variation.  This method includes anticipation surveys and input-output models.   Forecasting also could be categorized based on the time horizon of forecasting. The short -range forecast,  It’s time span is up to one year, and i t is used for job scheduling, planning purchasi ng, etc. The medium -range  forecast, It’s time span is up to three years, it is useful in sales planning, production planning, budgeting, etc. The  long-range forecast, it’s time span is more than  three years, it  is useful for planning new product, capital  expenditures, facility expansion, etc.   This study focuses on an important activity in construction business planning. It is about forecasting the  market demand of new single family houses in the USA. The study attempts to understand what the customers will   demand in future and how the market will behave.    Problem Statement   The construction industry is an important sector of the economy; a careful planning is required to run a successful  construction business. Forecasting future values of many elements of t he business is a very critical step prior  to  Copyright, American Society for Engineering Management, 2016 ",stcloudstate.edu,Saint Cloud State University,United States,45.55139,-94.14833
80,FAST-TRACKING AND CRASHING PROJECTS:  COMPREHENSIVEANALYSIS OF REASONS AND OF IMPLEMENTATION STRATEGIES,@hbv.no,"Project, management, fast-tracking, crashing, schedule compression, strategies, risks. ","Despite how frequent it is to have to fast- track and/or crash a project, not much has been published in general  relative to these activities. In relative terms, the literature available on these topics is not commensurate with their  actual presence in project management. The majority of the papers written on shortening project schedules have been  in the construction sector, although a number of their findings and recommendations can easily be extrapolated to  other domains. This paper addresses in a comprehensive way the need for compressing or shortening project  schedule. Many different reasons  originating at  either customer or contractor are identified, a number of which  usually go undetected. The many strategies that can be followed to fast -track or crash a project are reviewed, relying  in part on the classification of the project in the novelty -pace-technology-pace framework. All stages are illustrated  with real industry cases. The comprehensive analysis of project schedule compression, the cases presented and the  drawn conclusions and recommendations are valuable inputs to all project managers, many of which will face the  need to accelerate the speed at which they execute their projects.    Keywords  Project, management, fast-tracking, crashing, schedule compression, strategies, risks.    Introduction  A common denominator across all industrial and academic domains is the large percentage of troubled projects. The  reasons for projects fail ing to achieve their defined goals are boundless. Many authors have investigated the reasons  behind troubled projects (Pinto & Mantel, 1990; Kappelman, L.A., McKeeman, R., & Zhang, L., 2006; Zuofa &  Ochieng, 2014). In addition to that, many surveys show the devastating statistics of project failure (Hardy-Vallee,  2012; Bloch, Blumberg & Laartz, 2012). The triple constraint in project management, also known as the iron  triangle, states that the quality of the delivered product is constrained by the project s cope, its budget and timescales  (Shenhar and Dvir, 2007; Pinto, 2013). It is not possible to change any of the three, without having an impact in at  least one of the other two. Scope, cost and time are closely intertwined. Successful project managers are t hose who  are able to complete the project on time, within budget and within performance goals (or requirements). All projects  go though normal ups and downs. At some point, the situation may be perceived to be not a normal ‘down’, but  instead a significant problem. The problem may be mainly connected with costs, performance and/or timescales.  Each problematic situation will demand specific remedial actions. In case that the problem is a significant likelihood  of not being able to meet the commmitted dates in the project, it will then be necessary to accelerate the pace at  which the project is executed, in order to compress its schedule and finish it in a shorter period of time that is,  desirably, aligned with the undertaken commitments. Such schedule compression can be achieved by fast -tracking  the project, crashing it, or a combination thereof. To fast -track a project is to modify the project plan, having some  tasks performed in parallel in order to reduce to overall time required to complete the project. Tasks that were  initially planned to be carried out sequentially are now rescheduled and performed in parallel, if their nature and  precedence relationships permits so. To crash a project is to assign more resources to certain tasks, so that they can  be comp leted earlier and thus the total time required for project completion is shortened. There can be different  motivations behind the crashing of certain project tasks, but normally it is to speed up work progress and thus induce  a reduction of the overall project timescales. Both approaches can be pursued separately, or in combination. The next  sections are organized as follows. First, a literature review is conducted. Relevant papers on the topic are identified,  showing what has been addressed in the area of project fast-tracking and crashing. Next, the reasons for fast-tracking  or crashing a project are analyzed. Contrary to what is normally believed, the decision to compress a project ",hbv.no,,,46.3144754,11.0480288
81,INNOVATION AND THE COST OF QUALITY: ANALYSIS ANDIMPLICATIONS OF CURRENT RESEARCH,@ttu.edu,"Big Q, cost of poor quality, research void.  ","Prajogo and Sohal (2001) aimed to find a relationship between total quality management (TQM) and innovation. Their  extensive analysis of the state of the research led to the conclusion  that the relationship between innovation and cost  of quality (COQ) requires to be investigated since it was not considered in previous studies. COQ being an important  part of quality planning area, according to Rose (2014), the previous statement thus needs to be investigated. The  current work attempts to update and elucidate the veracity of Prajogo and Sohal's (2001) conclusions regarding the  lack of consideration of innovation as an important part of the COQ theoretical framework. To do so, an analysis of  the current state of research in this area was conducted  based on the State-of-the-Art-Matrix (SAM) approach. SAM  methodology defines search query, gathering procedures, quality assessment and analysis procedures to obtain and  analyze a sample of research publications in the open literature. No clear relationship between innovation and COQ,  also known as cost of poor quality (CO PQ) and quality cost, was found; t his confirms Prajogo and Sohal's (2001)  conclusions several years after their research. In addition, other analyses were performed using the sample of research  articles to obtain worki ng knowledge in the area of COQ. In summary, this work brings to light the very real void s  that are currently present in COQ research  such as the existence of a possible relationship between innovation and  COQ.     Keywords  Big Q, cost of poor quality, research void.     Introduction  Elizondo-Noriega et al. (2016) performed an analysis of the current state of the literature regarding the relationship  between innovation and quality. They observed voids in the areas where innovation and quality activities collide  in  their sample of articles, shown in Exhibit 1. The operational definitions used in Exhibit 1 can be found in the Appendix  of Elizondo-Noriega et al.'s (2016) study. From the innovation perspective, such voids are found in both internal and  external points of view of innovation from an organization’s standpoint. On the other hand, from a quality perspective,  the areas where the relationship between innovation and quality has not been intensely investigated are quality control  and planning. In other words, the role of these two quality activities in innovation is not entirely developed. Elizondo- Noriega et al. (2016) observed that it is confusing to detect if a systemic point of view of innovation is considered  in  the sampled literature, and in addition,  identified at least eight areas where the relationship between innovation and  quality management can be addressed. In fact, Elizondo -Noriega et al. (2016) identified the previously mentioned  areas for future research while attempting to identify which innovation area was targeted by each quality management  functional (QM) area based on previous studies performed to diagnose the relationship between innovation and QM. ",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
82,THE “STRATEGIC STRETCH”: ESTIMATING LEVELS OFDIFFICULTY IN INTRODUCING NEW PRODUCTS TO A MARKET,@uqo.ca,"Strategic stretch, lifecycle, market dynamics ","Three significant lifecycles interact in a given market and evolve concurrently as the market moves through its  phases from incubation to maturity and into decline.  The market, firm, and product lifecycles have their own  profiles and the best results for the firm are obtained when these three coincide; a gro wing firm introduces for  example a growth type product into a growing market; this would be a zero stretch situation where capabilities of the  firm are aligned with the principal requirements of the market and the introduction of the new product exacts the  least effort. In addition to these three, there is a fourth cycle defined by the evolution of external alliances of the firm  that accompany it along its lifecycle.  The alliances range from rather complex and flexible structures such as  ecosystems at the beginning of the lifecycle to late stage rigid value chains that are, in comparison, simple and  brittle, with dire consequences when the alliance is tested by shock loadings.  Underlying these external lifecycles  and their measures of coincidence are the i nternal functions of the firm.  Decision making, finance, HR, and  compensation regimes among many functions also progress along their own lifecycles. These internal functions can  be in or out of phase at different points of the market, creating serious challenges when their out of phase state is  maximal leading to internal stretch that further taxes the firm’s ability of execute.  This article proposes a metric of  strategic and internal stretch and examine the impacts on the firm’s ability to execute its strategy when the degree of  stretch is beyond a coping level.  .  Keywords  Strategic stretch, lifecycle, market dynamics    Introduction  The lifecycle concept has been around for many years now;  numerous theories and models have been developed in  an effort to explain the organization lifecycle process. For instance, the lifecycle model developed by Miller and  Friesen in 1984, where they conducted a longitudinal study of lifecycle stages based on extensive case histories of  36 organizations, they concluded that f irms do not necessarily move through the lifecyle’s stages in a linear fashion.  Other complex models were conceived, such ten -stage model  proposed by Adizes in 1989.  This paper is a  continuation of articles series and work related to market lifecycle and  firm lifecycle models developed by authors  (Koplyay and  Mitchell, 2014a, & b, and 2015a, & b).  The authors will share the true relevance of the lifecycle  concept as a unifying model for mapping the evolution of organizational functions from early market s to market  maturity and its impacts on firm’s capabilities to carry out its strategies.   The strategic stretch aspect invoked in this  article is related to some extend to the notion of the strategic stretch  versus strategic fit and the challenge to fill the  gap “chasm” between ambition and resources, focusing on the critical role of the leadership behaviors (Hamel &  Prahalad, 1994).  As for the methodology used in this paper , the literature surrounding firm lifecycle and strategic  stretch are interpreted from a qualitative perspective. Qualitative data are subjective and more difficult access and to  measure. Several models and templates have been developed to assist business leaders in knowing what information ",uqo.ca,Université du Québec en Outaouais,Canada,45.42226325,-75.73909824426678
83,"A TOOL TO ASSESS CONTINUOUS IMPROVEMENT CULTURE:ADAPTING SERVQUAL SCALE TO MEASURE EMPLOYEESATISFACTION, AND IDENTIFY FACTORS OF SUCCESS.",@oregonstate.edu,"Lean in healthcare, Quadruple aim, Employee satisfaction. ","Employee experience and satisfaction often drives organizational success and resilience. However,  organizations do not successfully meet the employee’s ex pectations and needs at their work. Moreover,  management struggles to effectively measure employee experience in their work environment.   SERVQUAL multiple -item scale is a tool to evaluate customer overall service perception, based on  five dimensions of qua lity: reliability, assurance, empathy, responsiveness and tangibles.  The scale model has  been modified and adapted to evaluate the existing gaps between employee  expectations and perceptions of  lean. These gaps could reflect the challenges and benefits that a new lean culture represents to employees work  environment. Therefore, lean system coul d be assessed as a service to the employee. The survey has been   designed to correlate those results with the potential factors of failure to change a culture in healthcare. These  factors were identified in the literature.   The potential benefit of the result ing factors is that managers can  identify gaps between expectation  and reality, and implement countermeasures to improve employee experience.     Keywords  Lean in healthcare, Quadruple aim, Employee satisfaction.    Introduction  Although global healthcare expenditure exceeds $3.2 trillion dollars, the increase of healthcare systems  complexity negates the effect of these resources  on the ability of healthcare providers  to deliver better care to  patients (Young, & McClean, 2009). Policy makers have responded to t his disparity by introducing the triple  aim, which is a set of goals  to monitor improvements in patient experience, health outcomes and in cost control  (Bodenheimer & Sinsky, 2014). As part of these efforts, healthcare organizations use Lean and Six Sigma as an  action plan to improve their overall performance; accordingly, Lean and Six Sigma are often applied in pursuit  of the triple aim in healthcare (D’Andreamatteo, Ianni, Lega & Sargiacomo, 2015). However,  just ten percent of  lean implementations succeed, in part because organizations are often unable to truly change their culture  (Halling & Renstrom, 2014).   Lean in healthcare has been limited as a process impr ovement approach because the cultural aspect is  often forgotten (Poksinska, 2010). Moreover, guidelines about implementation processes and sustainability of  lean in healthcare remain key issues that are largely uninvestigated (D’Andreamatteo et al., 2015). The absence  of research on Lean integration is compounded by production pressure, poor workflow design and high  proportion of non-value added work (Sikka, Morath, & Leape, 2015). Additionally, the high frequency of health  workforce injuries, compared to o ther industries, leads to high employee burnout and job dissatisfaction ( Sikka  et al., 2015; Weng et al., 2011). It is possible that employee dissatisfaction detracts from the triple aim because  dissatisfaction reduces service quality and induces pat ient dissatisfaction (Weng et a l., 2011). Recent research  on healthcare employee burnout suggests a transition from the triple  aim to the quadruple aim (Bodenheimer &  Sinsky, 2014; Sikka et a l., 2015) . Unlike the triple aim, the quadruple aim  fourth goal measure s the  improvement of employee experience or work life (Bodenheimer & Sinsky, 2014). Given high level of burnout,  healthcare organizations seek to adopt the quadruple aim, but often lack effective tools to gauge overall  employee (internal customer) experien ce. The goal of this paper is to assess the applicability of a tool used to  gauge external customer satisfaction, for assessing healthcare employee satisfaction.   SERVQUAL is a widely used psychometric scale for measuring  customer perception of service quality  (Parasuraman, Zeithaml & Berry, 1988). A survey has been designed to adapt t he SERVQUAL scale. The scale  is adapted to measure employee satisfaction in organizations transitioning to lean. The survey attempts to   measure healthcare employee’s satisf action based on leadership, training, communication, empowerment and ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
84,THE DYNAMICS OF ALLIANCES FORMATION IN HIGH-TECHMARKETS; FROM ECOSYSTEMS TO VALUE CHAINS,@uqo.ca,"Supply chain, network, lifecycle, and market dynamics ","As markets evolve the individual firms naturally coalesce into groups that offer  advantages over competitors and a  safe refuge from excessive competitive pressures.  From informal ecosystems to technology platform emergence to horizontal and vertical acquisitions to all   out  Mergers and Acquisition ( M&A) offensives, the market is replete with natural driving forces that create islands  of cooperation, alliances and outright mergers in response to the strategic landscapes and new dominant strategies.   This paper will describe the dynamics of alliances, the logic of the formation and susten ance of structures  in each phase of the market, the tipping points of change , and the evolution from loosely structured collaborative  ecosystems to tight couplings of value chains that dominate markets in late stages.   The driving forces behind these emerge nt structures will be examined and the strengths and weaknesses of  the structures discussed under market conditions   such as absorbing shock loading.   The accompanying structural  complexity of the market, the alliances and individual firms will be also c ommented, based on previous articles by  the authors.    Keywords  Supply chain, network, lifecycle, and market dynamics    Introduction  The lifecycle concept has been well articulated, and investigated by the authors in a number of key articles , as well,  several organization lifecycle models with different stages were developed, such as the four-stage model proposed  by Quinn and Cameron  in 1983  and the five stage model  theorized by Galbraith in 1982, and  Miller & Friesen in  1984. But the authors discovered its true relevance as a unifying model for mapping the evolution of organizational  functions from early markets to market maturity . Others papers use developments in the dynamic resource -based  view (RBV) of the firm to construct a capabilities framework that proposes how an organization can pursue specific  market outcomes (Menguc & Auh, 2006).  The power of this representation lies in the fact that mapping is symmetric over time, it forecasts the past as  well as the future, so if we know where the firm came from we can extrapolate its functional future and conversely if  the firm occupies a specific position in the market lifecycle, we can define where it is in its organizational and  functional evolution, along with its likely history leading up to the present profile. Three of the functions are  illustrated in Exhibit1a, 1b and 1c.  “Incentives” refers to the compensation regimes that keep the firm on passive or organic controls . “Culture  and leadership” interface map the congruence between guidance from the top and the ability and inclination of the  firm to respond to this guidance and “ staffing” defines the profiles of HR acquisitions that best match the corporate  needs at successive phases of the lifecycle.     ",uqo.ca,Université du Québec en Outaouais,Canada,45.42226325,-75.73909824426678
85,DYNAMICS OF MARKETS: LOCATING A FIRM ON THE LIFECYCLEIN FAST MOVING MARKETS,@uqo.ca,"Strategic positioning, lifecycle, market dynamics, Porter analysis ","It is intuitively obvious to anyone who has applied Porter -style analyses, such as Five Force s or Internal Value  Chain, (Porter, 2009; Shekhar, 2010) that these techniques were intended primarily for mature markets. Yet much is  to be gained by examining how such analyses and similar techniques evolve along the market lifecycle.   An important concl usion that can be derived from this extension exercise concerns the determination of  where a firm is located in the market development and how well the firm is adjusted to its market position.    This paper will show how internal value chain and five forces  can be generalized for companies at various  stages of their lifecycle: incubating, start up, growth, mature , and declining.  The analytical approach proposed in  this paper allows for triangulation of the firm’s position along the market lifecycle. Significant findings can be  generated and lessons learned in the process of determining where a company is in its market evolution because  many functions of the firm have now been mapped along this lifecycle.  Prescriptions for specific responses to  market conditions will be outlined and conclusions provided on what happens when a firm is not aligned, especially  when the misalignment is structural. Many of the examples used come from an extensive database of over a hundred  case studies primarily in the high-tech sector in Canada.  The analytical methodology described in the article can be  applied to any fast developing market with similar results.  However , for the sake of anchoring the concepts we will  refer to high-tech markets for examples.    Keywords  Strategic positioning, lifecycle, market dynamics, Porter analysis    Introduction  What generates market dynamics and what are the consequences?    Two of the major underlying driving forces that shape future market dynamics are the customer base and prevailing  strategy (other shaping forces such as innovation, access to equity, government regulation, etc. are the subjects of  future papers).    Changing global customer demands and expectations lie behind firm’s intense need to continuously,  search   for differential advantage (Day 1994; ELA, 2004). Customer base define s the product profiles and demand  characteristics such as breakthrough and cutting edge products that are price elastic in early market then some  conformity through standards, eventually a progressive emphasis on functionality, reliability, and ease of use as the  market enters maturity.  This progression comes from the particular client base that matches the phase of the market;  early markets are populated by innovators and early adaptors who seek out best of b reed products and cope easily  with product complexity and the challenge of use  – they expect poor quality and want to be first off the mark with  new ideas.  They often act as beta testers for the seller.  Once the market leaves behind the bowling alley of the early  majority and swirls into the tornado, product features take a back seat and the ease of use of the product and  especially its price competitiveness that drives its sale  as secondary and tertiary customers adopt products and ",uqo.ca,Université du Québec en Outaouais,Canada,45.42226325,-75.73909824426678
86,CLOUD-BASED DEMAND-RESPONSE SCHEDULER OF HVACLOADS IN SMART GRIDS,@uncc.edu,"Smart Grid, Thermal Model, Thermostat, Hadoop, Big Data. ","This paper discusses a Cloud based Demand -Response Scheduler at a utility, operating in a Big -Data Real -Time  environment under a dynamic energy cost scenario. A smart grid framework is used as a test bed, in which a utility  supplies an unlimited number of “smart” homes with their HVAC systems acting as loads. The predicted magnitudes  and time profiles of the electrical loads and internal temperatures of the homes are derived from MATLAB  simulations of their thermal models (executed on individual home servers), and this information is used together with  locational weather data for load forecasting and dynamic demand -response management across all homes (executed  on the utility cloud server). Furthermore, user mobile devices are integrated in the framework for real -time  observability, controllability and privacy management.    Keywords  Smart Grid, Thermal Model, Thermostat, Hadoop, Big Data.    Introduction  Many companies provide Smart Grid Solutions to utility companies around the world. The main aspects of the smart  grids are electronic power conditioning and control of the production and distribution of electricity (Federal Energy  Regulatory Commission,2008). Apart from the challenge in conserving the depleting energy resources, controlling  the distribution of energy in a uniform manner is the next biggest challenge faced by the energy utility companies. It  is well known that energy demand is not uniform throughout the  day. As a result many are calling for utility  companies to move towards a dynamic cost scenario.  In such a scenario, utility programs inform the customers  about the peak hour information, thus providing them an opportunity to take decisions on turn on and turn off timings  which will be informed back to the utility.  This demand response model is advantageous in terms of saving money for the customers during the peak  hours and helping the utility to maintain power demand at a uniform rate by peak curtailmen t or peak leveling. Peak  levelling of power help the utility companies in avoiding the overloading of power generators and other equipment in  the infrastructure by not over using them at certain periods of the day. Also utility companies can reduce the num ber  of standby generators which will be needed during higher power demands.  As utility companies serve millions of customers, terabytes of static and dynamic data is generated  and  utility servers require  sophisticated data processing applications which can  take up the challenges including data  capture and analysis, storage and transfer, qu erying and information privacy ( Shyam J. Dhoble1 , Prof. Nitin  Shelke2. 2015) Using Apache Hadoop framework to accomplish these tasks suits well for distributed storage an d  distributed processing of very large data sets on computer clusters built from commodity hardware. Hadoop being  fault tolerant will duplicate received data and gives to multiple servers in different locations (cloud data storage)  based on the replication  factor. Thus we have reliability and fault tolerance within the system which prevents loss of  critical data during server failures or similar situations.   Even though demand response scheduling can be implemented for all the devices in the house, as the power  requirement by those devices are negligible, only HVAC loads are considered in this paper.  Unlike other devices  HVAC loads’ power demand is not consistent and it does not follow a pattern. This is because of the variation in ",uncc.edu,University of North Carolina at Charlotte,United States,35.311795,-80.741203
87,,@donkennedy.ca,"Engineering Management Paradigm, Hiring Practices, Knowledge Worker Productivity ","Einstein’s theory of relativity recently celebrated its 100th birthday.  Frederick Taylor’s scientific management is  just as old. In those hundred years, most paradigms of the natural sciences are solidifying and any challenges to the  established body of knowledge become rarer .  Within management theory over the same timeframe, various  paradigms co-exist and many do not grow beyond the initial proposal.  Some fall out of favor, but newly packaged  versions of the old ideas will reappear and regain wide acceptance.  Throughout the debates, practitioners continue  to lack proven tools to help with many of the common problems encountered in the engineering management world.  I know of McGregor’s Theory X and Theory Y, but which one is better for the organization I am in?  How can I tell  if I need more people, less people, or different people?  Can I have a happy workplace and a productive workplace  or do I need to pick one? Why does my rival manager keep receiving full funding for tasks my people could do with  half the resources?  Should I measure performance in my subordinates or assume everyone has special talents to  bring or should I view variations as random factors outside the workers’ control? Although no clear answers  currently exist for these questions, it is hoped that this paper will help spur a solidifying management paradigm that  will strengthen over time.     Keywords  Engineering Management Paradigm, Hiring Practices, Knowledge Worker Productivity    Introduction  While proposing his theory that scientific paradigm shifts resemble political revolutions, Kuhn (1970) states that the  physical sciences have been retested in modernity to a point of approaching maturity.  He predicts that changes to  the established paradigms in the physical sciences will be less dramatic and less frequent in the future.  As an  example, he doubts the theory that matter is composed of atoms and molecules is likely to be replaced by a radically  new idea.  Kuhn also notes that the social sciences have not reached maturity and there will continue to be major  shifts in this area for the leading paradigm at any given time.    McGuire (1982) outlines how prevailing management theories do change over time, although there is  seldom a strong contender for a majority view for very long.  He states that academics in the management field are  subjected to a number of opposing idea, Taking a broad view of these competing paradigms can lead to confusion.   Although written more than thirty years ago, the lack of a mature management paradigm keeps McGuire’s words  fresh: “professors of management …still experience the pain and guilt that affect good people who cannot help  those for whom they have some responsibility.”  Following Kuhn’s idea of paradigm shifts, academics of any field  who unknowingly support a dead end theory may  delve into studying fine details and avoid recognizing the  inconsistencies.  For example, the discovery that rust weighs more than iron prompted some 18th century scientists  to propose the existence of an element called anti-phlogiston with negative weight to explain this observation.  This  led to much research into the properties of anti -phlogiston.  It took an outsider to propose our modern theory of  combustion and the existence of oxygen to bring the end of the prior train of study into matter with nega tive weight.   Kuhn notes that social science theory has not yet developed a paradigm to overtake the numerous opposing  viewpoints.  We will remain in a state of uncertainty in management until a more unshakeable base is developed.   Former ASEM conference r egular, John Whittaker (e.g. 1992) proposes that good managers are able to  develop their processes to work around most established protocols.  An example is at organizations with annual  budgets that go to zero at the end of the year.  After a few cycles, the good managers will modify their spending to  either advance or delay spending of specific items to hit whichever budget year can handle the purchase.  Whittaker  stressed the dangers of actually believing the underlying theories leading to the protocols that require such  Copyright, American Society for Engineering Management, 2016 ",donkennedy.ca,,,46.3144754,11.0480288
88,CONTINUOUS FUTURES CONTRACT DATA FOR COMPUTATIONALINTELLIGENCE,@mst.edu,"Soybean futures, Gann contracts, neural network, forecasting, rolling contracts ","Given that futures contracts have short durations, data manipulation is needed to create longer price history fo r back  testing when developing forecasting models . Various approaches have been used to develop  longer datasets, each  with its own advantages and disadvantages. A research study was conducted to investigate three different approaches  for creating longer an d continuous soybean futures data sets: the Gann method, the nearest-contract method, and the  back-adjusted contract method . Although t he Gann method has received little recognition due to possible  disadvantages with the rolling methods, low volume, and low open interest, the results show that creating a Gann  contract rolled in the manner proposed  creates a method that is a viable alternative to the other approaches tested for  long-term trading.    Keywords  Soybean futures, Gann contracts, neural network, forecasting, rolling contracts    Introduction  A large amount of scientific, business , and financial data is represented in time series . Given the presence and the  increasing size of time series data, there has been a surge in the interest of time series research (Grabusts & Borisov,  2009; Ratanamahatana, Lin, Gunopulos, et al., 2009).   The Chicago Mercantile Exchange offers a wide range of futures contracts , including agriculture, energy,  interest rates, and metals, just to name a few. Agricultural commodities have gained interest from speculative traders  for inflationary measures , as well as a means to provide portfolio diversification. In addition, agricultural  commodities are used by hedge traders to protect against price changes in both products and by-products.   Futures contracts have a limited life span and specific delivery months. There are seven  different soybean  futures contracts. T he contracts are based on the month of expiry , including: January, March, May, July, August,  September, and November. For  example, the August 2016 Soybean Future contract began trading on November 15,  2013 and will end on August 12, 2016, with the contract trading during this time period . Due to the first and last  trade dates of soybean futures, the amount of daily price points can range from approximately 350 to 750 data points  for each contract.   Various types of mathematical models have been used to help forecast the prices of financial  assets,  including statistical approaches, as well as those that utilize computational inte lligence. In particular, artificial neural  networks (ANN) have continued to remain a popular computational intelligence approach for financial forecasting.  Yet, for ANNs to be effective, they  require extensive historical data, on the order of roughly 30 times  the number of  weights in order to avoid overfitting (Sarle, 2002). This means that even a simple feed-forward ANN can require, at  a minimum, 1500 data points to properly train the network. With daily futures data having only 350 -750 data points,  continuous data contracts must be created.   During actual trading, the method of rolling over futures contracts requires  closing out one futures contract  position and entering into a new position. This is done to keep a  hedge position in place, or to stay active  in the  market. However, for backtesting and model development using computational intelligence , data handling is needed  in order to create a longer price history. Price distortion, inadequate open interest , and volume at the beginning or  end of contracts can cause trading mishaps and useless data. In the past, r esearchers have used different methods for  creating continuous time series for back testing  futures data. The nature of this research is to study th ree different ",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
89,TOTAL PRODUCTIVITY MANAGEMENT: ANALYSIS OF ANDDISCUSSION ON THE NATURE OF INPUTS AND OUTPUTS,@ttu.edu,"Total Productivity, Input, Output. ","There has always been the need to assess productivity at the organizational level ; companies want to know how  effectively and efficiently they are utilizing their resources. The challenge however has consistently  been in finding  a balance between accuracy of the measurement metric and its simplicity/ease of use. The concept of Total  Productivity (TP), which is the sum of all tangible outputs divided by the sum of all tangible inputs, has the potential  to address this need. However, before TP modeling can be applied, a discussion on the nature of inputs and outputs  is in order, especially when considering that these variables will be sourced from company financial statements . As  it stands, since the time the concept of Total Productivity Management  was proposed, the nature of a typical  organization has undergone a significant change  and s o have the methods of conver sion of  inputs into outputs.  Given this scenario, only clear, well-demarcated definitions of the various input and output variables can ensure tha t  no overlap exists,  thus allowing for the computation of TP that is more accurately descriptive of the actual  productivity scenario. Therefore, this work reviews the theoretical underpinnings of the major variables that  constitute TP and attempts to adapt them to better reflect the nature of today’s organization.    Keywords  Total Productivity, Input, Output.    Introduction  Performance measurement can be justified for several reasons , but one of several fundamental questions it seeks to  answer is “how efficiently and effectively are inputs being converted into outputs”. As the competition to capture  and/or maintain market share increases along with a concomitant incre ment in the acceptance of the fact that input  resources are limited, organizations feel an ever -increasing pressure to realize their primary goal, making profits  (Goldratt, 2014).   Of the several ways profits can be realized, one is to improve productivity and realize more from  the same  level of or fewer resources, and in that endeavor Total Productivity Management (TPM gmt), originally proposed by  Sumanth (1979), is a performance measurement method that allows for simple and relatively accurate prediction of  an organization’s productivity scenario. TPMgmt, as Sumanth suggests, is both “diagnostic and prescriptive” (1984,  p. 109) in that it helps both measure productivity and suggest possible improvement areas.  There are several benefits  of TPMgmt that make it really appealing  to managers in the industry who are pressed for time. For one, TPMgmt is  conceptually intuitive, does not trade off accurac y by much for simplicity, and provides a holistic, gestalt picture of  the productivity scenario in that it considers the interactions between the various factors of input and output. An  example of the holistic attribute of Total Productivity (TP) is as follows: a traditional manufacturing company,  whose production equipment might be depreciating faster in both ability  and value, will exhibit lower TP if it does  not replace or frequently repair such capital -intensive equipment given that the adverse effects of such an act may  include among other things increased energy costs and higher material costs due to lower quality output. TPMgmt   also allows for the measurement of  productivity at sev eral levels of an organization (corporation, division, plant,  department, product lines, work center, machine, and task) , and is struc tured in a way that  it permits deconstruction  of TP at an aggregated level to TP at a lower level such as product lines and vice versa. In addition, in conjunction  with p artial productivities , TP helps determine the various elements of input or output that  need attention or  improvement. The reader is referred to Sumanth’s (1984; p. 168) work for a deeper understanding of how TP works  at product and aggregated levels.  Another major advantage of T PMgmt is that it permits comparison of TP across  time periods accounting for factors of inflation thereby determining trends in productivity over time.  Besides these ",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
90,RELIABILITY PREDICTIONS OF PROTOTYPES OF COMPLEXSYSTEMS,@hbv.no,"Reliability, prediction, demonstration, prototype, complex, system. ","Many companies face the same challenging problem: how to perform reliability predictions in the early design phase  of complex systems, when the first prototypes become available. The reduced populations ( no more than a few  prototypes) make it difficult to perform tests from which conclusions can be drawn with sufficient statistical  relevance. Yet, they are essential in order to gain sufficient insight on the expected performance of the system, before  its architecture is frozen. This paper presents a suggested  approach to  developing a reliability model that allows  ongoing predictions of complex systems when the fi rst prototypes become available.  This approach leverages the  undertaking of a reliability growth program in two key aspects. The first is the division of the system elements into  three differentiated groups, based on their origin and previous utilization. The second is the identification of the  larger populations to which all system’s elements belong. These two aspects facilitate a six -step approach that  includes t he gradual development of demonstrations, relying when needed on accelerated tests, which lead to the  identification of the life distributions of the elements. The result is the capability of performing reliability  predictions, which pave the way for a reliability growth program and a continuous improvement of the design until  there is reasonable evidence that the specified requirements have been met and the proposed architecture can thus be  frozen.      Keywords  Reliability, prediction, demonstration, prototype, complex, system.      Introduction  Systems are designed to fulfill identified needs or perceived opportunities. The need or opportunity is stated in the  form of stakeholder requirements, covering the end user and all other groups of individuals that can either influence  the performance of the system, or be affected by it. The stakeholder requirements, which are solution -independent,  are translated into system requirements for the chosen design concept, together wit h their verification methods (Sols,  2014). Those system requ irements are solution -dependent and are stated at system level. Among the main  taxonomies for requirements are: (1) the domain to which they belong (correlated with the stage in the life cycle at  which the requirements are generated) , and (2) the focus on the definition the needed or desired performance of the  system. The former differentiates between the stakeholder and the system requirements, whereas the latter  distinguishes between the functional and  the non-functional requirements. Functional requirements are those that  express what the system is to be able to do or perform, while  non-functional requirements are those that define  overall qualities or attributes of the system or of a relevant part of it (like a subsystem). In other words, functional  requirements describe what the system is to do, while the non-functional requirements set conditions or constraints  on how the functional ones are to be implemented (Blanchard, 2008; Sage, 1992; Sols, 2014).  Dividing requirements in terms of the domain they belong to, and how they address the needed  performance, leads to four quadrants, as shown in Exhibit 1. The quadrants on the left hand side are the functional  and non-functional requirements, still in the  problem domain, whereas the two right -hand side quadrants represent  the translation of those solution -independent functional and non- functional requirements to solution -specific, or  system requirements. The system requirements feed the functional analysis  that leads to the identification of the  necessary system elements (Blanchard, 2008; Blanchard & Fabrycky, 2010; Sage, 1992; Sage & Armstrong, 2000) .  The identified elements, with their characteristics and inter -relationships, define the architecture of the system. The ",hbv.no,,,46.3144754,11.0480288
91,M,Missing,,,Missing,,,46.3144754,11.0480288
92,IMPROVING ARMY AVIATION MAINTENANCE ONE PART AT ATIME: A LEAN SIX SIGMA APPLICATION,@gmail.com,,"In the early 2000s, the Army adopted the Lean- Six Sigma (LSS) methodology for process improvement and has  been training LSS practitioners ever since.  A s part of this training, the Army requires black -belt level candidates to  successfully complete a project using the Define, Measure, Analyze, Improve, and Control (DMAIC) process.  This  paper presents the results of a six -month effort that applied this met hodology to the turn- in process for recoverable  repair parts in an Army Combat Aviation Brigade (CAB).  Initial analysis found that the unit had over $1.7M  in un- matched items that counted against the unit’s operating budget with secon dary impacts on train ing and rea diness.   Through the DMAIC process, the team conducted detailed analysis of the available data, identified several areas for  improvement, and implemented controls and procedures to get the process in control and reduce the number of  defects in t urn-in documentation.  The results of the effort were profound and reduced the un-matched items from  54% to 3 % and improved the sigma quality level from 1.4 to 3.1.  Additionally, the teams w ork recommended  changes to the Army’s Global Combat Support Syste m – Army (GCSS -Army) and identified enterprise level  changes to the Army’s turn -in procedures that the Army ’s Combined Arms Support Command is adopting.  The  paper provides an overview of the process and its application to the problem of un- matched turn-in of parts as a case  study for a successful implementation of the Army’s LSS process.    Key Words  Lean, Six Sigma, Army Maintenance and Supply    Introduction  A typical Army Combat Aviation Brigade (CAB) is comprised of 110 advanced helicopters distributed among three  major types, the AH -64 Apache, UH -60 Blackhawk, and the CH -47 Chinook.  Given the high operational tempo  asked of the Army today, e ach they has thousands of parts and require significant maintenance time and repair parts  to keep running.  A Brigade’s budget for repair parts is about $ 80M per year.  T he Army designates a significant  portion of these repair parts as recover able items because it is mo re cost effective for vendors to repair a damaged  part than manufacture one from scratch.  In order to receive “credit” for a recoverable repair part, a CAB must return  the unserviceable part and request a credit payment in GCSS -Army.  In the case of the CAB this paper examines,  23% of the unit’s overall operations and maintenance budget  is received in the form of credit payments .  Losing  these credits would have a profound impact on the unit’s training and readiness therefore credit retention  became a  key area of emphasis for the brigade’s leadership.  Since the early 2000s, the Army has implemented Lean- Six Sigma (LSS) as the standard process  improvement methodology across the Army.  Since its inception, the Army has credited the LSS program with  millions o f dollars and thousands of person -hours of savings through hundreds of LSS projects.  As part of the  program, the Army requires black -belt candidates to lead and complete a real -world project that applies the LSS  methodology to a significant problem facing  the Army.  This paper presents a case study of the application of the  LSS methodology to the problem facing an CAB in its process for the turn -in of recoverable repair parts.  The  results of the project were significant and the Army ’s Combined Arms Support Command produced a training aid to  share the lessons learned through this project across the enterprise as best practices for the turn -in process.    Literature Review  This section presents a review of the relevant literature to include a discussion on the  Army’s maintenance and  supply procedures, lean thinking, and the six -sigma methodology.  It describes the turn- in process for the Army  maintenance system that is the focus of this work and the regulations that govern that process.  Additionally, this ",gmail.com,,,46.3144754,11.0480288
93,COMPARING CORPORATE SOCIAL RESPONSIBILITY IN THE USAND EUROPEAN COMPANIES BASED ON GRI STANDARD,@csun.edu,"Corporate Social Responsibility, American and European Automotive, GRI ","Corporate Social Responsibility and Suitability reporting has become increasingly important in recent years, especially  in Automotive industries. Corporate social responsibility (CSR) is firmly connected with the principles of Sustainable  Development which help organization to make a decision based on Economic, Social, and Environmental aspects. On  the other hand, Global Reporting Initiative (GRI) is a leading organization in the sustainability field, which produces  guidelines on how organizations  should report their environmental, social, and economic performance as well as  sustainability initiatives.   One of the essential reasons that Organizations are progressively undertaking activities that they hope would  result in better social, financial, and environmental benefits as this is being increasingly demanded by the customer,  particularly by the millennials. By overseeing and looking to enhance natural, social, and financial execution and good  governance throughout their supply chain, organizations are likely to ensure the largest impact, which is critical when  trying to incorporate any aspect of sustainability.  In this paper, the authors have done a  comparison   of corporate  social responsibility based on GRI standards. This includes analyzing sustainability reports of American and European  Industries.  This analysis has enabled the authors to identify whether the environmental and social aspects of CSR in both  American and European Automotive companies, are accordance with the G4 version of GRI guidelines. The authors  have also made recommendations on how both the environmental as well as the social aspect of the Global Reporting  Initiative (GRI) can be improved.  Keywords  Corporate Social Responsibility, American and European Automotive, GRI    Introduction  Corporate social responsibility (CSR) is defined in many ways. CSR is firmly connected with  the principle of  sustainable development. The purpose of CSR in the organization is to assist in decision making based on economic,  environmental, and social aspects. The terms “CSR” and “sustainability” are used interchangeably, and there is some  confusion about what CSR and sustainability are. The comparison between CSR and sustainability can be considered  in some aspects, like vision, targets, business, management, reward, and drive. Some of these aspects are described  below (Last, 2012).    Vision  “Corporate Social Responsibility (CSR) looks backwards, reporting on what a business has done, typically in the last  12 months, to make a contribution to society. While, sustainability  looks forward, planning th e changes a business  might make to secure its future (reducing waste, assuring supply chains, developing new markets, building its brand”  (Last, 2012). But for CSR to be truly and strategically effective, it must look forward as well. As new operations are   planned, the company affected must make sure their new technologies, processes, and raw materials will not adversely  affect an environment or the people and lifeforms within the environment. Sustainability does indeed look to the future  in that we would a ll like to make sure we preserve what we have today for future generations, but it is principally  effective because of the history of past adverse effects. Thus , sustainability looks backward as well. I propose a new  21st-century aspect of CSR and sustainability that look both  forward and backward. This type of thinking solves  problems before they become process-involved and certainly before they become issues of litigation.       ",csun.edu,"California State University, Northridge",United States,34.2455346,-118.52632210641266
94,DISCOVERING THE RELATIONSHIP BETWEEN EMOTIONALINTELLIGENCE AND ORGANIZATIONAL COMMITMENT: PILOTSTUDY RESULTS AND IMPLICATIONS FOR FURTHER STUDY,@uah.edu,"Emotional Intelligence, Organizational Commitment, Pilot Study, Lessons Learned ","One of the most important assets of an organization is committed employees.  The literatur e has many studies that  illustrate employees that are organizationally committed are more apt to have better job performance/higher  productivity, lower turnover, less absenteeism, and other positive effects.  In light of this, multiple investigations  have been performed examining a wide array of factors that could possibly influence Organizational Commitment  (OC).  An area that has been rarely researched is Emotional Intelligence (EI) which involves being aware of one’s  own and others’ emotions, understandi ng and regulating those emotions, and utilizing emotions constructively to  create and maintain positive working relationships.    Since EI can be learned and improved through training efforts, it is important to better understand this  EI/OC relationship and  how engineering managers and leaders can capitalize upon this knowledge thereby  contributing more to organizational success. A pilot study investigating this relatively unknown relationship  in the  field of Engineering Management has been performed by the author in preparation for a much larger study that will  specifically target high technology workers.  This paper will discuss the background, design, and results of the pilot  study including lessons learned, implications, and how this knowledge will influence the design of the next study.    Keywords  Emotional Intelligence, Organizational Commitment, Pilot Study, Lessons Learned    Introduction  Organizations are in a state of employment culture flux with more and more baby boomers retiring each day and  younger generations entering the workforce.  There has been a paradigm shift in the concept of long term  employment with a single company.  Baby boomers, typically defined as post -World War II babies born  approximately between 1946 and 1964, range in age between 5 1 and 70 now (Brandon, 2014).  This generation had  the mindset that they would likely spend their entire career working for only one company and would retire with a  gold watch.  Younger employees, however, are more prone to changing employers, sometimes mu ltiple times, to  align with their personal beliefs, career advancement aspirations or other goals.  They typically do not feel the sense  or extent of loyalty (i.e., commitment) to an organization that their parents and grandparents exhibited.  In fact, a  survey of nearly 7,700 millennials representing 29 countries found that “Millennials, in general, express little loyalty  to their current employers and many are planning near -term exits ” (Deloitte Touche Tohmatsu Limited , 2016).    Specifically, the survey statistics revealed that if given the choice during the next year, 25% of millennials would  leave his or her current organization to do something different .  Over  the next two years, the number increases to  44% and by the end of 2020, 67% of millennials hope  to have moved on.  Only 16% of millennials expect to be with  their current employers ten years from now.   This turnover phenomenon, while perhaps beneficial to a person, can  cause havoc to organizations in terms of financial costs, productivity losses, in creased workload on remaining  employees decreasing morale, and loss of intellectual property.  Since millennials now represent the largest segment  of the United States workforce (Deloitte, 2016) , this should motivate organizations to determine how to incre ase ",uah.edu,University of Alabama at Huntsville,United States,34.7252,-86.6405
95,SIMULATION-BASED INVESTIGATION FOR THE APPLICATION OFMICROGRID WITH RENEWABLE SOURCES IN MANUFACTURINGSYSTEMS TOWARDS SUSTAINABILITY,@mst.edu,"Sustainable manufacturing, microgrid, onsite, renewable ","Existing research on the application of the microgrid with renewable sources mainly focused on the residential  buildings and critical facilities, while largely neglecting the industrial manufacturing sector. Indeed, manufacturing  industry contributes a large portion of energy consumption and greenhouse gas emissions to the environment. In  addition, the manufacturing industry  is also affected the most by power outages. The utilization of Microgrid  technology can reduce the environmental impact s and improve the resilience of the manufacturing activities. In this  paper, a simulation model including both a manufacturing system module and a microgrid module will be established  to investigate the feasibility of the application of microgrid technology in manufacturing. A real auto component  manufacturing system will be simulated. Different renewable resources with different sizes serving as onsite  generation sources to provide the energy to the manufacturing system will be examined regarding their financial  performance and energy generation capability.    Keywords  Sustainable manufacturing, microgrid, onsite, renewable    Introduction  A microgrid is a localized energy system that consists of distributed energy sources and loads. It is able to operate  either completely separate from, or connected to, the existing utility power grid  (Mahieux & Oudalov, 2015; U.S.  DOE, 2014;  Lawrence Berkeley National Laboratory , 2016) . The energy supply of the microgrid can be sourced  from renewable energy, such as wind and solar. Usually, the microgrid is connected to the traditional utility grid so  that it can satisfy the electricity demand with external utility when the load surpasses the energy supply of the  microgrid. It can also sell power back to the utility grid when the microgrid generates more than the on-site needs.  Microgrids can relieve the possible disturbances when the utility grid is down. Thus , the reliability,  affordability, resilience, and security of energy supply to end users can be greatly improved. In addition, it can also  reduce the greenhouse gas (GHG) emissions and relieve the stress on transmission and distribution systems.  Therefore, some pioneer microgrid projects have been implemented in residential housing  (Ahourai & Faruque,  2013; Roggia, Rech, Schuch, Baggio, Hey, & Pinheiro, 2011; Hawkes & Leach, 2007) and some critical facilities,  such as medical centers, financial corporations, military bases, and jails (NYDHSES, 2014; Stadler, 2014).  As for the industrial sector, it accounts for one third of total energy consumption in the United States  (U.S.  DOE, 2010), and manufacturing activities dominate energy consumption and GHG emissions in industrial sector  (Duflou, Sutherland, Dornfeld, Herrmann, Jeswiet, Kara, Hauschild, and Kellens, 2012) . Many research towards  sustainable manufacturing has been implemented to reduce the energy consumption or energy cost for manufacturing  activities through process level control ( Li, Kara, & Quresh, 2014; Winter, Li, Kara, Herrmann, 2014; Li, Winter,  Kara, & Herrmann, 2012 ) and production system/plant level operation optimization  (Sun, Li, & Dababneh, 2016;  Sun & Li, 2013; Wang & Li, 2014). Most endeavors are focused on the manufacturing side, while the utilization of ",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
96,UNDERSTANDING ENGINEERING LEADERSHIP:A CRITICAL REVIEW OF THE LITERATURE,@montana.edu,"leadership, professional skills ","Many of the greatest challenges facing society today require technical solutions that can only be created thro ugh  collaboration within interdisciplinary teams. For these collaborations to effectively harness the capabilities of groups  that may not normally work together, effective technical leadership must be deployed. Thus, the need for engineering  leadership and the development of engineering leaders - those who can communicate effectively and perform well in  leadership roles. This need for engineering leaders continues to gain wider recognition and acceptance within the  engineering education community, as evident by the development and growth of the Engineering Leadership  Development Division within the American Society of Engineering Education and the fact that more than half of the  current desired ABET student outcomes address items beyond design and analysis.      This increased activity level to develop greater leadership skills in engineers is a positive step to meeting society’s  needs. However, like the general study of leadership, it appears that engineering leadership does not yet have a  common definition or  conceptualization in the literature. This study examines the current versions of engineering  leadership available in the literature, compares those models with established models of leadership, and makes  recommendations for future study in the area of engineering leadership.    Keywords  leadership, professional skills    Introduction - The Need for Engineering Leadership  Many of the greatest challenges facing society today require technical solutions that can only be created through  collaboration by interdisciplinary teams (National Academy of Engineering, 2015) .  For these collaborations to  effectively harness the capabilities required from a diverse group of technical professionals not accustomed to working  together, effective technical leadership must be dep loyed. This increasing need for leadership is inclusive of all  technical professionals, but is especially important for engineers who are often expected to lead design and integration  work, which are critical steps to solving society’s greatest challenges. Thus the need for engineering leadership.     At the same time as the need for engineering leadership is becoming more acute, today’s young professionals are  growing into the dominant segment of the workforce.   A  recent survey related to characteristics of the millennial  generation, notes that millennials will comprise 75 percent of the global workforce by 2025 and this group believes  businesses are not currently doing as much as they could to develop their leadership skills (Deloitte Touche Tohmatsu  Limited, 2014).  Deloitte also notes that the need for business and industry to nurture future leaders is particularly  critical since organizations cannot count on millennials biding their time until senior positions arise.  The report notes  that almost one in four millennials are ‘asking for a chance’ to show their leadership skills and 50 percent believe their  organizations could do more to develop future leaders.  Assuming millennial engineers share similar perspectives with  their peers, we infer that many would echo the same sentiments found in the Deloitte survey.     While the need for engineering leadership  is real, it has received relatively little attention in engineering education  until recently, with most publications on the topic and formal programs to develop it appearing since 2004 (Graham, ",montana.edu,Montana State University - Bozeman,United States,45.6638859,-111.07928704602077
97,ELECTRIC POWER REGULATION: A COMPARISON BETWEENBRAZIL AND THE UNITED STATES,@aggies.ncat.edu,"Regulation, electric power, comparison, tariff ","One of the components involved in the measurement of a country’s economic development is the access its  population has to infrastructure services. The electric power supply and consumption ar e important factors in this  assessment. The electricity industry is regulated in many countries around the world, but the procedures and the  levels of regulation vary among them, due to differences in the political, cultural, and economic contexts. One of the  main responsibilities of the regulators is to determine the methodology to calculate the price that the customers will  pay for energy, or the tariff. The procedures used in Brazil for this calculation are being discussed, and it has been  discovered that some companies are not being properly remunerated. Therefore, the objective of this study was to  make a comparison between the regulatory procedures of the electric power sector in Brazil and in the United States,  and evaluate if there are some ideas that could be used within the context of Brazil. Information about both  electricity sector models  was collected and  similarities and differences  were identified. The study showed that the  regulatory procedures used in the U.S. are more flexible and better contemplate the companies’ characteristics. The  second part of the study was a quantitative analysis of the tariffs determined by electric power conces sionaires in the  two countries and economic indexes of their respective populations.     Keywords  Regulation, electric power, comparison, tariff    Introduction  Energy services are essential to human well -being and to a country’s economic development (International Energy  Agency, 2011). Access to energy not only promotes economic growth, but also fosters social deve lopments and  alleviates poverty (International Atomic Energy Agency; United Nations Department of Economic and Social  Affairs, 2007). Currently, it is estimated that 17% of the world population does not have access to energy. On the  contrary, in further developed countries such as the United States, 100% of the population has access to this utility.  In developing countries, a large portion of the population does not have access. Among the less developed countries,  Latin America has the highest percentage of access, listed at 95% (International Energy Agency, 2015).  In this context, measures related to energy production, consumption, and purchasing price play important  roles in the development analysis. Another statistic commonly used to measure the relations hip between energy use  and economic development is a  country’s overall energy intensity, given by the ratio of total energy consumption t o  GDP (Gross Domestic Product) (International Atomic Energy Agency; United Nations Department of Economic and  Social Affairs, 2007).   Data related to 2015 energy production and consumption in Brazil a nd the United States is  presented in Exibit 1.    Exibit 1.  Energy indexes. Source: Global Energy Statistical Yearbook 2015.    Country Production (Mtoe) Consumption (Mtoe) Energy intensity (koe/$2005p)  United States 4,330 3,830 0.15  Brazil 584 524 0.11 ",aggies.ncat.edu,North Carolina Agricultural and Technical State University,United States,36.077,-79.7718
98,UNDERSTANDING ENGINEERING LEADERSHIP:A CRITICAL REVIEW OF THE LITERATURE,@montana.edu,"leadership, professional skills ","Many of the greatest challenges facing society today require technical solutions that can only be created thro ugh  collaboration within interdisciplinary teams. For these collaborations to effectively harness the capabilities of groups  that may not normally work together, effective technical leadership must be deployed. Thus, the need for engineering  leadership and the development of engineering leaders - those who can communicate effectively and perform well in  leadership roles. This need for engineering leaders continues to gain wider recognition and acceptance within the  engineering education community, as evident by the development and growth of the Engineering Leadership  Development Division within the American Society of Engineering Education and the fact that more than half of the  current desired ABET student outcomes address items beyond design and analysis.    This increased activity level to develop greater leadership skills in engineers is a positive step to meeting society’s  needs. However, like the general study of leadership, it appears that engineering leadership does not yet have a  common definition or conceptualization in the literature. This study examines the current versions of engineering  leadership available in the literature, compares those models with established models of leadership, and makes  recommendations for future study in the area of engineering leadership.    Keywords  leadership, professional skills    Introduction - The Need for Engineering Leadership  Many of the greatest challenges facing society today require technical solutions that can only be created through  collaboration by interdiscip linary teams (National Academy of Engineering, 2015) .  For these collaborations to  effectively harness the capabilities required from a diverse group of technical professionals not accustomed to working  together, effective technical leadership must be depl oyed. This increasing need for leadership is inclusive of all  technical professionals, but is especially important for engineers who are often expected to lead design and integration  work, which are critical steps to solving society’s greatest challenges. Thus the need for engineering leadership.   At the same time as the need for engineering leadership is becoming more acute, today’s young professionals are  growing into the dominant segment of the workforce.   A  recent survey related to characteristics of the millennial  generation, notes that millennials will comprise 75 percent of the global workforce by 2025 and this group believes  businesses are not currently doing as much as they could to develop their leadership skills (Deloitte Touche Tohmatsu  Limited, 2014).  Deloitte also notes that the need for business and industry to nurture future leaders is particularly  critical since organizations cannot count on millennials biding their time until senior positions arise.  The report notes  that almost one in four millennials are ‘asking for a chance’ to show their leadership skills and 50 percent believe their  organizations could do more to develop future leaders.  Assuming millennial engineers share similar perspectives with  their peers, we infer that many would echo the same sentiments found in the Deloitte survey.   While the need for engineering leadership is real, it has received relatively little attention in engineering education  until recently, with most publications on the topic and formal programs to develop it appearing since 2004 (Graham,  2009). During the past six  years, the level of attention has increased dramatically with publication of a textbook on  engineering leadership (Bennett & Millam, 2013), over a dozen programs participating in the inaugural COMPLETE  Leadership Conference (Rice Center for Engineering Leadership, 2014) , the development of several named ",montana.edu,Montana State University - Bozeman,United States,45.6638859,-111.07928704602077
99,RESILIENCE FOR ENGINEERING MANAGEMENT: A STATE OF THEART MATRIX ANALYSIS OF SELECT LITERATURE,@oregonstate.edu,"Resilience, organizational resilience, resilience engineering, state-of-the-art matrix ","Resilient systems maintain regular function throughout a period of stress or disruption. Truly resilient systems are  learning systems that use lessons from disruptive events to improve performance. The literature presents numerous  factors that build or detract from resilient performance. Individual behavior is one such factor in social systems. It is  established that resilient social systems employ double -loop behavior, engage in continuous improvement through  organizational learning, and exhibit situationa l awareness. This paper uses a state- of-the-art matrix approach to  organize and analyze select literature in the field of organizational resilience. The components of resilient performance  are studied, with a focus on methods of quantitative measurement. T he analysis presented offers a basis for a more  complete view of the current state of research in resilience. While  frequently the focus of theoretical works, the   proactive and learn stages of resilient system performance are rarely addressed practical assessment. The analysis in  this paper  identifies the response stage of resilient performance to be most commonly addressed in the empirical  studies. A comprehensive mapping of resilience measures provides a useful tool for industry professionals aiming to  increase organizational resilience and set benchmarks for organizational learning and growth.      Keywords  Resilience, organizational resilience, resilience engineering, state-of-the-art matrix    Introduction  Resilience is the capability of a system to adapt in the face of internal and external changes (Holling, 1973). As regular  disruptions and organizational change are expected in today's world, the concept of resilience has gained relevance in  an organizational context. Disruptions to regular operations within an organization can be triggered  by emergency  events, such as natural disasters and terrorist events , or long-term periods of stress, such as economic downturn. The  literature often categorizes resilient systems based on their attributes and capabilities. Attributes are characteristics of  resilient systems that describe how the system  responds to a disturbance. C apabilities are measures of the extent to  which a system can effectively address, recover from, and prevent future disruptions. In this section, the attributes and  capabilities are defined and shown  through two illustrative examples in the fields of nuclear engineering and  healthcare.   Resilience research encompasses crisis management, risk management, and security, including the innate  strength of a system to handle a disaster or continued strain on operations. When a system breaks down, the recovery  is largely based on individual and collective decision-making power. For engineering managers, this implies that the  foundation of resilient systems is  a strong team  formed through well -defined role systems and distributed  decision- making power (Mallak, 1998). The field of Resilience Engineering (RE) emerged in the early 2000's  to address the  preventative and structural requirements of resilient performance. Resilience Engineering uses engineering principles  to design robust and adaptable processes, manage risk, and allocate resources in times of stress (Resilience  Engineering Association, n.d.).   With the rapid emergence and popularity of resilience studies, attempts at application of resilience concepts   have been seen across industries, including manufacturing (Heinicke, 2014) and healthcare (Nemeth & Cook, 2007).  Cross-discipline translation of resilience concepts  is essential for effective  application of resilience principles . As  such, there is a need to reduce inconsistency among definitions of resilience and refine key constructs in the literature  (Righi, Saurin, & Wachs, 20 15). Current frameworks to measure resilience are difficult to compare  and use varying  terminology. Moreover, the majority of measurement tools are industry specific and  frameworks to quantify system ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
100,SCHEDULE RISK MANAGEMENT FOR CONCRETE WORKS,@changwon.ac.kr,"Reinforced concrete construction, construction schedule risk, uncertainty, construction work process, Analytic ","As an outdoor industry , construction project has so many risks that have complex decision making process with  multiple participants. Especially, p re-construction decisions are very important for the success of a project, as they  concern the overall management of important issues such as  cost, quality, schedule, and safety, as well as resources,  such as manpower, materials, and equipment required in construction projects. As decisions made at the pre- construction phase have an enormous influence on a project throughout the entire process,  precise predictions of  uncertainties and risks at the pre -construction phase are essential for project success , especially for new delivery  systems such as BTL  (Build-Transfer-Lease) projects of  educational building construction  in order to win the  biddings of the projects.   Critical construction works’ risks occurring in the process of construction, especially a framing work, bring  to bear considerable influence on an entire construction duration. This study is thus to propose schedule risk  management for construction works by quantifying and ordering the weight of important risks using AHP (Analytical  Hierarchical Process); intends to determine the schedule risks potentially involved with critical construction works,  and help construction participants to reduce such dominant risks at the beginning of construction works. This study is  expected to help inexperienced field workers as well as experienced workers, manage schedule risks systematically  and effectively as a preventive purpose, whereby the risk man ager analyzes the risk factors of the past and  establishes risk management policy for addressing them in the future according to the risk priority order.     Keywords  Reinforced concrete construction, construction schedule risk, uncertainty, construction work process, Analytic  Hierarchy Process (AHP)    Introduction  Backgrounds and Objectives  Construction projects are initiated in complex and dynamic environments resulting in circumstances of high  uncertainty and risk, which are compounded by demanding time constraints (Mulholland & Christian, 1999). Delays  on construction projects cause financial losses for project stakeholders (Kim, Van Tuan & Ogunlana , 2009). Most  previous studies in construction project risk management have been focusing on the factors co ntributing to the  success of risk management, but little attention was given to factors significantly affecting decision makers’ risk  attitudes in construction projects (Wang & Yuan, 2011).   As decisions made at the pre- construction phase have an enormous influence on a project throughout the  entire process, precise predictions of uncertainties and risks at the pre- construction phase are essential for project  success, especially for new delivery systems such as BTL  (Build-Transfer-Lease) projects of educati onal building  construction in order to win the biddings of the projects.   Especially as a framing work process is a critical path in schedule, schedule risks occurring in the framing  work process bring to bear considerable influence on an entire construct ion schedule. Despite many disadvantages in  terms of construction time, reinforced concrete construction is increasingly being employed in the construction. R isk  probability of form work, reinforced concrete work, and concrete work  is very high. This study is therefore intended  to investigate structural work process that have the greatest influence on an entire construction project schedule, and  identify the probability of risk.        ",changwon.ac.kr,Changwon National University,"Korea, Republic of",35.2456,-128.692
101,BUILDING A PROACTIVE SCHEDULE RISK STRATEGY FOR ACONSTRUCTION COMPANY,@changwon.ac.kr,"Construction schedule, schedule strategy, construction company, construction duration, SWOT analysis,  ","World construction industry as well as Korea is very competitive due to the depressed construction market and the  changes of delivery method and system. These changes have necessitated that construction companies, both those  involved in domestic and the globa l arena, to establish multifaceted solutions in pioneering a new construction  industry environment such as BTL  (Build-Transfer-Lease) projects of  educational building construction . However,  there are numerous uncertainties involved in the construction busi ness, and these can cause project delays. Without  appropriately identifying a schedule risk and establishing a proactive schedule risk strategy for these liabilities, it is  difficult to occupy favorable competitive positions in the world as well as Korea.   In recognition of this, this current study aims to address these issues for implementing a competitive strategy in terms  of construction project duration, rather than formulating general strategies for construction companies. A  construction schedule strat egy is established on a SWOT (Strength, Weakness, Opportunity, Threat) analysis  considering the internal/external factors pertaining to Company J. Therefore the objective of this paper is to present a  proactive schedule risk strategy for competitiveness in  construction duration by using SWOT analysis including  evaluation of external environment pertaining to the construction industry and internal environmental of a  construction company. As more attention is being paid to construction duration, not only in K orea but worldwide,  this research provides some basic data for construction companies to establish and execute strategies for achieving  competitiveness in this area.    Keywords  Construction schedule, schedule strategy, construction company, construction duration, SWOT analysis,     Introduction    Backgrounds and Objectives  Diminishing size in the construction market, changes of delivery system, and recent shifts in the labor market have  all contributed to the increase in competition among construction companies  in domains such as BTL  (Build- Transfer-Lease) projects of  educational building construction . Consequently, construction companies  have now  realized the importance of improving their competitive advantage in terms of construction project duration . However  there are numerous factors conspiring to delay construction  project, and without a clear -cut strategy for managing  such uncertainties, it can prove difficult to establish competitiveness in the construction industry.  Despite this pressing need to form and execute a solution to these problems, construction companies have  not been particularly proactive as yet. As a construction company’s strategy for achieving increased competitiveness,  and the failure thereof, can affect each aspect of all cycle of construc tion project, it is important to consider all the  latent risks involved rather than implementing narrow tactical solutions. In other words, it is crucial to locate a  multidimensional strategy that accurately takes into account all the external/internal factors at play, and also  considers both the managerial and operational aspects of construction project.  Compared with other industries, the construction industry is more dependent to risks due to  the construction  characteristics, such long term duration, intricated processes, uncontrollable environment, financial risks and vibrant  organization (Flanagan & Norman, 1993; Akintoye &  MacLeod, 1997; Smith, 2003; Zou, Zhang, & Wang 2006).  Therefore, taking effective risk  management strategy for variable constructio n activities is very important for the  successful delivery of a project.   In order to solve the problem, recent studies have been performed. For example, on the basis of the Risk  Breakdown Structure (RBS) of the Project Management Body of Knowledge (PMBOK Guide) , it is tried to ",changwon.ac.kr,Changwon National University,"Korea, Republic of",35.2456,-128.692
