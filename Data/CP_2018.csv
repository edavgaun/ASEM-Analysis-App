,Title,email,KeyWords,Abstract,domain,Institution,Country,Latitude,Longitude
0,NETWORK ANALYSIS FOR THE SCHEDULING OF A BUS ROUTE:CYRIDE CASE STUDY,@gmail.com,"Network Analysis, Optimization, Operations and Supply Chain Management. ","This study aims to apply the concept of network flow in a real life problem. The application has been made by working  on a transport organization called CyRide, the bus network in Ames, IA, in the US. The study has been conducted on  two routes of CyRide called route numbers 6 and 6A. Both these routes basically cover the same roads and stops, but  at different times. Hence they both could be considered tog ether as a single route. This study focused on minimizing  the number of drivers working on this route. This study is defined as a scheduling problem that must be solved to  ensure optimum utilization of the organization’s resources. In this study, a network  model has been developed to  address the scheduling problem of a CyRide route so that the model can be solved using a shortest path algorithm.  System of difference constraints has been followed to develop the model. Real -life data has been used in the mode l  and the model has been solved using Excel Solver as optimization software. By knowing the optima l number of  drivers, the scheduling performance should be improved. This will lead to lowering the operating cost of CyRide.    Keywords  Network Analysis, Optimization, Operations and Supply Chain Management.    Introduction  Any organization needs to use scheduling for the optimum utilization of its resources. For example, a manufacturing  firm needs to schedule its production processes to optimize the use of its machines (production scheduling); a transport  organization needs to schedule its vehicles so that it can cover all the routes efficiently (vehicle scheduling); and a  health-care facility needs to schedule its doctors and nurses in order to meet the needs of the patients (staff scheduling).  Techniques of network flow can improve the performance of different types of scheduling significantly. Thus, this  project will be based on one of the three common types of scheduling: i) production scheduling by a manufac turing  firm, ii) vehicle scheduling by a transport organization, and iii) staff scheduling by a health -care facility.   Scheduling entails many different kinds of issues such as scheduling for people, machines, vehicles, and /or  places. Each of these types o f scheduling can be further divided into subtypes. For example, scheduling for people  could be further divided into scheduling of employees such as drivers, factory workers , and telephone operators  or  scheduling of customers such as patient.   Nowadays, employee scheduling is one of the most important issues in any organization . This is because  today’s management not only wants the organization to work effectively, but they also want to get the best schedule  to improve the organizational efficiency . An optimum scheduling of the employees can reduce the wastage of time  and other resources leading to the maximization of profit (Ernst et al., 2004).   In this study, we investigate the application of network modeling for the scheduling of drivers. Particularly,  we apply the concept of minimum cost network for the scheduling of CyRide routes in order to minimize the number  of drivers for the targeted route(s). It is expected that this study will reveal some interesting insights, which will be ",gmail.com,,,46.3144754,11.0480288
1,ASSESSING GRADUATE PROGRAMS IN ENGINEERINGMANAGEMENT,@usma.edu,"Engineering Education, Assessment, Graduate Education ","Most assessment in engineering education is driven by ABET and to a lesser extent regional accreditation.  Therefore,  assessing student outcomes and learning objectives is the focus of most  university assessment systems.  However,  nontraditional, customer driven executive education (to include online) assessment systems must e nsure quality  expectations are being met with a focus on  customer satisfaction  using a quantitative and sustainable assessment  methodology.  T his paper address es what needs to be assessed for customer satisfaction such  as content, customer  service, faculty, facilities, etc.  We will also present several scoring methodologies, along with pros and cons,  as the  best means to track and score assessment data  along with candidate questions to ensure stakeholder satisfaction is  being properly captured.  Like any expensive high-end product, professional graduate programs must maintain a high  level of customer satisfaction.  This paper is meant to present a set of criteria, questions, process es, and scoring  methodology for that purpose.     Keywords  Engineering Education, Assessment, Graduate Education    Introduction  Few will argue that the landscape of engineering and thus engineering education is changing.  With the emergence of  online learning, for profit universities, massive open online courses or (MOOCs), professional degrees at the post  masters taught in nontraditional formats, onsite delivery, etc. , universities must evolve and adapt to  market forces.   Engineering management (EM), much like the Masters of Business Adminis tration (MBA) programs, is especially  susceptible to these market forces because of their customers, role within a university, and their nontraditional legacy.  As shown by the systemigram in Exhibit 1, long gone are the days of traditional graduate programs taught on campus  in a lecture format.  Map the  current educational  landscape onto the evolving nature of engineering (see Exhibits 2 and 3),  graduate engineering education has become complex and segregated into research and customer driven programs.   Even the traditional Tier I research focused universities have developed nontraditional formats with a target audience  of the working professional. These customer driven programs must deliver relevant and connect content, be flexible,  and meet the customers’ needs in terms of delivery, content, and cost.     ",usma.edu,United States Military Academy,United States,41.3927227,-73.95986305044411
2,APPLICATION OF LEAN SIX SIGMA TO REDUCE REPEATEDHANDLING OF MATERIAL AT TOBYHANNA ARMY DEPOT,@usma.edu,"Lean Six Sigma, process improvement, repeated materials ","The Tobyhanna Army Depot (TYAD) focuses on the maintenance and support of command and control systems across  the Department of Defense (DoD).  Lean Six Sigma (LSS) uses the Define -Measure-Analyze-Improve-Control  (DMAIC) process to facilitate improvement in TYAD’s fabrication of components by eliminating waste and  streamlining manufacturing.  The United States Military Academy’s (USMA)  LSS capstone team partnered with  TYAD Directorate of Continuous  Process Improvement on a Lean Six Sigma project to evaluate and reduce the  repeated handling of raw materials and end items associated with the Machining Branch which contribute to high  transportation and storage co sts.  The application of  DMAIC process to a process with low  production volume of a  high variety of products demonstrated the applicability of the LSS process to reduce the number of repeated material  runs for end items and raw materials by 50% in TYAD’s Machine Shop, which resulted in an estimated annual cost  savings of $71,500, while gaining insight on stakeholder interactions.  USMA’s LSS team’s work allowed for an  increase of utilization of LSS for Army applications.    Keywords  Lean Six Sigma, process improvement, repeated materials    Introduction  The Tobyhanna Army Depot works to provide “full-spectrum logistics support for sustainment , overhaul and repair,  fabrication and manufacturing, engineering design and development, systems integration, Post Production, Software  Support, technology insertion, modification, Foreign Military Sales and Global Field support” (“About Tobyhanna,”  2017).  Tobyhanna focuses on the maintenance and repair of Command, Control, Communications, Computers,  Intelligence, Surveillance and Reconnaissance (C4ISR) systems for all services within the Department of Defense.   Within TYAD, the Depot’s various manufacturing shops seeks to align themselves with the Army’s Office of Business  Transformation (OBT) to reduce waste within the Army, seeking to “make the business side of the Army as efficient  as the war-fighting side is effective” (Schmidt).  USMA’s LSS team employed the DMAIC process to reduce repeated  turn-ins on requested material in the Machine Shop within the manufacturing department. Due to the Depot layout,  which has a significant distance between the warehouse containing the raw materials and  the Machine Shop, there  was a significant cost impact on production due to the repeated moving of materials used for production and excess  material sent back when not fully used. The team’s goal was to streamline and improve the process in order reduce  the costs due to this wasteful movement for the Machine Shop.  The application of the DMAIC process in the Machine  Shop sought to reduce the repeated pick up of raw materials (picks) from the warehouse by 50%, and also considered  practical ways to batch end items together on a single raw material to reduce the repeated picks and returns of materials  that were used on many small production runs.  The application of LSS principles on a production of high variety, low  volume products demonstrated the applicability of the LSS process as it pertains to OBT’s mission to eliminate waste  throughout the DoD.  ",usma.edu,United States Military Academy,United States,41.3927227,-73.95986305044411
3,CREATION OF A DYNAMIC MODEL BASED ON PERFORMANCEINDICATORS OF AN ORGANIZATION,@deacero.com,"Performance, Management, Indicators, System Dynamics. ","The understanding of existing interrelationships between performance indicators within a Performance Measurement  & Management System (PMMS)  is an ongoing topic of research. There have been many attempts to deepen the  understanding of the relationships between performance indicators although this is not always well defined in  PMMS methodologies. This paper presents a structured approach for quantitatively establishing these  interrelationships based on a current set of indicators of an organ ization, through the creation of a dynamic model  that simulates their behavior based on Interpretative Structural Modeling (ISM) , autoregressive exogenous  regression models (ARX) and System D ynamics theory. The procedure also allows the model to represent dynamic  complexity such as delays, feedbacks and nonlinear relationships within a set of performance indicators. This can be  used by companies to model and analyze their performance with an increased understanding about how their  processes and areas are co nnected, thus improving competitiveness by focusing initiatives in the right spot. As a  form of validation the procedure is applied to a  set of indicators related in a preexisting dynamic model to show the  application of the procedure and determine its effectiveness.    Keywords  Performance, Management, Indicators, System Dynamics.    Introduction  A big number of organizations have recurred to Performance Measurement & Management Systems (PMMS) in  order to get a quantitative perspective about how the organizations are reaching their goals that have established for  themselves. In general, the information regarding the pe rformance of an area or proces s is synthesized through one  or more numeric values known as performance indicators (PIs). A branch of th e PMMS literature deals with the  understanding of the existing interrelationships between PIs with the objective to facilitate monitoring, control and  management tasks regarding an area or process , and providing insights about how the interaction s between different  areas and processes affect each other, allowing decision makers to take more informed decisions regarding the  repercussions about how these interactions will impact the organization.   The reliability of a PMMS highly depends in the correct establ ishment of cause and effect relationships  between indicators. Assuming these relations could be mapped, the benefits would be substantial, because it  would  be the starting point of  the problem regarding how performance predictive metrics can be identified (Neely, 1999).  This would allow the reduction in the number of performance indicators being monitored by the organization, the  development of causal relationships between strategic and competitive objectives, the development of causal  relationships between processes and activities, and  could also generate the ability to quantify the interrelationships  between them within the context of a system (Rodriguez, Alfaro, Ortiz, & Verdecho, 2010).   There have been many attempts to find the ca usal relationships between performance indicators although  this is not always well defined in PMMS methodologies. Most research is focused about discussing correlation  conditions rather than causal. For example, cause and effect in a Balanced Score Card is n ot based in statistical  causality tests, but rather in logical assumptions and interdependencies that usually are only based in the personal  beliefs of PMMS users (Yusof et al., 2014). Also, in the research conducted by Bititci & Turner (2000) is mentioned  that the main barriers to an organization’s ability to adopt a more dynamic approach to performance measurement  systems can be sum marized as (1) lack of a structured framework  which allows organizations to differentiate  between i mprovement and control measures  and develop causal relationships between competitive and strategic ",deacero.com,,,46.3144754,11.0480288
4,ETHICAL CRITICAL INFRASTRUCTURE DECISION MAKING INPERPETUAL SUST AINABILITY,@ttu.edu,"Perpetual sustainability, ethical decision making, critical infrastructure  ","Planners and policy makers responsible for various economic establishments such as cities and other resource  conservation districts have an unrecognized ethical responsibility to their constituents: the pursuit of perpetual  sustainability.  Perpetual sustainability involves, as much as is practicable, infinite time horizons on the scientific,  economic, and social components of critical infrastructure decision making.  Failure to resolve long-term sustainability  threats can lead to avoidable disasters that destroy lives and fortunes.  In the early months of 2018 Cape Town, South  Africa, a city of  over four million inhabitants, narrowly e scaped shutting down the municipal water supply system  after its reservoirs fell below 6.5% useable capacity before “Day Zero” was pushed back into 2019.   On the brink of  disaster, Cape Town has taken amazing steps to reduce consumption and buy time but the effort and investment come  too late to mitigate a disaster that experts have identified as years-in-the-making.  Another 11 major cities around the  world are presently under severe water stress but, like Cape Town, local leaders have focused on engineering solutions  to the water crisis: acquiring additional supply or consuming the present supply more efficiently.  These approaches  can be successful but planners must recognize and respect the natural resource limits of their local area and recognize  sustainability-based limits -to-growth far in advance of the taps running dry.  The authors argue, with regard to  sustainability, that “in perpetuity” is the minimum acceptable measure of duration and that perpetual sustainability is  no less than an ethical mandate for natural resources and critical infrastructure decision making.  Keywords  Perpetual sustainability, ethical decision making, critical infrastructure   Introduction  Unsustainable practices, as they concern energy, the environment, or any other natural resource are untenable.  Yet,  while this observation appears to be painfully obvious, it fails to materialize in the practical decision making of water  resource managers.  In violation of common sense, municipalities frequently overlook natural resource av ailability  and reliability as a limit to growth and seldom respond effectively to known threats until crisis is imminent.   In fact,  11 major cities across the globe are presently under severe water stress (BBC News, 2018) but the City of Cape Town  (CCT), South Africa, may be the closest to “Day Zero,” when the municipal water system will have to be shut down.   In fact, on World Water Day, March 22 nd, 2017 , Capetonians were advised that, “with the [water supply] system  standing at 28.6% [capacity], there were officially just 101 days’ supply left,”  (Muller, 2017) and, as of April 23 rd,  2018, the storage has fallen further to 20.0%.  (City of Cape Town, 2018)  If the CCT does reach “Day Zero,” it will be a modern crisis which snuck up on precisely no one.  The issue  was known – or at least could have been known – but remained unmitigated until it was too late to avoid extreme  rationing as a minimum response.  Gleick,  (1996) the authors, and others would argue that the CCT’s extreme level  of rationing already amounts to a disaster; it just remains to be seen how bad it will get. (The Lancet Planetary Health, ",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
5,MULTI-LEVEL REQUIREMENT MODEL AND ITS IMPLEMENTATIONFOR MEDICAL DEVICE,@iupui.edu,"Requirements, Reusability, Knowledge Management, Medical Devices ","The rapidly improving technologies and changing market needs require a shorter time to market and more diversified  products. The reusability of requirements data becomes more and more important, especially for the medical device  industry. We consider the requirements data as an important part of companies’ knowledge body and classified them  into four levels based on their changing frequency: the regulatory level, the product line level, the product level and  the project level. The auto-injectors, which were relatively simple but had many reconfigurations, were chosen as the  application to implement our multi-level model. There are three major steps in our research approach: (1) to develop  requirements and classify them into different levels; (2) to build requirement templates and develop workflows and  guidelines for using and maintaining the templates; and (3) to test the two templates by using the developed workflow  to create requirement models for a specific auto-injector development project and automatically generate requirement  reports. The results include two auto-injector requirement templates, workflows and guidelines for using and  maintaining the templates, and two automatically generated requirement reports. The conclusions are: (1) a practical  multi-level requirements management model for a medical device—the auto-injector can be developed, and  implemented in different software tools; (2) the workflow and guidelines to support the application and maintenance  of the requirement model can be developed and implemented; (3) requirement documents/reports can be automatically  generated through the software tool by following the workflow; and (4) the new method can improve the reusability  of the requirement model.    Keywords  Requirements, Reusability, Knowledge Management, Medical Devices    Introduction  Requirements describe how the system or product should achieve. While requirement engineering refers to the process  of requirements being analyzed, documented and managed throughout the life cycle, for systems and products.  It is a  foundation stone for product development.(Wohlin, 2005)   Requirement engineering is a complex task, which demands a tremendous amount of work and an enormous  cost.(Doig, 2015)  Rapid improving and developing technologies enable companies to provide varied products to  satisfy the fast-changing customer needs. Instead of developing a new product, reconfigurations to an existing design  become more and more common. Consequently, product-related requirements often get reused, too. However, how to  effectively reuse existing requirement data remains an unanswered question. Some companies are using the “copy and  paste” approach to reuse their requirement documents. However, it is a practice which is  very time-consuming and  highly relies on the experiences of the engineers. Other companies are using an information system, either a standalone  requirement management tool or a module within a larger information platform (for example, a PLM or ERP platform),  to management requirements data. There are two different approaches to use software tools to management  requirements: manage requirement documents and manage requirements as structured items in the database. Managing  requirements as structured ite ms aligned with Model -Based Engineering, which is proved to be able to  raise the  abstraction level in requirement engineering activities .(Aluwihare, Waite, & French, 2014; Gonçalves, Rettberg,  Pereira, & Soares, 2017) . Therefore, it is critical to developing a better approach to model requirements, especially  for software tools, in order to improve the reusability.   Several requirement models are currently used in requirement engineering focusing on developing good  requirements. The goal-oriented model helps to analyze and organize the requirements logically(Arenas, Massonet,  Ponsard, & Aziz, 2015; Bui-Thanh, Willcox, Ghattas, & van Bloemen Waanders, 2007; Horkoff et al., 2016;  Mougouei & Powers, 2016); while the models for requirement engineering process, including the waterfall model, the  spiral model, and the V and W model, are trying to optimize the development process and reduce the risk of ",iupui.edu,Indiana University/Purdue University at Indianapolis,United States,40.4237,-86.9212
6,APPLICATION OF CONTINUOUS IMPROVEMENT METHODS TO ADYNAMIC ENGINEERING LABORATORY ORGANIZATION,@slb.com,,"Many organizations have engineering laboratories that are responsible for various activities, such as failure analysis,  mechanical testing, chemistry, pr oduct evaluation, etc., all of which the final product achieved will be data. This is  analogous to a traditional manufacturing organization that has the purpose of creating a tangible product. The  methodology to increase efficiency and quality of a manufacturing facility can also be applied, analogously, to an  engineering laboratory setting.   This paper delves deeper and explores the necessity to implement continuous improvement techniques and  waste elimination in a dynamic engineering environment.  Introduction  This paper will introduce the benefits that Lean Six Sigma (LSS) and Continuous Improvement (CI) methods can have  when applied to the Engineering Laboratory setting.  LSS and CI principles were developed out of manufacturing best  practices and date back to original concepts introduced by Fredrick Taylor’s Scientific Management philosophy.  When we describe an Engineering Laboratory, we are specifically describing a metallurgical laboratory.  This  type of laboratory specializes in failure analysis, product testing, manufacturing qualification and physical property  tests, such as tensile, yield and abrasion or erosion testing.  The principles in this paper could clearly be applied to  other laboratories, with specific applications being tailored for the specific work.  There are various cultural and organizational reasons that have historically prevented LSS and CI  methods  from being applied in the laboratory setting, which will be described within the paper.  We, as authors,  were faced with a significant  Engineering Management challenge at the start of 2017.   Laboratory backlogs were rapidly increasing due to a rapid improvement in the oil field market after its’ historic down  turn in 2014 -2016.  Our solution was to consider the laboratory product, knowle dge, data  and report  creation, as  equivalent to the physical products produced by a manufacturing center.  Applying LSS and CI principles to an  organization that had previously never had exposure to these ideas generated marked improvement in throughput,  despite the laboratory culture remaining focused on development of technical knowledge and report writing.   Background  In his seminal work “The Principles of Scientific Management, 1911” Fredrick Winslow Taylor laid out a new vision  for organizational efficiency.  Taylor was a key player in the conversion of manufacturing efficiency in the United  States at the start of the industrial revolution . Taylor recognized in his various roles that efficiencies could be gained  by moving away from “artisan” style work, to standardization.  He recognized that repetitive work should be broken  down into separate tasks. Those tasks could be measured and scientific thought could be applied to improve each one.   In so doing, the entire operation becomes  more efficient.  Additionally, by discovering the “one best way” of doing  the task variability between workers was eliminated.  Taylor started applying this philosophy to steel production at the end of the 1800s.  The workers he interacted  with were considered “artisan” at the time, meaning, each worker had discovered the method best for him or her, and  protected that method to defend their value to the organization.  Taylor recognized that if each worker was doing the  same activity a different way, then the organization would see variances in waste and efficiency.  The invention of time studies and standard work by Taylor has become the foundation for nearly all  management and organizational theory since.  His ideas can be seen in Henry Ford’s assembly lines and the modern  matrix organization.   His theory has been termed “reductionism” as it is the process of reducing an activity to the  basic tasks and then working on optimizing those tasks.  LSS and CI ideas and principles would likely not have been  possible without Taylor’s initial groundwork. ",slb.com,,,46.3144754,11.0480288
7,A FOUNDATION FOR SYSTEM SET-BASED DESIGN TRADE-OFFANALYTICS,@uark.edu,"Set-based design, Trade-off Analytics, Decisions Analysis, Analysis of Alternatives, Tradespace Exploration ","Systems designers make design decisions to provide customer value at an affordable cost and manageable risk. Trade- off analysis is an important tool to compare potential design decisions against competing objectives. Designers  evaluate trade-offs for systems, subsystems and components at various levels, such as design parameter versus design  parameter, performance parameter versus performance parameter, or system value versus system cost. In point-based  design (PBD), system designers and systems a nalysts use models and simulations to assess the performance of the  potential system, subsystem, and component concepts to support trade -off analysis. Due to system complex ity and  multiple stakeholders, a PBD best practice for system  trade-off analysis is to use an aggregated value function based  on decision theory, which provides a sound mathematical foundation. Set -based design (SBD) is similar to  conventional PBD approaches, with the exception of explicit consideration of sets of design choices. SBD iden tifies  and evaluates a larger number of points through sets. Each set may include hundreds or thousands of points. Therefore,  SBD has the potential to identify points in sets on the Pareto frontier that may be better than the original PBD points.  This paper will provide insights into SBD quantification literature, describe best practices for SBD trade-off analytics,  and propose new techniques for quantifying SBD trade-off analytics.    Keywords  Set-based design, Trade-off Analytics, Decisions Analysis, Analysis of Alternatives, Tradespace Exploration    Introduction  In the past several years, developing engineered resilient systems has been in the forefront of research for complex  systems due to evolving adversities (Small et al., 2018) . The Department of Defense (DoD) created the Engineering  Resilient Systems (ERS) program to expand resilience research and to help create resilient systems. The DoD  anticipates the ERS program will help improve its analysis of alternatives (AoA) process by incorporating resilience,  as seen in Exhibit 1 (Holland, 2015). This process will help the DoD improve its return on investment by 1) using  tradespace tools and analytics, such as high performance computing, for design space exploration and alternative  comparison, 2) developing and integrat ing analysis capability and workflow, and 3) leveraging the Computational  Research and Engineerin g Acquisition Tools and Environments (CREATE) to perform prototyping, design  verification, and operational testing in a virtual environment (Holland, 2015). ",uark.edu,University of Arkansas - Fayetteville,United States,36.0970389,-94.17033216657404
8,,@gmail.com,"Continuous improvement, front line workers, innovation  ","In the ever increasingly competitive landscape in every industry, companies can no longer afford to underutilize the  innovation potential in contributions from their front -line employees. The unique position of front line workers as a  resource in improveme nt projects allows for more explicit consideration and knowledge of the current state of the  system and an ideal state that needs to be developed. Continuing work from a previous publication, this literature  review is a case study compilation of companies and studies who involved front line workers in improvement efforts.  The comparison between methods and results will provide insight as to what practices are effective for integrating  front line workers into the improvement process. As a compilation the com parison also takes  into consideration  industry, location, and size. Prominent methods of integrating front line workers into continuous improvement efforts  include company culture or management through employee design, job autonomy, and open dialog or a pr ogram  through continued employee education, adapted programs, or company/employee designed programs. Those  companies who successfully utilize front line workers as subject matter experts in improvement efforts aid in  developing a competitive edge by acting as a guiding resource for further integration of front line workers in projects.   Keywords  Continuous improvement, front line workers, innovation   Introduction  A company must continuously improve in order to stay competitive within the ir relative industry. However,  improvement and innovation in current operations  can be costly  (Reeves & Deimler, 2011) . Front line workers are  those employees who perform the work and operate the processes that is closest to providing a product or service to a  customer. Under this definition, f ront line workers who are involved in everyday work of a system serve as subject  matter expert to these processes due to experience and familiarity with the work system . This is a  resource already  possessed by every company and can be utilized as a source for opportunities for improvement, resources for solution  generation, or evaluators for potential solution alternatives. Utilizing front line workers in improvement efforts can be  achieved with many methods and in varying levels of capacity.     However, there is little to no literature on the theory behind utilizing  front line workers as subject matter  experts in the continuous improvement of their respective work system ( Kelly & Sc hell, 2017 ). There are proven  benefits to involving front line workers in designing aspects of their jobs including but not limited to  the sense of  ownership, higher employee well -being and morale, higher performance, and lower counter -productive work  behaviors (Kelly & Schell, 2017 ). The large variation of methods in employee involvement for continuous  improvement only attests to the lack of understanding in best practices for eliciting and utilizing employee  contributions. There are decades of publication in management, human resources, and communication as resources   for any company to either adopt a practice or develop and customize their own (Fairbank, Spangler, & Williams, 2003;  Fell-Carlson, 2004; Barton & Srivastva, 200 2; O’Brien, 1995; Carrier, 1998; Bassford, 1996; Akin, Broo, Byers, &  Llyod, 2016). This abundance of options in previous publication produces two  issues. The first being there is little to  no follow up literature that evaluates effectiveness post-implementation from companies who adopt proposed methods  and practices for involvement of front line workers in improvement efforts (Kelly & Schell, 2018). Therefore, the lack  of further documentation means there is no method of discerning which methods do or do not work and why. However,  companies have been published case studies . The second issue being the great span of time  between the earliest  published employee involvement methods and present day. Those methods published in the 1980s and then used as a  basis for newer contributions methods do not account for  societal or generational changes  (Wagner, 2007; Bresman  & Rao, 2017). What may have worked in the 1980s may not be as effective three decades later. While this is a struggle  for any literature in management, the prevailing recent publication in subject matter such as suggestion systems truly ",gmail.com,,,46.3144754,11.0480288
9,PMO IMPLEMENTATION FOR PROJECT MANAGEMENT IN ACOLLABORATIVE RESEARCH CONTEXT,"@lsbu.ac.uk,","Project Management Office, PMO, Collaborative Research Projects. ","The management of collaborative research projects can present certain challenges, such as the development of  multidisciplinary teams, ensuring alignment of the collaborative partners and generating the required level of impact  from the project. The project management office (PMO) is an organizational unit designed to standardize how projects  are delivered and achieve efficiencies through deploying best practice gained from the delivery of successive projects.  Therefore, there is much scope for the PMO to support the management of collaborative research projects. This paper  will provide an overview of how a PMO has been established at  a higher education institution in order to support the  development and management of collaborative research projects. The case study investigation includes  a number of  managerial insights on how the PMO structure and pro cesses were implemented in a collaborative research context.   These insights have been synthesized into a set of critical success factors for PMO implementation that will inform  future research in this area and will also be of use to practitioners looking to implement a new PMO to support  collaborative research projects.    Keywords  Project Management Office, PMO, Collaborative Research Projects.    Introduction  Research collaboration has increasingly become the norm for the pursuit of scientific and technological developments  that require adoption of a multidisciplinary approach along with contributions from multiple partners ( Lee and  Bozeman, 2005).  Multidisciplinary research collaborations have been described as being key to finding solutions to  global challenges, such as the development of environmentally friendly technologies, enabling sustainable food  production and the development of new medicines (Knapp et al., 2015). Indeed, many governmental funding agencies  around the world actively commission collaborative research projects (Kardes, 2014) and promote knowledge transfer  activities (Kochenkova et al., 2016) according to their funding requirements.   Collaborative research projects offer a number of benefits (Philbin, 2017), such as enabling the exchange of  knowledge between collaborators, increasing the scope and scale of data generated by research studies, and allowing  access to complementary research infr astructure (such as experimental equipment and numerical models) . However,  there are also certain challenges associated with research collaborations  (Cummings and  Kiesler, 2005), and these  include the coordination costs, development of multidisciplinary teams, ensuring alignment of the collaborative  partners, and generating the required level of impact from the project. Moreover, if collaborative research and  technology projects are to generate potential solutions to address societal needs , across areas su ch as improved  healthcare solutions, mitigating the effects of climate change and new forms of transportation, it is important that such  projects can be managed to a high degree of success. This level of performance can be viewed in terms of achievement  of the project milestones in the required timeframe and according to the quality, budget and schedule requirements,  but this performance is also predicated on the research including the necessary creativity and scientific freedom to  facilitate developments in the particular scientific or engineering discipline.  A potential mechanism to support the delivery of collaborative research projects is the project management  office or PMO ( Hobbs et al., 2008) . The project management office (PMO) is an organizational u nit designed to  standardize how projects are managed and achieve improved efficiencies through deploying best practice gained from  the delivery of successive projects  (Philbin, 2016). The PMO provides management and administrative support to  enable the delivery of projects and the PMO approach has been successfully implemented across a number of sectors,  including construction ( Qi et al., 2014) , information systems ( Ward and Daniel, 2013)  and research administration  (Wedekind and Philbin, 2018) . Adopting a PMO structure has been associated with improved project performance  and key drivers that support these improvements include the development of project standards and methods that detail ","lsbu.ac.uk,",London South Bank University,UK,51.4979,0.102
10,MAYBE THINGS REALLY ARE DIFFERENT THIS TIME,@donkennedy.ca,"EMBoK, Engineering Management, Automation, HI-MI, Change ","Peter Drucker maintained there is a tendency for people to believe their situation is unique and a new world is  dawning.  He cautioned against this view  and to look back to similar situations for guidance to analyze the present.  Proven management tools and the underlying basic principles will continue to hold true.  Drucker’s stance was  demonstrated when the claim of a new economy in  1929 fell flat and again when the dotcom bubble burst a fter  2000. Both events saw a return to the realities of sound managerial principles.  However, in 2018 there seems to be a  case for things really bei ng different  this time .  A review of cornerstones of engineering management practice  reveals that their relevance may be decreasing due to fundamental changes in the organizational world.  The current  engineering management body of knowledge might be left behind in a rapidly changing business environment.   Evidence t hrough literature reviews, case studies and tours of modern businesses support s the view that the old  standards are losing their importance.  Subjects of much EM research highlighted in this pap er are productivity,  motivation, team work, continuous improvement, learning curves and organizational knowledge.  Also , is there a  potential for bitcoin or other digital currencies to disrupt the bases of engineering economy?  Will even the world of  engineering management education be disrupted by online courses reducing the need for teaching faculty? The  engineering management world may soon find itself scrambling to regain lost relevance and credibility.     Keywords  EMBoK, Engineering Management, Automation, HI-MI, Change    Introduction  Our introduction to the engineering management body of knowledge (EMBoK) included attending the 1999 ASEM  annual conference.  During presentations, we were introduced to the idea that the behavior of people does not  change much over time and there are certain time -proven principles that continue to hold (Thompson, 1999).  Under  this premise, a new twist might be added to an old idea to create excitement for what becomes a new fad, but the  basic underlying concept is unchanged.   If the stance for a consistent nature of management principles is valid, the  challenge to engineering managers is to educate largely unreceptive organizations to these time proven techniques  and get buy -in for their implementation (Rossler & Kiser, 2002) .  Believing in the consistent nature of humans  allows the engineering manager to trust these principles that have been tested for many decades.    The highly influential Peter Drucker was a firm believer in the concept of looking back for guidance on  how to interpret current situations.  When starting out in his career, Drucker was assigned the task of proofreading a  book his boss had written that ex plained why the world had changed and how a red -hot economy was assured to  continue on its upward path forever (Drucker, 1999).  The year was 1929 and the release of the printed book was  coincidentally timed with the beginning of the ensuing Great Depression.  This drastic lesson engrained into Drucker  the idea that the consistent principles of human nature that shape history continue to influence the present.  Drucker  correctly predicted the decline in the demand / value of computer hardware technicians ex perienced in the latter half ",donkennedy.ca,,,46.3144754,11.0480288
11,"UNDERSTANDING THE PROFESSIONAL, NON-ACADEMICEXPERIENCE OF ENGINEERING FACULTY",@montana.edu,"engineering faculty, professional skills, engineering education ","The literature in both Engineering Management  (EM) and the nascent field of Engineering Leadership (EL)  periodically discusses the relative importance of industry experience for faculty engaged in these areas. Recent work  in both fields found that EM / EL faculty appear to have considerable industry experience. However, this finding runs  counter to the commonly accepted knowledge summarized by former leaders of the National Academy of Engineering  Wulf and Fischer as “Present engineering faculty tend to be very capable researchers, but too many are unf amiliar  with the worldly issues of ‘design under constraint’ simply because they’ve never actually practiced engineering.”  While this knowledge is commonly accepted, prior reviews of the literature were unable to identify any studies that  quantified this lack of practice experience among engineering faculty. A finding further supported by discussions with  engineering education programs officers at the NAE.    This work seeks to close that gap through examination of the backgrounds of a representative national  sample of  engineering faculty. The collection looked at faculty from fifteen different universities representing each of the three  Carnegie research classifications. For each universit y, faculty backgrounds from three different programs were  collected and examined using faculty’s publicly available curriculum vita and professional social media identity (i.e.  LinkedIn). Through this analysis a composite profile is created that shows the differences in practice backgrounds of  faculty, considering type of app ointment (level, tenurable vs. non -tenurable) and engineering field. Significant  differences are found in many categories, supporting some aspects of the commonly held beliefs.     Keywords  engineering faculty, professional skills, engineering education    Introduction and Motivation  The purpose of this study is to better understand the level of non-academic professional and leadership that engineering  faculty possess . The motivation to understand these backgrounds is two pronged. First, as pressures increase on  academia to produce graduates who are ready for the workforce (Inglis, 2016; Selingo, 2015), some are revisiting the  call started by Boyer (1990) nearly 30 years ago regarding how scholarship is practiced at universities. If both the role  of the university and the types of scholarship considered worthy within higher education are changing, the preparation  of faculty may also need to change. Second, there is growing recognition of the need for engineers with well-developed  professional skills  (ABET, 2012; National Research Council, 2004; Passow, 2012) . If faculty are to support the  development of these professional skills, perhaps they need to have experienced the m in a professional setting.    The development of this study grew from prior work to understand the professional and academic background of  engineering faculty engaged in the field of engineering leadership and how the broader engineering faculty view  leadership and define leadership in an engineering context (Schell & Kauffmann, 2016). That work found that faculty  who are engaged in engineering leadership research have an unusually high level of non -academic professional  experience when compared with what is understood to be the typical engineering faculty member, with 98% of study ",montana.edu,Montana State University - Bozeman,United States,45.6638859,-111.07928704602077
12,,@iser.msstate.edu,"Manufacturability, Risk, Assessment, Technology, Readiness, Lifecycle, Taxonomy  ","The transition of products from conceptual design, through development, and ultimately to production and use is  fraught with potential risk. The cumulative magnitude of associated risk depends largely on the maturity of the  technical aspects of the product’s design and the extent to which the required manufacturing processes are sufficiently  ready for production. Products that incorporate proven technology with a demonstrated history of successful transition  to production and sustainment are inherently less  risky than products that integrate newly developed technology in a  design which, in turn, may require development of yet unproven manufacturing processes. The United States  Department of Defense (among other organizations) has established defined Technolo gy Readiness Levels as well as  defined Manufacturing Readiness Levels. The criteria contained therein effectively provides a baseline for the  comprehensive information and activities necessary for transitioning a design through each of the successive stage s  of the product lifecycle.  This research paper proposes a taxonomy based assessment of the ascertained risk associated with the  baseline criteria at the early stages of a product’s lifecycle that, unless mitigated, accumulates throughout the  subsequent stages of the lifecycle. This phenomenon can result in cost overruns, functional degradation, and even  failed product launches. The proffered methodology is intended to provide for an “early warning” system so that  discrepancies in the criteria can be readily addressed at even the earliest stages, subsequently reducing or eliminating  the resulting negative cost impacts of unmitigated risk.    Keywords  Manufacturability, Risk, Assessment, Technology, Readiness, Lifecycle, Taxonomy     Introduction  The ability for a product design, from simple to complex, to be manufactured in accordance with design specifications  is always fundamental. It is also necessary to be able to produce a product at a cost that is within economic bounds  commensurate with the need for the pro duct. A well established fact is that, ultimately, much of the eventual  cumulative costs associated with the production, use, sustainment, and disposal of a product are determined by  decisions made early during the design stage. This phenomenon will be fur ther discussed later in this research paper.  However, most attempts to assess, using a structured method, the manufacturability of a product require a certain  design fidelity be defined before the assessment is performed. However, once the fidelity of a de sign becomes  concrete, the manufacturing costs associated with the design follow. At this point in the design process, the cost  associated with changes to the design and manufacturing criteria increase significantly. There is a need for an  assessment methodology that can be used early in the conceptual design phase so that manufacturability issues are  considered sooner rather than later in order to reduce the total lifecycle costs associated with a product. The purpose  of this research is to identify such an assessment from both previous research work in this field as well as new research  where it is applicable.    Literature Review  In their book on engineering design  (Pahl, Beitz, Feldhusen & Grote, 2007) provide a comprehensive guide on a  systematic approach to engineering design. This frequently referenced text provides a thorough description of  fundamental engineering activity , in detail, throughout the product  lifecycle. The text includes  the following  definitions for phases in the engineering design process that are significant for this research:  ",iser.msstate.edu,Mississippi State University,United States,37.9537,-91.7756
13,PREDICTIVE POWER OF GARCH MODEL AGAINST VMA AND FMATRADING RULES: THE CASE OF DJIA AND DAX,@wne.edu,"Stock Market, Technical Analysis, Trading Rules, Time Series Prediction, Statistical Significance, GARCH ","We use variable and fixed length moving averages to generate buy and sell signals for D JIA and DAX over the 2005  to 2018 period. We determine the properties of the best null model that fits the stock return time-series. The results of  our study challenges the Efficient Market Hypothesis, and  posits that the technical trading rules help to pr edict stock  returns. The returns cannot be explained by GARCH model. Our observations lead to the view that the German stock  market is not as efficient as the U.S. stock market.    Keywords  Stock Market, Technical Analysis, Trading Rules, Time Series Prediction, Statistical Significance, GARCH    Introduction  Stock markets are affected by many factors and characterized by uncertainties which make it difficult to predict the  market behaviors (Joe, Oh, & Park, 2018). Efficient market hypothesis states that stock prices are not predictable since  the current stock price reflects all publicly available information; hence it is impossible for the investors to outperform  the market (Gozbasi, Kucukkaplan, & Nazlioglu, 2014). However, there has been a long debate to reject the efficiency  of the stock market (Chen, Zhou, & Dai, 2015; Patel, Shah, Thakkar, & Kotecha, 2015; Qiu, Song, & Akagi, 2016).  Those who are against the efficient market hypothesis argue that there are some forms of market inefficiency which  cause the market movements to be predictable based on technical analysis of historical prices an d volumes  (Scholz, Nielsen, & Sperlich, 2015).  Traders need to determine the entry and e xit points in order to make profit on the capital invested (Chong,  Ng, & Liew, 2014; Almasifard & Khorasani, 2017). Hence, technical analysis may be used statistical tools to help the  traders determine the right time to enter and exit the market (Han, Yang, & Zhou, 2013). Technicians believe that the  future trends can be predicted by studying historical patterns. They use pattern recognition techniques and attempt to  uncover the patterns in the prior data (Metghalchi, Chang, & Garza -Gomez, 2012). Technical trading rules can then  be used in to initiate the buy and sell signals.These  technical trad ing rules can be evaluated by examining the  economical and statistical significance of the profitability of the rules (Fang, Jacobsen, & Qin, 2014).  Quantitative finance, on the other hand, uses mathematical tools to capture the changes in stock prices. The  stochastic properties of stock prices can be determined by developing an appropriate time-varying equilibrium model  (Dash, Dash, & Bisoi, 2015). GARCH model has been extensively used to predict future stock returns (Engle &  Sokalska, 2012). Once the parameters and residuals of the GARCH process have been  estimated, the time series of  stock returns can be simulated from the model.  In this study, we test the predictability of trading rules as well as the accuracy of time -varying equilibrium  models aimed at explaining stock returns. Even though stock returns are the most studied financial series, these series  suffer data-snooping biases (Brock, Lakonishok, & LeBaron, 1992). In order to eliminate these biases, we examine a  selection of trading rules over a long data series as well as sub -periods, and present the robustness of our res ults for  statistical inference.  The remainder of this paper is o rganized as follows; Section 2 describes the data; Section 3 introduces the  technical trading rules and discusses the test results of the moving averages; Section 4 presents the GARCH model  and discusses the test results of trading rules baesd on the simul ated time series; Section 5 presents a comparision  between the results from trading rules on DJIA and DAX; and finnally Section 6 provides the conclusion of the work. ",wne.edu,Western New England University,United States,42.1125825,-72.51478135708936
14,COLLECTIVE INDIVIDUALISM: AN INCLUSIVE FRAMEWORK FORCOMMUNICATION SYSTEMS,Missing,"Inclusivity systems framework, essential human services, ICT4D capability maturity, communication equity ","Information and communication technology for development (ICT4D) continues to enable provision of essential  human services through digitization of human  interactions and service delivery. Domain -specific design  methodologies such as Rapid Application Development (Software Development) or Service Learning (Education) as  well as conventional systems and economic frameworks have informed the design and creat ion of ICT4D systems.  However, these frameworks do not adequately capture the perspectives of and barriers faced by individual  beneficiaries, and soften the multi -faceted nature of economic and technological conditions. Thus, the modern  challenge lies with  creating collaborative, sustainable, and evolving change that is not just designed for the  beneficiaries, but by them as well. The author s propose an Inclusivity Systems Framework (ISF) for ICT4D systems  to describe the multi-dimensional, multi-stakeholder nature of systems at the Base of the Pyramid. The framework is  built on the theory that trust drives success in community -based groups, and inclusive communication in turn drives  trust. The framework for inclusivity includes economic inclusivity, technol ogical inclusivity, and participatory  inclusivity, all of which in turn have underlying factors and attributes. The author submits that the resulting framework  can be used to capture the current capability maturity of ICT4D in a certain region or organizat ion, and inform the  design of new platforms that may provide capability uplifts. An interdisciplinary, service learning research partnership  between University of the Free State, University of Virginia, and HiComm LLC demonstrates the framework in action  in South Africa.    Keywords  Inclusivity systems framework, essential human services, ICT4D capability maturity, communication equity    Introduction  The United Nations’ Millennium Development Goals reached their 2015 deadline, and there is much to celebrate by   the numbers. Several worldwide goals have been reached, including halving by 2015 the percentage of people that are  earning less than $1 per day and the percentage of people without access to improved water supply (UN 2015, UN  2017). Given that these deve lopment efforts constitute multiple stakeholders with multiple objectives and limited  resources, systems thinking serves as a framework to understand and assess these compl ex systems (Pailla and Louis  2011). Particularly, systems integration becomes a nece ssary activity in aligning, balancing, and coordinating the  activities of these stakeholders so that an end goal of a healthier, more resilient society may be achieved (Bouabid and  Louis 2015, Barrett and Constas 2014). This paper introduces a framework to  assess the current capability maturity  of communities with respect to information and communication technologies for development. The framework was  used to assess ICT4D capa bility maturity in South Africa . Preliminary results from the implementation indic ate  viability for expansion and testing for capability uplift over time.    Growing Importance of ICT4D  In contrast to the developed world that has been thriving with penetration and expansion of modern internet,  developing countries, particularly rural areas, still face barriers. The penetration has not been equal. As of 2017, while  Europe and North America (excluding Mexico) have had mobile penetration of over 84% and mobile internet  penetration over 70%, Asia -Pacific, Middle East and Northern Africa, and S ub-Saharan Africa all have mobile  penetration levels of 45-70% and mobile internet penetration of 20% - 40% (GSMA 2018). Other areas of the world  fall somewhere in the middle, with Latin America leaning lower. Unlike Western Nations, which include the US a nd  EU, much of the developing world is still far behind in basic telephony and networking infrastructure, much less  advanced cables or satellites to connect to servers for internet access (GSMA 2018). This has had tremendous impact  economically, but to und erstand why that may be the case, it is necessary to review the role that information has in  economic development. ",Missing,,,46.3144754,11.0480288
15,RESEARCH AGENDA IN DEVELOPING CORE REFERENCEONTOLOGY FOR HUMAN-INTELLIGENCE/MACHINE-INTELLIGENCE ELECTRONIC MEDICAL RECORDS SYSTEM,@odu.edu,"Electronic Medical Records, HealthCare, Human Intelligence, Machine Intelligence  ","Beginning around 1990, efforts were initiated in the medical profession by the U.S. gov ernment to transition from  paper based medical records to electronic medical records (EMR).  By the late 1990s, EMR implementation had  already encountered multiple barriers and failures.  Then President Bush set forth the goal of implementing electronic  health records (EHRs), nationwide within ten years.  Again, progress toward EMR implementation was not realized.   President Obama put new emphasis on promoting EMR and health care technology.  The renewed emphasis did not  overcome many of the original problems and induced new failures.  Retrospective analyses suggest that failures were  induced because programmers did not consider the medical socio -technical communications structures that had  evolved around paper records.  Transition to electronic records caus ed breakdowns in the medical socio -technical  communications systems; induced inconsistencies in information exchanges among clinics, physicians, hospitals,  laboratories, pharmacies, and health insurance providers; and resulted in the incorrect administration of prescriptions,  errors in patient care, and unnecessary treatments and surgeries.  With the rapid integration of machine intelligence  (MI) in medical socio-technical systems, there is a potential to repeat the failures of EMR implementation.  To address  the MI integration issue, this paper reports research  design into the development of a human -intelligent/machine- intelligent (HI-MI) EMR core reference ontology around which EMR-MI knowledge can be encoded to form the basis  for informed transition to artificially intelligent electronic medical records.    Keywords  Electronic Medical Records, HealthCare, Human Intelligence, Machine Intelligence     Introduction  The use of electronic medical records (EMRs) has increased significantly over the past few years. In the health industry  and in medical research, EMRs have proven to be a critical source of information for pharmaceutical, diagnostic  research, insurance purposes, coding enhancement for billing, and understanding of an overall patient encounter and  history. Studies to date have revealed that the text content in EMRs may contain important additional information  relevant to outcomes, concomitant diseases, procedures, interventions or test results in observational studies (Call, B.,  2013). Public demand for flexible access to health information and services is growing as well, encouraged by internet  trends and policies promoting patient rights and empowerment (Cayton H., 2004).   Beginning around 1990, efforts were initiated in the medical profession to transition from paper based  medical records to electronic medical records (EMR).  In 1991, the Institute of Medicine (IOM) issued a report which  also called for pap erless health records within ten years (Institute of Medicine, 1991). However, the more medical  information becomes available in electronic form, the need for more complete security implementations becomes  necessary. The National Research Council (NRC) of the National Academy of Sciences was charged in 1995, with  evaluating the practical measures that can be used to reduce the risk of improper disclosure of confidential health  information while providing justified access to those interested in improving the quality and reducing the cost of care  (National Research Council. 1997). In 1996 the Health Insurance Portability and Accountability Act (HIPAA) was  introduced in response to growing issues facing healthcare coverage, privacy and security in the United St ates. In the  1997 State of the Union address, then President Clinton noted that “we should connect every hospital to the Internet,  so that doctors can instantly share data about their patients with the best specialists in the field.” (Clinton, William.  1997). However, the security and confidentiality of the clinical data are a major impediment to realizing this noble ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
16,A SYSTEMS FRAMEWORK TO CHARACTERIZE HOW TO IMPROVEOFFENSIVE CYBER OPERATIONS FOR CYBER WARFARE,@gmail.com,"Cyber warfare, offensive cyber operations (OCO), cyberattacks ","The future of warfare will likely not be won by those with merely the greatest military might. Rather, the world powers  of the 21st Century will be those  states that have  the most robust cyber defensive capabilities coupled with an  experimenting culture that allows them to develop the most effective offensive cyber weapons. Arguably the greatest  challenge any sta te faces when coming up with an  arsenal of powerful  cyberattack capabilities is properly defining  what its offensive cyber operations (OCO) will encompass. This research tackles the complexities of this problem by  examining the attributes most necessary for effective OCO. Through the review of literature o n cyber operations and  on real-world cyberattacks that have already taken place, this paper defines five factors which OCO must consider in  order to be truly effective: degree of damage inflicted, target specificity, undetectability, surprise, and degree that the  attack can be prevented. This research employs systems engineering tools and techniques to prioritize how the US can  focus on OCO for the 21st Century and beyond so that it is prepared to fight and win in its future wars.     Keywords  Cyber warfare, offensive cyber operations (OCO), cyberattacks    Introduction  With the turn of this  century and the rapid spread of technology in our globally connected world , victory in future  wars now seems to lie  in the hands of those who can  dominate the cyber domain. Cyberattacks that had once been  considered pranks and nuis ances have matured into strategic  cyber weapons that can inflict considerable damage to  critical national infrastructure. No longer are cyber threats merely storylines created by the imagination s of science  fiction and Hollywood screen writers. In just this past decade, the discovery of widespread OCO in computer networks  have piqued our sensibilities  into just how powerfully devastating  cyber weapons have demonstrated : the Stuxnet  worm which physically crippled Iran’s Natanz nuclear uranium enrichment facility (Zetter, 2014); the Flame malware  that enabled audio recording, screenshots, and keystroke logging of  infected computers which led to pervasive cyber  espionage in a number of Middle Eastern countries (Zetter, 2012); and the BlackEnergy Trojan horse which resulted  in synchronized distributed denial of service (DDoS) attacks in Ukraine that served as a prelude to military combat in  the country  (Vincent, 2018) . Although the efficienci es gained from today’s globally connected networks confer  numerous benefits and transform lives in positive ways, the increasing reliance on such technologies make all of those  who use these networks increasingly vulnerable to cyberattacks.    From An Initial Research Task to a Revised Problem Statement  This research  project originally started with the task of developing concepts to attack an adversary’s legitimacy  through cyber means. However, after the  team’s preliminary background research on  cyber capabilities and cyber  warfare, the researchers determined that the scope of the initial problem statement was too limiting and  that it would  be much more efficacious to examine what is the best way a natio n can leverage cyberattack capabilities in order to  achieve its desired goals. Accordingly, this research focuses on the following revised problem statement: what makes  for an  effective offensive cyber operation (OCO)  so that it maximizes its intended results while simultaneously ",gmail.com,,,46.3144754,11.0480288
17,GIRLS TO ENGINEERS NETWORK (GEN),@mocs.utc.edu,"Engineering Education, Project-Based Learning (PBL), Soft Skills, Community Engagement, Middle -School Girls, ","Engineering Management  & Technology , Engineering, and Computer Science Departme nts at the University of   Tennessee at Chattanooga (UTC), in collaboration with the Chattanooga Girls Leadership  Academy (CGLA), has  designed UTC Girls to Engineers Network (GEN), a project -based learning (PBL) to encourage middle school girls  in engineering. The goal of UTC GEN is to increase the number of young women interested in and confident in their  ability to pursue postsecondary education and engineering careers by exposing them to hands-on engineering content  in a real - world context. In this program, UTC guided 67 CGLA eighth -grade girls in designing, developing,   showcasing, and distributing an operational prototype to address a need faced by a community partner.  The program integrated the following evidence-based strategies:  1. Core Engineeri ng Content and Soft Skills through PBL: Participants learned effective  brainstorming; designing,  drafting, and sketching; structural design; load-bearing and weight distribution principles; modular design; and ethical  design practices through hands-on, PBL modules.  2. Real-World Experience that Impacts the Community: By working to develop prototypes  to meet the needs of a  community partner in real time, participants gained a greater appreciation for the real-world application of engineering  and the satisfaction of making a positive impact in their community.  3. Mentorship by Faculty and Students: Faculty and students supported curriculum  development and program  implementation, and provided participant mentoring and  encouragement to pur sue postsecondary educa tion and  engineering careers. Female engineering management and engineering students provided additional face-to-face time  with participants.  UTC GEN combined the research -backed benefits of PBL with the real -world impact of community-based  projects.    Keywords  Engineering Education, Project-Based Learning (PBL), Soft Skills, Community Engagement, Middle -School Girls,  Science, Technology, Engineering, and Mathematics (STEM).      Introduction  According to the U.S. Bureau of Labor Statistics, Science, Technology, Engineering, and Mathematics (STEM) career  opportunities are projected to reach more than 9 million jobs by 2022 (Vilorio, 2012).  Despite the plethora of STEM  opportunities, however, severe employment gaps remain. According to a recent Nationa l Science Foundation (NSF)  publication, women comprise half of the total U.S. college -educated workforce, but currently represent only 30% of  the STEM workforce (National Science Foundation, 2017). In engineering, the percentage of women earning degrees  has stagnated, lingering at 20% since 2004, and fewer go on to remain in engineering careers, with less than 15% of  employed engineers estimated to be women. The gap is even more pronounced for African Americans and Hispanics,  who represent less than 5% and 8% of employed engineers, respectively.   This gap begins in childhood, with girls’ enjoyment of and interest in STEM subjects dropping significantly  by the time they enter middle school while boys’ interest increases over time  (Girl Scout Research Institut e, 2012).   Lack of STEM identity, lingering stereotypes about academic performance, and limited exposure to STEM role models  have been cited as major barriers to STEM persistence for girls and minorities  (Krishnamurthi, A., et. al., 2014 ).   Middle schools provide a vital link between the foundations formed in elementary school and the academic and social  readiness required to persist through high school. As such, research shows that middle school is a critical point for  solidifying STEM interest and ambitio n to pursue college degrees that will enable them to pursue careers in STEM  fields (Murphy, T., 2011).  In fact, studies have shown a positive correlation between the STEM career aspirations of  eighth grade students and earning a STEM degree. Even students with only average mathematics skills who expressed ",mocs.utc.edu,The University of Tennessee at Chattanooga,United States,35.0459,-85.2953
18,THE RELATIONSHIP BETWEEN A TECHNOLOGY’S DIFFUSIONRATE (TIME) AND ITS ECONOMICAL IMPACT (MONEY),@swri.org,"Technology, S-Curve, Technology Diffusion  "," Understanding the economical impact of a technology’s diffusion rate can assist decision makers in making  informed decisions on how resources (labor, equipment/materials, time, and capital) are managed.  The goal of this  research is to  demonstrate a link between a technolog y’s diffusion rate and its value (economic impact) to warrant  further examination of its impact on abandonment.  An overview of the technology diffusion curve is presented, and  three diffusion rates (categorized as fast, medium, and slow) are modeled to demonstrate the impact on a technology’s  value along a technology’s diffusion curve.   Results show a link between diffusion rate and market value for a  technology’s diffusion curves.      Keywords  Technology, S-Curve, Technology Diffusion     Introduction  There exists a robust body of research on technology adoption in engineering management, but the arena of technology  abandonment is far less developed (Greenwood, Agarwal, Agarwal, & Gopal, 2016).  Technology abandonment is the  dissolution of a technology by an organization.    Prematurely abandoning of a technology could result in loss (e.g.,  loss revenue), conversely; delaying abandonment could also result in losses (e.g., miss opportunity).   To be able to  manage technologies within an organization, the quantification of the economic effects of abandonment wa rrant  examination.    The technology life cycle s-curve was introduced by Everett Roger’s Diffusion of Innovation Theory in 1962  (Rogers, 2010).  Technology forecasting traditionally has been focused on determining when a technology enters the  emerging stage of the technology life cycle and not the declining/obsolescence stage.  By understanding a technology’s  complete life cycle (i.e., life cycle stages: development/introduction, growth, maturity, and decline), decision-makers  are in a better position to make choices as to where to assign or remove resources.  Advances in technology are a key  to long-term economic growth (Comin & Mestieri, 2014; Solow, 1956) .  Thus, the endeavor to study technology  diffusion has economical significance.     Technology diffusion is characterized 3as the accumulation of adopters over time, resulting from individual  acceptance or rejection decisions (Comin & Mestieri, 2014) .  The cost of a technology , in the framework of an  organization, is often narrowly focused on the cost of adoption, that is; the cost (labor, equipment, material, and  overhead) of bringing the technology into the organization.  A decision-maker may even examine what the opportunity  cost of one technology is over another, in reference to, whether to invest or not.  In terms of the technology life cycle,  the opportunity cost examination is traditionally used to give economic ins ights of a technology at the start of its life  cycle.  There currently exists a gap of knowledge on the economics (impact or measure) of technology abandonment,  leaving decision-makers vulnerable to poor decisions.  To begin the process of closing the knowledge gap, this paper  investigates the link between a technology’s diffusion rate (time) and its economic value (money) in an effort to show  a basic link for the outline of future research on technology abandonment .  Thus, the focal emphasis of this resear ch  is to evaluate if there is a link between a technology’s diffusion rate and the technology’s economic value.         ",swri.org,,,46.3144754,11.0480288
19,M,Missing,,,Missing,,,46.3144754,11.0480288
20,REVOLUTIONIZING ADDITIVE MANUFACTURING IN SPACELOGISTICS - A PATH TOWARDS SELF-RELIANT NATURE,"@buffalo.edu,",station). ,"The ability to manufacture parts in space rather than sending it from earth would give us a shift in the paradigm of  current space logistics and supply chain. The contemporary inventions, scenario , and situations are making space  exploration a necessity. The supply chain and logistics field have a vital importance in the field of space exploration  and the approach of additive manufacturing will make it much more efficient. The purpose of this research is to  evaluate the potential impact of additive manufacturing in the deep space missions, to develop a comprehensive supply  chain framework in the creation of spare parts in space when there is a critical demand. This could further reduce the  payload during the launch. In the future there will be human  transportation taking place in the many deep missions  like Mars One and the vision to colonize it can be made feasible. This gives a new perspective on supply chain and  logistics and is the first step towards long missions in space making them self -reliant. The paper discusses the  importance of additive manufacturing which would reduce the lead time, transportation costs of a product, also the  challenges that include material handling in space logistics. Furthermore, we will discuss the future scope of add itive  manufacturing in space logistics, supply chain networking, healthcare etc., which could take over the traditional  methods of reducing the lead time from months to weeks.    Keywords: Space logistics, A dditive Manufacturing, Spare parts, Deep space miss ions, ISS (International space  station).    Introduction  According to the most commonly accepted theory, the humans evolved from apes and the link got separated 3 million  years ago. From that time the process of exploration, migration, and colonization began. Early humans started to  migrate from Africa to Eurasia before 1.5 million years ago and the human race started spreading. Exploration has  been a part of the human race even before understanding the possibility of space exploration. The  first successful  mission that comes to our mind when we talk about manned space exploration is Yuri Gagarin’s space exploration on  12th April 1961 as a Soviet pilot. But the exploration of space started way back in the 16 th century by observing the  night of sky, discovering Jupiter’s moon, lunar craters and phases of Venus by Galileo Galilei. As of 2013, there have  been 536 astronauts from 38 countries have participated in the space exploration programs in which 12 astronauts  landed and walked on the Moon.  It took 55 hours and 40 minutes from Earth to reach Moon during the Apollo 8 mission. Based on the recent  human race challenging issues like climate change, global warming, increase in population, the human race is left with  only one option. Finding another planet to colonize. In the year 2012, a nonprofit Dutch organization proposed a  mission called Mars One – a one way trip to Mars and establish a permanent human colony (Wikipedia, 2018) . Even  before some scientist has conducted missions to Mars. The first spacecraft Mariner 4 from NASA is the first spacecraft  that made the journey from Earth to Mars (The National Aeronautics and Space Administration, 2006). The total flight  time was 228 days. In th e 50 year Mars exploration it takes approximately 150 -300 days to reach Mars. The first  reason for such a long duration is the distance. Mars is 55 million km away and the main challenge is pointing the  direction of launch since both the planets orbit arou nd the Sun. Till date, the longest space exploration mission is  completed by Russian cosmonaut Valery Polyakov (Wikipedia, 2018). The duration of his space exploration was 438  days from January 1994 to March 1995.    ","buffalo.edu,",University at Buffalo,United States,43.0018,-78.7895
21,AN ATTRIBUTE AGREEMENT ANALYSIS METHOD FOR HFACSINTER-RATER RELIABILITY ASSESSMENT,@odu.edu,"Accident Investigation, HFACS, Inner-rater Reliability ","Inter-rater reliability can be regarded as the degree of agreement among raters on a given item or a circumstance.  Multiple approaches have been taken to estimate and improve inter-rater reliability of the United States Department  of Defense Human Factors Analysis and Classification System used by trained accident investigators. In this study,  three trained instructor pilots used the DoD-HFACS to classify 347 U.S. Air Force Accident Investigation Board  (AIB) Class-A reports between the years of 2000 and 2013.  The overall method consisted of four steps: (1) train on  HFACS definitions, (2) verify rating reliability, (3) rate HFACS reports, and (4) random sample to validate ratings  reliability.  Attribute agreement analysis was used as the method to assess inter-rater reliability. In the final training  verification round, within appraiser agreement ranged 85.28% to 93.25%, each appraiser versus the standard ranged  77.91% to 82.82%, between appraisers 72.39%, and all appraisers versus the standard was 67.48%.  Corresponding  agreement for the random sample of HFACS rated summaries were within appraiser 78.89% to 92.78% and between  appraisers 53.33%, which is consistent with prior studies. This pilot study indicates that the train-verify-rate-validate  attribute agreement analysis approach has the potential to aid in improving HFACS ratings reliability and contributing  to accurately capturing human factors contributions to aircraft mishaps.  Additional full-scale studies will be required  to verify and fully develop the proposed methodology.    Keywords  Accident Investigation, HFACS, Inner-rater Reliability    Introduction  Reason (1990) Accident Causation Model, also known as the Swiss Cheese Model, is a theoretical model that seeks  to explain how accidents manifest across organizational levels.  The model’s main assumption is that accidents occur  in such a way that the causes have relationships across organizational levels.  A second assumption is that, at minimum,  organizational levels need to function together to prevent accidents.  From these assumptions, Reason theorizes that  most accidents can be traced to active and latent human failures resulting from prior latent human failures at higher  organizational levels.  The  Human Factors Analysis and Classification System (HFACS), originally adapted from  Reason’s model to aviation by Wiegmann and Shappell (2003), identifies four tier levels within an organization at  which human errors can occur: Organizational Influences, Unsafe Supervision, Preconditions for Unsafe Acts, and  Unsafe Acts.  The HFACS has been used by the United States Department of Defense (DoD) since 2005, (DOD, 2005)  as DOD HFACS with some changes especially at the levels of Preconditions for Unsafe Acts and Unsafe Acts.  The  DOD HFACS (2005) is composed of the 4 main tier, 14 sub-categories (referred to as category in the Wiegmann and  Shappell study), and 147 nanocodes for detailed classification of organizational human errors contributing to aircraft  accidents. .   There have been many studies toward the development of accident causation models and frameworks due to  desire for decreasing human errors in aviation accidents that result in fatalities and cost a great amount of resources  in terms of investigation time, loss of aircraft assets, and litigation. (Yesilbas & Cotter, 2014)  Among these studies,  no particular or a notable method has been found for evaluation or testing the HFACS taxonomy for validity and rater  reliability.  Given the HFACS’s central role in classifying human errors that contribute to aviation accidents, its  validity and the raters’ reliability constitutes a substantial function that empower to comprehend the real cause of the ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
22,,@temple.edu,Engineering Leadership and Entrepreneurial Personality ,"This paper attempts to empirically test the confluence of two emerging streams of literature and their impact on the  development of engineering leaders.  Recent research into the development of engineering leadership in industry argues for three dimen sions of  engineering leadership: mentoring of younger engineers, networking across organizational boundaries to solve  unique challenges, and the ability to bring new products and services to market.  The third vector of bringing new  products to market is closely aligned to the concept of entrepreneurship. Additionally, recent research in the field of  entrepreneurship argues for a typology of three identities in the entrepreneurial personality: the inventor, the  founder, and the developer.   It is reasonable to argue that these personality identity types are not limited to entrepreneurs and are likely  to also exist in the engineering population. The potential connection between these personality identity types and the  third vector of engineering leadership poses two questions of importance to engineering managers. First, how widely  are these personality identity types distributed among the engineering population and secondly, can engineering  schools encourage the development or activation of these personality identity types?  This paper is an initial attempt to address this significant issue. A fully validated survey instrument will be  used to measure the three entrepreneurial personality identity types at a large engineering school in the United  States.  Additionally, the presence of these personality identity types will be measured at strategic points in the  senior design sequence to test the effectiveness of senior design to activate latent entrepreneurial personality identity  types among engineering students preparing to enter industry. Statistical analysis will be presented.     Keywords  Engineering Leadership and Entrepreneurial Personality  Introduction  The call to include leadership development in engineering education is heard from many stakeholders. This often  conflicts with the core mission of engineering schools to educate students in the scientific principles basic to the  practice of engineering. This paper utilizes emergent theory and measurement instruments in the field of  entrepreneurship scholarship to study if worthwhile leadership development can be accomplished with a minimal  addition to the burden of undergraduate engineering education.     Literature Review  It is increasingly argued that the education of engineering students should include the path toward greater leadership  responsibilities.  Industry has called on engineering schools to produce graduates capable of leading  multidisciplinary teams and combining technical and business capabilities to contribute to the maintenance of a  firm’s competitive edge (Farr and Brazil, 2009).  The National Academy of Engineering (NAE) has identified  opportunities for “engineers to exercise their potential as leaders, not only in business but also in the nonprofit and  governance sectors” (Cough, 2004).The NAE further emphasizes that graduates “must understand the principles of  leadership and be able to practice that in growing proportions as their careers advance”.  Not only has the NAE ",temple.edu,Temple University,United States,39.981188,-75.15628280757346
23,ENGINEERING AN ALLIANCE BETWEEN WMU AND FSW,@gmail.com,"engineering management, engineering management e ducation, AA degree transfer, BS in engineering management ","Western Michigan University (WMU) and Florida SouthWestern State College (FSW) entered into an alliance in  2016. This alliance allows students the ability to take WMU courses at either WMU’s Punta Gorda Regional Center  (co-located with the FSW satellite campus) or online.  Students can choose from multiple high -profile and in - demand academic programs.  The vision is to provide programs that will meet the demonstrated needs of the local  community as well as enhance economic development.    This paper investigates FSW developing a two -year feeder program into WMU's Engineering Management  bachelor’s degree program, common ly referred to as a two -plus-two (2+2) program. Analyzing the viability of  instituting a program where students from Florida could graduate with an associate of arts degree from FSW and  then transfer as a junior to WMU to complete an engineering Bachelor of Science degree will result in a proposed  plan for consideration by both institutions.   This opportunity is designed to meet the needs of the Southwest Florida community, enabling the  institutions to collaborate for the mutual benefit of all students. Thi s paper will develop a detailed plan to lay the  groundwork for a 2+2 programs.  The lessons learned from this program development process will be relevant to  others in the ASEM community who are interested in developing other programs and other institution al  collaborations, particularly in engineering management.      Keywords  engineering management, engineering management e ducation, AA degree transfer, BS in engineering management  technology    Introduction  Western Michigan University (WMU) and Florida SouthW estern State College (FSW) entered into an alliance in  2016. The alliance is designed to provide integrated programs based on the community's need and impact on  Charlotte County’s economic development  (WMU News, 2016) . The initial focus of the alliance was  on aviation  flight science and aviation management and operations programs, each leading to a bachelor's degree. The aviation  flight science program is offered on the WMU Punta Gorda campus (co-located with FSW’s satellite campus) and at  the Punta Gorda A irport through collaborative efforts with the Charlotte County Airport Authority (WMU News,  2017), with inaugural students beginning the program in the fall of 201 7 (Ruane, 2017) . This alliance allows  students the ability to take WMU courses at either WMU’ s Punta Gorda Regional Center or online.  Since that time, ",gmail.com,,,46.3144754,11.0480288
24,IDENTIFYING THE ELEMENTS OF REAL SYSTEM AGE,@usma.edu,"Systems Engineering, System Lifecycle, ilities ","Engineered systems do not age at the same rate and the calendar age of a system may not be as important of an  indicator of age as other system attributes.  Within the Department of Defense (DoD), services often perform upgrades  and modifications to systems to extend the life of these systems.  These modifications are efforts to extend the life of  a system that may continue to provide value to the DoD and in essence reduce the age of an engineered system.  This  paper examines the components of an engineered system’s theoretical real system age.  Literature on biological  systems, software, and the medical real age indicators provide ins piration and background for this work.  Much like  the medical real age for humans, the real age of engineered systems will provide an indicator of the health of these  systems when it comes to making the important decision to extend the life of a system or retire the system.  The paper  examines several systems, that the DoD has extended, to determine what attributes of the system, characterized by the  systems engineering ilities, reduce the real age of a system.  Engineering managers and systems engineers can use the  real system age to determine if the DoD should extend the life of a legacy system, or retire it in favor of a newly  developed system.  This work provides a vital first step to identify important attributes of systems that impact a  system’s real age that will lead to a mathematical representation of real system age.    Keywords  Systems Engineering, System Lifecycle, ilities    Introduction  Within the Department of Defense (DoD), systems often operate well beyond their initial planned retirement  date as DoD decision makers often decide to upgrade and extend the life of systems rather than develop a new system.   The calendar age of a system is often considered one of the main factors when making the decision to retire a system;  however, other attributes of a system are more influential in making this decision.  Engineered systems ages at different  rates based on their non -functional attributes and often operate well beyond their planned retirement.  Examples of  these systems include the B-52 bomber, which will operate for over ninety years when it is finally retired, and the F - 117 stealth fighter, which was retired after only twenty -five years of service.  The difference in system attributes  between these two systems influence the decision to retire versus extend the life of a DoD system.   This paper examines the components of an engineered system’s theoretical real system age to understand when  a system should be retired.  Literature from biological systems, software aging, and the medical real age indicat ors  provide inspiration and background for this work.  Similar to the medical real age for humans, the real age of  engineered systems will provide an indicator of the health of these systems when it comes to making the important  decision to extend the life  of a system or retire the system.  The paper examines several systems, that the DoD has  extended, to determine what attributes of the system, characterized by the systems engineering ilities, reduce the real  age of a system.  Engineering managers and syst ems engineers can use the real system age to determine if the DoD  should extend the life of a legacy system, or retire it in favor of a newly developed system.  This work provides a vital  first step to identify important attributes of systems that impact a  system’s real age that will lead to a mathematical  representation of real system age.     ",usma.edu,United States Military Academy,United States,41.3927227,-73.95986305044411
25,APPLICATION OF PROCESS MINING IN PROJECT MANAGEMENTCONTEXT: AN EXPLORATORY CASE STUDY IN SOFTWAREDEVELOPMENT PROJECTS,Missing,"Process Mining, Case Study, Project Management, Data Mining Processes, Project Data Mining.  ","In the fast changing environments where projects are presently inserted, more and more events are being recorded in  current information systems, providing detailed data about the projects. The present work aims to show how Process  Mining can be used to create knowledge from data logged of project management processes. Through a case study  of a software development company, seven projects were mined. Following the steps of a process mining project  model, Process Mining techniques were employed and analysis on resources and activ ities were performed. The  main contributions of this paper are the detailed application of the steps of a process mining project model and the  practical resources and activities analysis resultant of the Process Mining.    Keywords  Process Mining, Case Study, Project Management, Data Mining Processes, Project Data Mining.     Introduction  Kerzner (2015) explains that work life of current project, program and portfolio managers is assaulted by massive  amounts of information that needs to be coordinated, organized and communicated. In current project situations, the  project managers find themselves negotiating scope changes, reallocating resources, checking the impact of budget  cuts and in parallel needs to deal with the demand of managing massive, fast -paced and m ultisourced flows of  information, organizing it and making the right decisions at the right time.  Considering the dynamic, complex and fast changing e nvironment where projects are inserted today,  PMBoK (2017) states that, through the project life cycle, a significant amount of data is collected, analyzed and  transformed. Project data is analyzed in its context, aggregated and transformed, becoming project information to  the various project management processes and a valuable repository of organizational kno wledge. The information  is verbally communicated or stored and distributed as reports in various formats.   According to PMBoK (2017), organizational process assets (OPAs) are all the plans, processes, policies,  procedures and knowledge bases that are specif ic and used by the organization that performs the projects. Those  OPAs include all artifacts, practices and knowledge used by one or all organizations involved in the execution of the  project that can be used to perform the project and have influence on it . Among the assets are included the lessons  learned from previous projects and historical information, for instance, time plans, risk assessments, quality  information, performance indicators history and all documentation created during the project life cyc le.  Larson and Chang (2016) affirm that the documentation cre ated in the project is valuable  but it has an  intrinsic problem: usability. Documentation has been a dreaded aspect for traditional project management  methodologies because it takes a lot of time  and effort to be completed, tends to be out of date and is rarely used  after its initial distribution. Create a comprehensive documentation is not an easy and fast task but producing no  documentation is even worse. Due to the speed that information needs to be available, it is primarily ignored.   Much of the success of the current and future project managers depend on the software and hardware that  they have available. This infrastructure will help  control the flow of incoming project information, process i t, make  decisions based on it and communicate it out to the project team members, stakeholders and clients (Kerzner, 2015).    Once the volume of data and information that project managers need to integrate increases, one emerging  trend practice in project management is the use of automated tools able to collect, analyze and transform information  so that it can be used to achieve project goals (PMBoK, 2017). Larson and Chang (2016) explain that the use of  tools like fast analytics, which are systems able to a cquire and show information quickly, can help accelerate the  access to information and aid in decision making processes. Whyte, Stasis, and Lindkvist (2015) describes that while ",Missing,,,46.3144754,11.0480288
26,DATA ANALYSIS FOR IDENTIFYING HIGH CHANCE SCENARIOS OFHAZARDOUS MATERIAL HIGHWAY TRANSPORTATION INCIDENTS,@mst.edu,"Hazardous material transportation, h azardous material transportation incidents , highway, worker safety, data ","Although the transportation of hazardous materials in the U.S. follows rigorous safety regulations, incidents happen  during any transportation phases. During past ten years, over 87% of the incidents occ urred on the highway. These  hazardous material highway incidents (HMHIs) have caused near 760 million dollars of damage, 194 hospitalized  injuries, and 110 fatalities. 81% of the fatalities and 87% of the hospitalized injuries are transportation workers. We  study HMHIs that caused hospitalized injuries  (type-H HMHIs) , and those caused fatalities  (type-F HMHIs) , all  during 2008-2017, to develop  a thorough understanding of these severe incidents. A distribution- based comparison  method is developed for assessing the heterogeneity of any incident sample. Pareto -type distribution  charts are  generated for each sample of study and its population on va rious data fields. Then, a series of metrics are developed  and used to measure  the heterogeneity of the sample through a series of pairwise  comparisons of the distribution  charts. Results show that both type -F and type -H HMHIs  are non -homogenous samples of  their population .  Moreover, unique features of each sample are identified . Following that, the study further develops a sequentially  unfolding strategy for efficiently identifying high chance scenarios of HMHIs. Six scenarios are identified, where  assistances provided to transportation workers will effectively lower the chance of type- H HMHIs, type-F HMHIs,  or both of them.    Keywords  Hazardous material transportation, h azardous material transportation incidents , highway, worker safety, data  analysis    Introduction  Hazardous Materials (HazMat) are those that can cause harm to living organisms , environment, or property.  They  pose risks when being transported from one place to another. The t ransportation of hazardous materials needs to be  operated in a safe manner by following safety procedures, such as T he Hazardous Material Transportat ion Act  (HMTA) enacted in 1975  (OSHA). In spite of following all the required safety procedures, incidents still occur  during any transportation phases. An accident in which an affected person receives immediate medical treatment is  called an incident. The persons affected include transportation workers, responders, and the general public. Incidents  occurring in the transportation of hazardous materials  may cause spillage, explosion, gas dispersion, fire, and  environmental damages (ECFR, 2018). Many of the incidents resulted in significant financial damages, injuries, and  fatalities.    The Pipeline and Hazardous Material Safety Administration (PHMSA ) is an agency of the United States  Department of Transportation , which acquires and maintains the data and statistics of hazardous material incidents  for all the modes of transportation including air, road or highway, rail, and water. The PHMSA statistics  of  hazardous material transportation incidents during 2008- 2017 shows that highway is the transportation  mode with  the largest numbers of fatalities, hospitalized injuries, and financial damages. Hazardous material highway incidents  (HMHIs) account for ov er 95 % of all fatalities, 72% of  injuries, and 72 % of all the damages , of all Haz Mat  transportation incidents. ",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
27,THE STATE OF THE ART IN SYSTEM DYNAMICS MODELING OFCOST OF QUALITY,@ttu.edu,"Quality costs, economics of quality, strategic planning, scenario planning ","Several studies have been performed to review the state the art in the cost of quality (COQ) literature. These studies  have focused on different aspects such as models, adoption, limitations, etc. Few works have been found addressing  the COQ simulation pro cedures. Simulation is a crucial tool  to attain preliminary inexpensive results prior to  potentially expensive or complex in situ studies. Currently, there are several simulation-based approaches to estimate  the COQ. These studies were based on different simulation techniques such as discrete event and linear optimization  to mention a few. One simulation technique that seems to be useful to understand the causal relationship between the  elements on COQ is system dynamics (SD). SD allows the analyst to obse rve causal effect relationships and their  aggregated structure to answer research questions, improve policy making and observe previously unknown or  unobserved behaviors. However, no review has been performed presenting the most suitable or best practices to  address theoretical issues in COQ via SD. The current work provides a review of different SD-related works to address  this problem aiming to provide a scope for future research on SD simulation of COQ.     Keywords  Quality costs, economics of quality, strategic planning, scenario planning    Introduction  Elizondo-Noriega, Güemes-Castorena, and Beruvides (2017a) proposed a hypothetical  model (Exhibit 1) to explore  potentially hidden costs in the cost of quality (COQ) composite that may be preventing some companies with a quality  management (QM) initiative from innovating and potentially causing large opportunity costs. To understand Exhibit  1 better, it is important to trace back its foundations. For instance, it is based on the prevention (P), appraisal (A) and  failure (F) model to measure COQ also known as PAF model, i.e., the costs that are related to prevention, appraisal,  internal and external failures are aggregated into those classifications which in turn are later aggregated into the total  COQ. The PAF model was proposed by Joseph Juran, and extended by Armand Feigenbaum, and others. In this regard,  two of the most renown theoretical representations or profiles of COQ are shown in Exhibit 2; the classic and the  modern. Regardless of the profile that is considered to understand the COQ phenomenon, its behavior exhibits a  common trait, i.e., while the investments in P and A increases, F costs decreases  up to certain point. Returning to the  discussion on Exhibit 1, it aims to provide an explanation to Foster’s (1996) theoretical model where technology  implementation should cause a discontinuity in either one of the COQ profiles given in Exhibit 2. In this sense, Exhibit  1 proposes that such discontinuity could be caused by the constant investment and reinvestment in QM and technology;  both being represented as feedback loops. In theory, such investments may have a critical role in affecting the market. ",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
28,AN EMPIRICAL STUDY ON THE EVALUATION OF URBANINFRASTRUCTURE INVESTMENT EFFICIENCY IN CHINA BASED ONDEA-MALMQUIST MODEL,@tju.edu.cn,"Urban infrastructure, DEA-Malmquist model, investment efficiency ","With the accelerating process of urbanization in China, the government's investment in urban infrastructure is  increasing year by year. The problem of reckless investment, over -investment and unbalanced regional development  have emerged while increasing the scale of investment. From the perspective of time evolution, this paper  analyzes  the total factor productivity（TFP） index, technological progress（TP） index and efficiency change（EC） index  of infrastructure investment efficiency in 31 provinces in China from 2008 to 2015. The analysis shows that the  investment efficiency of infrastructure fluctuating slightly in 2008 to 2015. The change of TFP in 2008-2010 is mainly  affected by the change of technological progress. The change of TFP in 2010 -2015 is mainly affected by the change  of overall efficiency, and there is a certain degree of substitution effect between technological progress and technical  efficiency. From the cumulative effect, the efficiency of Chinese infrastructure investment is declining. From the  regional distribution, The TFP growth of the three major regions in China shows the rising trend of central, western  and eastern of China. The change of technological progress is the same as that of TFP in the central region, and the  efficiency of infrastructure investment shows little difference in the western regions. The TFP of infrastructure is  mainly affected by technological progress in the eastern regions. This paper helps to improve the decision -making  process of investment which in infrastructure projects with limited funds and the management level of infrastructure  construction projects.    Keywords  Urban infrastructure, DEA-Malmquist model, investment efficiency    Introduction  By the end of 2016, China’s urbanization rate reached 57.35%( National Bureau of Statistics, 2017), and China is still  in the rapid development period of urbanization.  Among the critical factors related to the Chinese new -type  urbanization strategy, investment and construction of urban infrastructure projects have been the most dominant  driving forces (Yang et al. 2015). Different infrastructure projects will be constructed to meet the increasing demands  for basic public service, which is crucial for the citizens’ life and production (Li et al. 2016) "" The national new  urbanization planning (2014 -2020)"" clearly points out that it is necessary to establish a sound statistical monitoring  index system as well as the statistical comprehensive evaluation index system, and monitor dynamically and analyze  the development of urbanization , and promote the implementation of the new -type urbanization planning.  The  carrying capacity of urban infrastructure directly restricts the development process of urbanization (Yuan Xiaoling,  Zhang Yusheng, 2015). In February 2016, ""the Several Opinions on Strengthening the Management of Urban Planning  and Construction"" empha sized the need to strengthen infrastructure construction and promote the development of  new-type urbanization. This indica tes that infrastructure investment and construction in various cities have become  an important foundation for the development of new-type urbanization in China. In 2017, the government work report  pointed out that China should continue to strengthen the effective investment of infrastructure, such as rail transit and  telecommunications, and should make full use of the key role of the inf rastructure effective investment in adjusting  market structure. ",tju.edu.cn,Tianjin University,China,39.107253299999996,117.17789778037583
29,PROCESS APPROACH IN DESIGN OF INDUSTRIAL MEGAPROJECTS,@projectqualityconsulting.com,"Industrial megaproject, engineering design, process approach, rework, design processes ","Industrial megaprojects may experience cost and schedule overruns in excess of 30% and 15% respectively.  Given  that these projects have budgets over $1,000,000,000 (some have budgets exceeding $10 billion dollars) and multi - year schedules, this represents a massive potential loss for owners and construction  companies.  Companies involved  in megaprojects must rely on their business and field work processes to control the huge amount of work that takes  place and to reduce the likelihood of major cost and schedule overruns.   Quality Management philosophies such as  ISO 9001 and Total Quality Management require a process approach to work activities.  Rework is the biggest single  contributor to overruns and the majority of rework originates in the Engineering phase.   This paper discusses  complexity, rework, and other factors that influence cost and schedule overruns, then identifies 22 proc esses that are  critical to engineering design work on industrial megaprojects and briefly discusses their contribution to the completed  design.  It then looks at  11 commonly problematic design areas and identifies the design processes associated with  each of the areas.  The process approach is then examined to determine if it is suitable for addressing the problematic  design areas and improv ing design quality.  This analysis assists technical engineers, project engineers, and project  managers efforts to reduce the severity of cost and schedule overruns.     Keywords  Industrial megaproject, engineering design, process approach, rework, design processes    Introduction  Industrial megaprojects are an intense endeavor where projects with a planned cost of at least $1.0 billion (COAA  Alberta Report #2, 2014; Sun & Zhang, 2011) are executed as quickly and efficiently as possible to construct a new  capital asset for an organization.  Industrial megaprojects are prone to cost overruns that can be up to double the initial  estimated cost and average 30% for Canadian oil sands projects (COAA Alberta Report #2, 2014).  Schedule overruns  for these projects average over 15%.  Cost and schedule overruns occur in over 90% of projects undertaken (Abdullah,  Hamzah, Ismail, & Ab Razak, 2015).  When a project experiences a massive cost overrun, it can be devastating to the  owner(s) as not all owners can absorb an additional several hundred million or few billion dollars to complete a project.    Megaprojects (often referred to as programs) are not commonly studied even though their size, budget, and  scope make them exceptionally attractive and challenging.  When studied, the focus has been mo re on the owner  management strategy and the project execution strategy than on the inner workings of the project (Sun & Zhang,  2011).  The majority of issues experienced in construction projects are design -induced (CII, 2016A; Lopez & Love,  2012; Love, Edwards & Irani, 2008).  Processes and procedures are in place for the engineering phase of megaprojects.   These are similar to processes used for most industrial projects but the rigor is increased to accommodate the quantity  of work pushed through them.  Ho wever, engineering process failures continue and cause significant rework  in the  field.  Accepted Total Quality Management (TQM) and International Standards Organization (ISO) principles require  process audits and continual improvement to address the failu res.  The authors are unaware of research  publications  that suggest using audits to address engineering process failures in megaprojects.  This paper provides a brief  background and then examines the process approach advocated by TQM, ISO and the British S tandards Institute  (BSI). The process approach is then suggested for use in the engineering (design) phase of megaprojects .  This paper  also discusses audits, the complexity associated with industrial megaprojects, and issues related to project rework.  A  publication from the Construction Industry Institute (CII) released in late 2016 identifying 11 design deliverables that ",projectqualityconsulting.com,,,46.3144754,11.0480288
30,A TALE OF TWO ETHNOGRAPHIC CASES,@wmich.edu,"ethnography, empathic design, innovation, VOC, disruption, new product development ","The practice of ethnography is making a slow move from academe to private enterprise. Taking ethnography’s  customer-centered approach ( emic) instead of a company -centered approach (etic ) can yield unique insights into  product and service design. Christensen’s (1997) Innovator’s Dilemma is an accepted model whereby a lower - featured, lower-cost product is adopted widely and eventually disrupts the incumbents. But what about when a product  causes a disruption without lowering the price? Many R&D units, especially those operating at the high end of the  market, can’t afford to enter a low-price market after spending millions in development and having a reputation as a  high-end market leader.  This paper shares the strategies of Ethnographic Innovation (EI n) and Ethnographic Disruption (EDi)—two distinct  modes of applied corporate ethnography — where new products either advance a market leader’s position (EIn) or  disrupt the competition ( EDi). Ethnographic Innovation is when a market leader in the upper price tier o f a mature  market uses ethnographic tools within a research process to re- energize growth without reducing price. Microsoft’s  development of the Xbox controller illustrates the use of this strategy. Ethnographic Disruption is when a company  enters a mature market (with large incumbents) and takes significant market share without reducing price. Samsung’s  development of the flat -panel television provides a rich case of Ethnographic Disruption. Through case studies and  rich descriptions, this paper investigat es strategies to conduct EI n and ED i in today’s competitive, R&D -driven  organizations.  Keywords  ethnography, empathic design, innovation, VOC, disruption, new product development  Introduction: The Rise of Corporate Ethnography in New Product Development  Once the province of well-funded Fortune 100 R&D departments, corporate ethnography is now complementing the  traditional voice -of-the-customer (VOC)  methods (cf. Griffin & Houser, 1993) . Ethnography is rooted in cultural  anthropology and its methods were primarily practiced by academics with prolonged research studies (those lasting a  year or more were very typical). Corporate R&D departments typically conducted  customer research by asking  customers what they want, giving them prototypes and asking what the y would tweak, or observing customers using  their products and asking about things that should change. These traditional practices were very rational, had  checklists, and a good fit with the traditional, structured, engineering mindset. These are called etic approaches—they  are company-centered (Ladner, 2016). In the etic approach, the company creates questions and observation protocols  based on what they are looking for. Unfortunately, when you know what you’re looking for, you probably won’t  surface what you’re not looking for —the unarticulated user needs, the things the customer w ants but can’t tell you  that they want them.   This exists in contrast to the emic approach—the customer -centered approach taken by ethnographers (Ladner, 2016).  When we’re etic, we write our ten questions, read them off in order, take notes, and go home. When we’re emic, we’re  visiting the customer in situ, we have some ethnographic questions that start the conversation, and we’ll ask probes as  we go along. We may also observe the customer not just using a certain product, but performing a specific task or just  participating with them in activities where some discovery concerning products or innovations may occur  (Mallak et  al., 2011). For example, if an R&D engineer wanted to learn how a specific medical procedure could be improved  using new medical devices , he or she could practice particip ant observation in the operating  room (OR) to see the ",wmich.edu,Western Michigan University,United States,42.2832928,-85.61026110719504
31,MODELING AND UNDERSTANDING ENERGY SHARINGCOMMUNITIES: A SYSTEM OF SYSTEMS APPROACH,@mst.edu,"Renewable energy sources, sharing economy, system of systems, complex adaptive systems  ","The renewable energy technology has recently advanced dramatically, accelerating the society’s pace of transitioning  to a sustainable living environment. Distributed renewable energy generators and energy storage devices are widely  installed, which are owned and operated by various entities. Facing inter mittent and volatile renewable generations,  they have recognized the need for collaborative energy management. As more and more distributed renewable  generators are being connected to distribut ion networks, owners of the networks are under the pressure of changing  their business model to adapt to the new trend. Forming sharing communities locally is a potential solution which  allows the participants to share excess generations and unmet demands within their community. Forming energy  sharing communities also benefits distributed networks from multiple aspects. This paper aims to develop a thorough  understanding of this new business model and, meanwhile, explores an approach to the management of energy sharing  communities. Through analyzing the participants of  energy sharing communities, the paper first identifies nine  characteristics of the communities. Accordingly, the paper justifies that cooperative sharing communities can form a  decentralized complex adaptive system of systems (DCASoS). The paper further classifies the nine characteristics  into two types: underlying characteristics and the derivative characteristics. The goal of managing energy sharing  communities is to enhance the good effects and reduce the bad effects of the derivative characteristics gi ven its  underlying characteristics. Based on this fact, the paper develops a system of systems (SoS) approach to describing,  modeling, and analyzing sharing communities, which builds a foundation for engineering the corresponding DCASoS.     Keywords  Renewable energy sources, sharing economy, system of systems, complex adaptive systems     Introduction  Sharing economy has had a positive impact on many industries including the energy industry. A sharing economy is  an innovative solution to the influx of co nsumer demand while conventional  forms of electricity supply are  diminishing. Distributed renewable energy generators and storage devices are widely installed along with the decrease  of the investment cost required for traditional consumers to evolve into  prosumers who have the capability to privately  generate electricity (Kargarian  et al., 2014). Th e distributed generation can be  initially used to fulfill the owner’s  demand while any excess electricity can be shared with their neighbors who need more. Sharing energy provides an  additional option for prosumers to deal with overgeneration besides storage, selling back to the grid, or curtailment.   Current net metering laws require utility companies  to financially compensate prosumers  who put excess  electricity back into the grid (Rossi, 2016). Utility companies are now presented with various challenges. They are not  only facing additional stochastic electricity supplies (Stoutenborough & Beverlin, 2008) but financially compensating  prosumers for their supply (Rossi, 2016). A potential solution to these issues is to cluster local end-users of utility into  individual energy sharing communities and coordinate the energy shared within individual communities and between  communities. The similar concept has been successfully applied to the management of multiple micr ogrids (Zhao et  al., 2018). Allowing for locally generated supply to be maximally disbursed by local consumers would reduce the  impact of stochastic supplies on the utility company (Liu et al., 2017). The utility is also able to minimize the cost by  distributing prosumer generated electricity locally.    This paper aims to develop a thorough understanding of the new business model for utility companies while  exploring an approach to the energy management of sharing communities . By taking into consideration current net  metering laws, an appropriate energy sharing model is proposed. The effectiveness of energy shar e is analyzed using  the proposed model and defined characteristics identified w ithin decentralized complex  adaptive system of systems   (DCASoS). The end the paper summarizes findings of this study and future work.      ",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
32,A SYNTHESIS OF DEFINITIONS FOR SYSTEMS ENGINEERING,@msstate.edu,"Systems Engineering, Attributes, Grounded Theory Coding. ","Systems engineering (SE) was developed as a means to address the intricacies stemming from a complex system.  Unlike traditional engineering, systems engineering is not limited to a set of rigidly defined basic theorems that are  anchored in science and related to physical properties. I nstead, SE applies structured methodologies to optimize the  performance of complex socio -technical systems.  B ecause of the diverse applications of systems engineering, the  existing scope and paradigms of SE are difficult to define, and this often leads to confusion and misunderstanding  regarding the universal definition of SE and its derivative terms. To lessen the confusion associated with  the use of  SE terms, this paper begins with a discussion of the origination of SE paradigms from several different standpoints.  The sco pe of the paper is organized i) to review the existing literature of SE and  discuss and synthesize the  definitional paradigm s related to SE , ii) to derive a set of underlying attributes of SE using advanced coding  techniques and analysis. We believe that the proposed approach will reduce the misunderstanding of SE connotations  and promote the understanding of different SE perspectives.     Keywords  Systems Engineering, Attributes, Grounded Theory Coding.    Introduction  After World War II, there was a major revolution in the industrial and construction sectors around the world and new  process and techniques started to evolve to manage the intricacies stemming  from a complex system. World War II  was the first-time practitioners realized the importance of managing and synchronizing various complex systems to  achieve long -term objectives.  As a result, a new discipline named “systems engineering” emerged to manage the  challenges associated with the newly developed process and system. In 1943, the National Defense Research Board  formed a Systems Committee in cooperation with Bell Laboratories to conduct a project named C -79 to improve the  communication system of the Air Warning Service. Bell Telephone Laboratories was perhaps the first organization  to use the phrase “systems engineering”.  When different systems group together and integrate to accomplish an  emergent mission, it is known as S ystem of S ystems (SoS). Interested readers can refer to the works of Krygiel  (1999), The Defense Acquisition University (2010), Sillitto (2010) to understand the different complex structure of  the systems such as Federation of Systems (FoS), Families of System, Ultra-large -Scale Systems (ULSS).  From a generic point of view, Systems E ngineering (SE) is the application of structured engineering  methodology to the design, analysis, and development of complex s ystems in order to meet the stakeholder’s needs.  The activities of systems engineering include the identification of the stakeholders’ requirements from the conceptual  design to the system development and operation to the product disposition. While, traditional engineering disciplines  often fail to provide an appropriate solution for the complex environment and it’s rapidly shifting states and   emerging circumstances, systems engineering approaches have proven to be effective in addressing complex systems  problems.   Today, the term systems engineering encompasses many different meanings and interprets  to mean different  things to different practitioners . This  lack of clarity  has resulted from the SE literature, which is a fragmented  compilation of the use of t he term “systems engineering” by practitioners from different field s and domains  who  define the term from different perspectives based on the nature of their workplace (Kasser, Hitchins, & Huynh, 2009; ",msstate.edu,Mississippi State University,United States,33.4386876,-88.79432320417112
33,"THE INFLUENCE OF INNOVATION ON MICRO, SMALL AND MEDIUMSIZED ENTERPRISES IN COLOMBIA",@tdea.edu.co,"Innovation, MSME performance, product and process innovation, alliances, ANOVA.  ","This paper studies the influence of innovation on Micro, Small and Medium Sized Enterprises (MSME) performance  in Colombia through the 403-MSMES-survey analysis. In particular, this paper measures the effect of participation in  R&D alliances, product innovation, and process innovation on it. MSME performance is measured through a  composite index, estimated through Princ ipal Components Analysis – PCA- using polychoric correlations, which is  based on eight self -reported assessments of MSME performance. Then, this measure of performance is related to 1)  MSME participation in R&D alliances, and 2) the product and process development stance of the MSME based on an  adaptation of the Miles and Snow business classification scheme, by means of an ANOVA and a linear regression.  Colombian SME’s are not significantly benefitted from participation on R&D alliances. Instead, their perfo rmance  appears to be dependent upon their internal innovation efforts directed to product development. Moreover, the results  suggest that imitators get a performance almost as high as innovators.    Keywords  Innovation, MSME performance, product and process innovation, alliances, ANOVA.     Introduction  Micro, Small and Medium Sized Enterprises (MSMEs) – hereafter we will use the terms SME and MSME  interchangeably- are key players in Colombia’s business field. As they make a big contribution to employment and  output. Recent estimates, show that SMEs provide 67% -jobs and 28% -Output (GDP) in Colombia (Dinero, 2016).  Similarly, Bell and Teima (2015) estimate that formal SMEs provide 45% of jobs and produce 35% of Output (GDP)  in emerging markets, figures that coul d be higher if informal sector SMEs were accounted for. The aforementioned  facts together with SMEs fast adaptability to changing market conditions makes them a key player in the production  and distribution of wealth (Mezgár et al., 2000; Restrepo and Vanegas 2015).   Innovation and SMEs are closely tied. Firstly, the development of a new idea —be it a new product or a new process—  is the key reason why entrepreneurs establish a new business. Secondly, “The entrepreneur or small business manager  needs to have an innovative edge to compete against bigger incumbents” (Rosenbusch et al. 2011). Otherwise, it is  likely they going out of business in long-term. Thirdly, SMEs can adjust to environmental changes faster than bigger  organization “due to their nimbleness, missing hierarchies, and quick decision-making” (Rosenbusch et al. 2011).  Innovation can foster the SME performance. Many of the studies tie in with on the positive effect of product innovation  on firm performance  (Mohd Rosli and Sidek 2013; Rosenbusch e t al. 2011; Zahra and Bogner 2000) . Although on  some occasions the relationship between innovation and financial performance does not always happen.  (Vermeulen  et al., 2003). After mixed results so far, It might be said that exist other dimensions, included that CEO in SMEs takes  responsibility in almost all the decisions, including those regarding with technical changes (Bougrain and Haudeville,  2002; Cataño et al., 2008; Rosenbusch et al., 2011).  Nonetheless, SMEs face big challenges, as several studies p inpoint SMEs’ structural weaknesses, which undermine  their innovation capabilities and competitiveness, thus compromising their survival. Increasing market openness, an  accelerating rate of technological change, together with a loose management of knowledg e assets and human capital,  poor qualifications of firm owners and employees, localization and infrastructure disadvantages, all make it difficult  for SMEs to prosper in highly competitive markets  (Edwards et al., 2005 ; Singh, et al., 2008; Terziovski, 2010;  Gunasekaran et al., 2011; Rosenbusch et al. 2011 ; Restrepo and Vanegas, 2015; Taneja et al., 2016; Vanegas et al.,  2019).   In such a complex market environment, it is believed that SMEs can earn a competitive edge through the added  flexibility conferred by its small size. It is argued that this flexibility and fast decision making create an environment  conducive to innovation. Innovation can either be disruptive, or incremental –like a small improvement in a production ",tdea.edu.co,Tecnológico de Antioquia,Colombia,6.28027755,-75.582804
34,INTEGRATING SUSTAINABLE PROJECT TYPOLOGY ANDISOMORPHIC INFLUENCES: AN INTEGRATED LITERATUREREVIEW,@mst.edu,"Sustainable Projects, Project Typology, Integrated Literature Review, State of the Art Matrix Analysis. ","Prior research demonstrates that projects are affected by external factors (e.g. the environment in which the projects  exist), and sustainable projects are no exception. Understanding the effects of these external factors may help project  managers be better informed in decision making in the earlier planning phase. This research uses an integrative  literature review to determine how the coercive, normative, and mimetic external influences of institutional theory  can best be used to develop a sustainable project typology. The literature is group ed by topic using a State of the Art  Matrix Analysis (SAM). Key research questions that emerged include how these institutional influences affect the  expected level of change, level of uncertainty, pro ject team skills and experience levels, and the level of technology  information exchange. Results of this research will provide the engineering manager with a better understanding of  issues surrounding the influence of institutional theory on sustainable project decision making.     Keywords  Sustainable Projects, Project Typology, Integrated Literature Review, State of the Art Matrix Analysis.    Introduction  The use of traditional energy sources is accompanied with several issues such as pollution, dependency of  geographical location, low efficiency, as well as the non-renewability of these resources (Qin, Grasman, Long, Lin,  & Thomas, 2012) . These traditional energy sources that use fossil fuels are known to be a major cause of global  warming, as well as being generally harmful to human health and the environment through the Green House Gases  (GHGs) emitted by these sources (Alma soud & Gandayh, 2015) . Consequently, it is imperative that alternative  sustainable energy sources are found and developed as a substitute for traditional sources. F ortunately, worldwide  efforts to promote sustainability has led to increased awareness of  the adverse effects of using traditional energy  sources, leading to an increase in research devoted to sustainability in a wide variety of fields.  In the past 10 years, there has been a significant growth in the sustainable project management research,  where project management played an important role in  the implementation of sustainable development within  organizations and societies (Silvius, Kampinga, Paniagua, & Mooi, 2017) . To help project managers and decision  makers in the decision -making process, part of this resea rch was devoted to developing project typologies in which  projects are classified into a set  of ideal types. The degree to which a project fits any of the ideal types provides an  indication of the outcome of the project (Niknazar & Bourgault, 2017) . In these typologies, projects are classified  based on many different project characteristics  such as level of change, uncertainty level (Shenhar & Dvir, 1996) ,  project team skills and experience levels, and the level of technology information exchange (Stock & Tatikonda,  2000). These characteristics, and many others,  are subject to influence from a wide variety of external factors  consequently impacting the overall proj ect outcome (Gudienė, Banaitis, Banaitienė, & Lopes, 2013; Musa,  Amirudin, Sofield, & Musa, 2015). ",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
35,DETERMINING MICROGRID ENERGY SYSTEMS DYNAMIC MODELINPUTS USING A SAM ANALYSIS,@mst.edu, ,"With a crumbling energy infrastructure, the need for innovative solutions towards grid modernization are imperative.  Local and state governments will play a central role in the adoption and regulation of such solutions. This study takes  the barriers to entry as determined by a state government and cross references them with the research being conducted  in the field of microgrid evaluation through means of a State -of-the-Art matrix (SAM) analysis and integrative  literature review. The results of this study indicate that some of the barriers to adoption are adequately covered in the  literature while others are not.  A system dynamics model is then developed from SAM inputs . These results may be  used by engineering managers to formulate experiments to more effec tively integrate microgrid energy systems into  the national energy infrastructure.     Keywords: Microgrid, Evaluation, State-of-the-Art Matrix, Systems Dynamic Model Inputs    Introduction  The United States’ energy infrastructure is in a state of disrepair. The American Society of Civil Engineers has  published “report cards” evaluating all facets of the country’s infrastructure for decades. In 2017, energy infrastructure  received a “D”. This grade is a function of several components, but the electricity compo nent’s contribution is  primarily due to aging infrastructure and economically devastating outages. Fortunately, ASCE provides guidance on  how to raise the grade: integration of renewable energy sources and distributed energy generation (ASCE Report Card,  2017).  Currently, renewable energy generation accounts for 10% of all generation compared to 15% for coal, 29%  for natural gas, and 37% for petroleum (EIA, 2017) . In addition to the economic and reliability issues addressed by  the ASCE, the  use of conventional energy sources has a considerable impact on the environment by means of  greenhouse gas emissions. In 2016, the United States emitted 6511 million metric tons of carbon dioxide equivalents  (USEPA, 2018). To adhere to the guidance given by the American Society of Civil Engineers, the United States must  increase renewable energy’s portfolio share and microgrids have emerged as a potential solution.    The Department of E nergy (DOE) defines microgrids as “a group of interconnected loads and distributed  energy resources within clearly defined electrical boundaries that act as a single controllable entity with respect to the  grid. A microgrid can connect and disconnect from the grid to enable it to operate in bot h grid-connected or island- mode” (USDOE, 2012). Given that microgrids can utilize renewable energy sources and serve as distributed energy  generation sources, there should be wide-spread adoption.   Technological innovation is not always met with wide -spread acceptance. That said, it is imperative that  researchers develop an understanding of specific barriers to adoption to better serve the public on critical technological  advancements (Long et al. , 2016). As it relates to microgrids, the question becomes: what are the barriers to wide - spread adoption and is the research addressing those areas?   In this study, an integrative literature review is used to analyze and discuss the current state of research related  to microgrids an d their evaluation. By assessing the literature, this analysis is intended to provide a comprehensive  and robust survey of the research being done, identify gaps in the research, and provide future researchers direction.  This will be achieved by a State -of-the-art matrix (SAM) analysis of past literature related to microgrids and their  evaluation.  ",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
36,APPLICATION OF LEAN TO REDUCE INTER-PLANTLOGISTICS: THE MARINE INDUSTRY PERSPECTIVE,@utc.edu,"Lean Manufacturing, Marine, Production Logistics, Simulation ","At times when organizational pressures such as on time delivery of products to customers and challenges in market  expansion, etc. become persistent, continuous improvement initiatives on the shop floor has been a key success story  for some companies. Applications of Lean principles play an immense role in these initiatives. Lean as a philosophy  focuses on reducing waste, understanding customer values, and identifying critical processes to embark on consistent  improvements. This paper focuses on a systematic  implementation of Lean at a local manufacturing enterprise in the  marine sector. The objective is to reduce the logistics between subsequent production plants and minimize the  production costs per product. The systems approach of implementing Lean is base d on four steps: 1) data -driven  identification of impacting major factors, 2) identifying the critical process, 3) visualizing the current and future states  of the critical process, and 4) scenarios analysis  of resources’ utilization and cost -benefit analy sis. The approach  embraces on an integral use of tools such as Cause and Effect Diagram, Critical Path Method (CPM), Value Stream  Mapping (VSM), and a discrete- event simulation (DES). Onsite study and historical data analysis unequivocally  supported the ne ed for systematic allocation of resources for the inter -plants logistics. With an engineered  rearrangement of facilities in respective plants, the subsequent plants are now designated clearly as fabrication and  finishing operations plants. The results indi cated about 28.35% reduction of production costs per piece because of a  cumulative reduction of costs, which were associated with logistics inefficiency based on three product families.     Keywords  Lean Manufacturing, Marine, Production Logistics, Simulation    Introduction  Nowadays globalization and market expansion have required an increasingly and continuous improvement of the  processes and systems of organization s. Many companies seek support for these improvements in L ean principles.  Lean or Lean  Thinking as a philosophy focuses on reducing waste, understanding consumer values , and focusing  efforts on critical processes of  a production. Lean as a philosophy, methodology, or management concept has been  used and advocated by many including the experts who coined it “Lean”, Womack and Jones (1997). The application  of Lean has been boundaryless reaching regardless of the industry type, for example in healthcare (Yang et al., 2014),  in process industries such as cement production (Abrha et al., 2015), in systems design (Sawhney et al., 2010; Sawhney  & de Anda, 2014), and sectors.   However, there are very limited number of case studies, or applied research done in the marine products  manufacturing sector. In fact, this project served as a pioneer input on how Lean principles can be applied in local  manufacturers in the areas of marine-related products. In the conjecture of the company, as one of the leading marine- related products and accessories such as  boat towers, the efforts for the implementation of  Lean principles and ",utc.edu,University of Tennessee at Chattanooga,United States,35.0459,-85.2953
37,IDENTIFYING KEY VARIABLES ON SURFACE RADIATION BUDGETUSINGDESIGN OF EXPERIMENTS METHODS,Missing,"Experimental design, scientific data analysis, atmosphere, surface radiation budget, determinant optimal design  ","Earth’s Radiation Budget (ERB) is an accounting of all incoming energy from the sun and outgoing energy reflected  and radiated to space by earth’s surface and atmosphere.  ERB data collection and measurement poses a major  scientific and engineering challenge. The National Aeronautics and Space Administration (NASA)/Global Energy  and Water Cycle Experiment (GEWEX) Surface Radiation Budget (SRB) project pro duces and archives long-term  datasets representative of this energy exchange system on a global scale. Ground sites are capable of obtaining direct  measurements of all of the components of the SRB, but it would be very costly to cover the entire globe wth ground  sites.  Therefore, satellites are utilized to acquire data where ground sites would be impractical.   The problem is that  surface irradiance cannot be directly measured by satellites, so it must be derived from a variety of satellite  assimilation products and acquired atmospheric data.  This study utilized design of experiments to aid engineering managers in identifying key predictor variables  and their interactions for 14 shortwave radiative components of the atmosphere that are produced by the SRB  algorithm.  A D-Optimal design was chosen to study SRB inputs consisting of 13 atmospheric properties from a  sample geological location.  This enabled a 313 full factorial design of experiments to be reduced from over 1.6M to  128.  Second order response surface models were constructed from the results of a regression analysis to determine  the most influential input variables and two-factor interactions. This approach may enable scientists and engineering  managers in collecting and analyzing satellite data for Earth’s Radiation Budget.    Keywords  Experimental design, scientific data analysis, atmosphere, surface radiation budget, determinant optimal design     Introduction  The goal of this project was to produce reliable, globally derived top-of-atmosphere (TOA), within atmospheric and  surface radiative flux products relevant to the exchange of radiative energy between space and the Earth. Properly  calibrated ground sites measurements are capable of obtaining direct measurements of the downward fluxes of  energy to the surface of the Earth accurately. However, since it is not practical to cover the entire globe with ground  data measurement sites, including the oceans, it is desirable to use satellite analysis to estimate this information to  obtain global estimates of the data quantities (Whitlock, et al., 1994). Satellite instruments also provide observations  that are used to directly estimate the TOA information which ground sites cannot. The knowledge of atmospheric  properties and their fluxes is beneficial to many areas of research, including the determining of the overall radiation  budget of the planet and its variability (Stephens et al., 2012).     ",Missing,,,46.3144754,11.0480288
38,ROADMAP FOR AN INTEGRATED LEAN SIX SIGMA MODEL,@unl.edu.ec,"DMAIE, DMAIC, Quality, Service industry ","One of the main differences between a product and a service is the interaction and interdependency of people when a  service is delivered. Therefore, the complexity in the ladder becomes critical and mistakes such as: excessive cycle  time, inefficient processes, high costs and high variability; often affect the proper execution and delivery in the service.  Increasing efficiency and effectiveness could save money and time for the organization. Methodologies like Six Sigma  and Lean are traditionally used at manufacturing companies, but in the service industry, the effect and framework is  not yet clarified. The  aim for this paper is to present a roadmap of Lean Six Sigma that helps practitioners on the  service industry to reshape tools and steps of DMAIE (Define, Measure, Analyze, Improve and Embed). This research  is based on literature review focusing at the ch aracteristics and patterns of Lean and Six Sigma as well as the  integration models of Lean Six Sigma and service applications.    Keywords  DMAIE, DMAIC, Quality, Service industry    Introduction  Organizations are constantly looking for ways to improve their processes in order to obtain a satisfactory performance  and at the same time offer the best product or service to clients. In the case of the service industry, in which the process  is commonly executed in front of the client, improvements are desired to pr ovide customers with a fast and efficient  service, ensuring their satisfaction.   Companies that offer services search for  customer satisfaction and loyalty, providing confidence and  guarantees about their services; however, the customer is more demanding and also looks for quality and speed. The  dimensions of quality that customers of a service consider when evaluating the company, according to Parasuraman,  Zeithaml & Berry (1988) are:  a. tangibles (the appearance of physical facilities, equipment, and personnel);  b. reliability (the ability to perform the promised service dependably and accurately);  c. responsiveness (the willingness to help customers and provide prompt service);  d. empathy (the provision of individual care and attention to customers); and  e. assurance (the knowledge and courtesy of employees and their ability to inspire trust and confidence).   This type of companies must excel in those aspects, as well as in the innovation and continuous improvement,  to be able to compete in the current market. In the search for this continuous improvement, several methodologies  have been applied, like Six Sigma and Lean.   The Six Sigma m ethodology is a strategy used to improve performance, reduce costs, and improve the  efficiency and effectiveness of all processes of the organization (Antony & Banuelas Coronado, 2001) . Lean  meanwhile, is a p roduction philosophy that emphasizes the minimization of resources (including time) used in the  different activities of the company (Blackstone & Cox, 1998). These methodologies together create what is known as  Lean Six Sigma (LSS). The commonly known concept of LSS considers this methodology as a business strategy that  improves the performance of processes, costs and quality, thereby increasing customer satisfaction (Snee, 2010) .  Some service companies have used Six Sigma with successful results, examples of well -known success  stories of service implementations include Bank of America, Citibank, GE Capital Corp., GE Medical Systems, Mount  Carmel Health System and Virtua Health (Chakraborty & Leyer, 2013). ",unl.edu.ec,Universidad Nacional de Loja,Ecuador,-4.0360004499999995,-79.20192518678297
39,A THEORY OF ENGINEERS' MOTIVATION TO INFLUENCE,Missing,"lead, follow, influence, motivation, engineers ","This paper describes a theory of engineers’ motivation to influence  which explains why some engineers accept  invitations to become managers and others decline. The theory is based largely upon two recent ly published   qualitative studies. In the fi rst study, six engineers who recently accepted positions as engineering managers , were  interviewed to understand their motivation to lead. In the second study,  six engineers who, although qualified to be  managers, declined promotions to managerial roles  were interviewed  to understand their motivation to follow; as  such, the second  study sought to understand the experience  of these engineers’ motivation to lead through the  perspective of followership theory. Curiously, the explaination of their career choices offered by participants in both  studies were similar; in other words the same motivations drove some engineers to accept managerial roles and  others to refuse managerial roles. The present paper, then, proposes a  model of motivation to influence which is   intended to explain why some engineers accept management roles  and others do not. To do this, the theory  postulates the existence of constructs called motivation to influence, desire to manage, and openness to manage.  Furthermore, the proposed model speci fies how these three new constructs relate to motivation to lead and the  likelihood that an engineer will accept a promotion to management. The central concept of the theory is that  leadership and management are distinct constructs,  and with engineers, mot ivation to lead does not necessarily  translate to motivation to manage because, in many cases, engineers believe they can have greater influence upon an  organization in non -managerial roles. Implications for practice are also described along with recommend ations for  future research.    Keywords  lead, follow, influence, motivation, engineers    Introduction  This paper describes a theory intended to explain why some engineers accept invitations to become managers and  others decline. The theory is largely based upon two empirical studies. In the first study, Ulrich (2017a), interviewed  six engineers who recentl y accepted positions as engineering managers. In the second study, Ulrich (2017b)  interviewed six engineers who, although qualified to be managers, declined promotions to managerial roles.  Surprisingly, when asked about their motivations to either accept o r decline promotions into managerial positions,  both groups offered similar explanations. The central concept of the theory is that leadership and management are  distinct constructs, and with engineers, motivation to lead does not necessarily translate to motivation to manage  because, in many cases, engineers believe they can have greater influence upon an organization in non -managerial  roles.    This paper is organized as followers. First, a review of the literature is presented. Second, the theory of  motivation to influence is described in detail. This description includes ten  formally articulated propositions. Third,  practical implications and recommendations for future research are provided.    Literature Review  This literature review is presented in three sections. First, the literature is used to establish the difference between  the constructs of leadership and management. Second, the motivation to lead literature, beginning with Chan and  Drasgow (2001) is reviewed. This review includes seven quantitative studies which build upon Chan and Drasgow  and one quantitative study, Cerff (2006) which challenges Chan and Drasgow’s (2001) fundamental model. Third,  two recent qualitative studies of motivation to lead in engineers are described. These two papers, along  with Chan  and Drasgow’s (2001) seminal work, form the basis of the proposed model of engineers’ motivation to influence.   ",Missing,,,46.3144754,11.0480288
40,A SYSTEMS FRAMEWORK TO INTEGRATE A CIVILIAN RESERVECYBER FORCE FOR THE US ARMY,@gmail.com,"Cybersecurity, cyber reserve forces, talent management ","As threats continue to evolve within cyberspace, defense of the cyber domain requires that the most qualified and  technically skilled individuals be able to help protect this increasingly important area in national security. The US   Army defends the cyber domain against adversaries along multiple fronts —doing this effectively in cyberspace  requires that the military develop a plan to leverage the talents and capabilities of individuals in the private sector to  form an effective civil ian cyber reserve force. To successfully integrate this cyber reserve force, the US And must  take into consideration 3 critical concerns:  the employment of effective security controls, the cultivation of key cyber  skills, and the implementation of a cost- effective program that efficiently and effectively allows private sector  cybersecurity professionals to serve in the Army Reserves. This research employs both qualitative and quantitative  systems engineering models that help prioritize key stakeholder valu es and shape recommendations for future  consideration. The goal of this research is to equip the US Army with a framework that can successfully integrate and  implement a civilian reserve cyber force that is prepared for 21st Century warfare in cyberspace.     Keywords  Cybersecurity, cyber reserve forces, talent management    Introduction  Following the establishment of the US Army’s Cyber Branch in 2014, its duties have grown to encompass a wide  range of missions. With such an increase in responsibilities, the requirements for US Army Cyber personnel to  accomplish these duties have increased as well.  Even though the Army Cyber Branch is composed of a number of  Army elements such as US Army Network Enterprise Technology Command (NETCOM), 1st Information Operations  (IO) Command, 780th Military Intelligence (MI) Brigade, as well as several other key units, the vitally important tasks  that each of these organizations conduct on a daily basis have necessitated support and expertise from the civilian  cyber sector (US Army, 2010). This report summarizes the work that has resulted from a semester-long undergraduate  systems engineering capstone project. It analyzes what key stakeholders regarding this issue value most in order to  determine alternative solutions and the feasibility of these proposals. This report proposes how to constitute a civilian  reserve cyber force for the US Army that affords, offers, and provides the nation with the best opportunities to defend  its critical infrastructure in cyberspace.      Background  As part of the literature review for this study, the researchers analyzed key lessons from how the US Army has already  integrated civilians into its ranks, including how the US Army currently structures its Reserve Component forces,  employs a US Civil Reserve Air Fleet (Congressional Research Service, 2006), and leverages the expertise of medical  professionals in periods of national emergency. Through this background research, a consistent theme has emerged  centered on the significant risk posed by insider threats and personnel security concerns. The research has also revealed  a number of noteworthy considerations for operating in the cyber domain that highlight the urgent need to include  civilian cybersecurity specialists in the defense of US national interests in cyberspace.    ",gmail.com,,,46.3144754,11.0480288
41,MOTIVATION TO LEAD AND ASIAN AMERICAN ENGINEERINGMANAGERS,Missing,"culture, Asian, motivation to lead, engineers  ","A research effort intended to explain why Asian American engineers are underrepresented in management at  American companies is described. Specifically, the literature establishes that Asian Americans generally outperform  European Americans in American schools and yet Asian American engineers are underrepresented in leadership  roles in American companies. Furthermore, the literature establishes individuali sm and collectivism as antecedents  to motivation-to-lead. Accordingly, a cross -sectional quantitative study was conducted using validated instruments  to measure (a) four patterns of individualism and collectivism, and (b) three factors of motivation -to-lead. These  data were used to test seven hypotheses which reflected the notion that as Asian American engineers experience  their Asian culture in an American context, their overall motivation to lead may be less than their European  American coworkers. Further more, the data were also used to test three hypotheses relating to how engineers’  motivation to lead differs from non-engineers, regardless of culture. Three findings emerged from the analysis. First,  Asian American engineers do experience higher levels of  vertical collectivism than European American engineers.  Second, although differences in vertical collectivism exist, there was no difference between the motivation -to-lead  between the Asian American engineers and the European American engineers. Third, regardless of culture and  ethnicity, engineers experience less social/normative motivation to lead than non -engineers. Taken together, it is  theorized that the perception that Asian American engineers do not want to lead may be rooted in a failure to  recognize that observed lower levels of motivation -to-lead may be due to occupation (engineer) rather than culture  (Asian).    Keywords  culture, Asian, motivation to lead, engineers     Introduction  An abundance of evidence demonstrates that Asian American students tend to outperform European American  students in American elementary schools, secondary schools, and universities (Liu & Xie, 2016; Hsin & Xie, 2014).  As such, scholars have gone as far as describing Asian Americans as “those amazing Asians” (Xin, 1997, p. 335)  and the “model minority” (Sy et al., 2010, p. 904). One result of the academic excellence of Asian Americans is that  Asian Americans are three times more likely to become engineers th an European Americans (Tang, 1993). Because  of their predisposition toward engineering and their track record of academic superiority, one might reasonably  expect Asian Americans to be overrepresented in engineering leadership positions (Sy et al., 2010). However, the  same body of data that suggests Asian Americans outperform European Americans in academics demonstrates that  Asian Americans are underrepresented in engineering leadership roles (Tang, 1993). In fact, the underrepresentation  of Asian Americans in leadership roles extends across all industries in the United States (Xin, 1997; Sy et al., 2010).    At least four explanations for the underrepresentation of Asian Americans in leadership roles have been  proposed. First, it may be a simple case of discrimination (Xin, 1997). Second, Xin found that ineffective (and  culturally bound) impression management by Asian Americans is a contributing factor. Third, Sy et al. (2010) found  that differences in perception of leaders in Asian cultures and Western cultu re is also a contributing factor. And  finally, especially in engineering, Xin (1997) suggested that it may be simply that, within the American context,  Asian Americans do not aspire to leadership.    The present research was designed to investigate this last theory, that disinterest in leadership is a  contributing factor in the underrepresentation of Asian Americans in American engineering leadership roles.  Specifically, the present research is a cross -sectional quantitative study which uses (a) Chan and Drasgow’s (2001)  Motivation to Lead (MTL) instrument and (b) Triandis and Gelfand’s (1998) Culture Orientation Scale instrument  for measuring individualism and collectivism. Because Chan and Drasgow (20 01) identified individualism and ",Missing,,,46.3144754,11.0480288
42,EVALUATING FIRM’S READINESS FOR BIG DATA ADOPTION:A HIERARCHICAL DECISION MODEL,"@pdx.edu,","Big data, project management, adoption, challenges ","This paper investigates big data challenges, leading to the development of a Hierarchical Decision Model (HDM)  model that can be used by firms to evaluate readiness to adopt big data, and highlight/address probable causes of  failure before the project even starts. Hence, increasing the chances of a successful big data adoption that can deliver  value to firm and provide insights and analytics that will significantly help in addressing the problems it is built to  help solve. The mo del was evaluated by experts from the industry, and then tested against a hypothetical case, in  which Portland State University readiness to implement a big data project to address a main problem facing the  university was conducted. Finally, a discussion a bout the results of the model, experts’ evaluat ion, and case study  was offered.      Keywords  Big data, project management, adoption, challenges    Introduction  Big data is one of the leading technologies in the last few years (Cearley, 2016) . Firms use big data to support  decision making on the strategic, operational, and product levels, by leveraging on big data insights and analytics  (Barham, 2017) . A Harvard Business Review survey of the 1000 fortunate firms’ CEOs found that: 70% of the  CEOs reported that big data is of critical importance to their firms and 63% of the firms reported having Big Data in  production (Bean, 2016). However, studies indicate that more than half of big data projects fail. It either never finish  or do not generate the expected outcome. The reasons behind this high percent of failure were the subject of many  studies in the past few years (Barham, 2017; Marr, 2015; McAfee et al., 2012).   The focus of this paper is on finding out what are the main challenges facing big data adoption, and to offer  a framework that firms can use to assess their readiness against those challenges. The framework will be based on an  HDM decision model that can be used to evaluate firm’s readiness to adopt big data. Therefore, after a firm decides  to implement a big data project, they can use the model to evaluate the firm’s readiness before starting the project,  and determine what are the weak areas, within the firm, that might become hurdles to big data adoption, and  consequently, address those areas before the project even started.    Literature Review  There is no unified definition of is big data. The term big data has been used in literature to point to different things.  In most publications, the term big data points to one of three things  summarized here: Big data as an entity, which  refers to the attributes of the data itself, characterized by the 3Vs (column, velocity, variety), and other Vs added  later on (e.g. veracity, value).  Big data as a process,  which refers to the  use of information technology o acquire,  cleans, aggregate, store, and analyze big data . Finally, b ig data analytics , which refers to the use of big data to  generate insights to help making the right decision at the right time  (Barham, 2017; Ransbotham & KIRON 2 017; ","pdx.edu,",Portland State University,United States,45.5111,-122.6833
43,REDUCING STEERING GEAR HANDLING DAMAGES – THE SIXSIGMA WAY,@utc.edu,"Automotive Manufacturing, DMAIC, Sequencing Mode, Six Sigma, Steering Gear  ","The US automotive manufacturing sector, which encompasses a great spectrum of supplier –  customer networks is  one of the most important industries. A study by the Center for Automotive Research (CAR) -  an independent and  non-profit organization, indicated that this industry contributes more than 3% to the Gross Domestic Product (GDP)  and it spends between $16 to $18 billion annually in research and development. Nevertheless, neither the sector as a  whole nor do most automotive manufacturers and their tiers of suppliers are complacent free from quality concerns.  The focus of this study is to reduce quality issues arising from handling damages of steering gears. Sequencing is one  mode of line feeding of parts and subassemblies from storage to assembly line. Some literature has shown that handling  damages of steering gears is persistent problem across the manufacturing processes which use sequencing mode. The  case study selected for this study is a tier – 1 supplier of steering gear assembly to an Original Equipment Manufacturer  (OEM). This local company has been experiencing the quality concerns for years. On average about $95,000 has been  lost annually, due to part scraps and downtimes alone. Using Six Sigma’s Define, Measure, Analyze, Improve, and  Control (DMAIC) framework, root causes of steering gear damages were found, and countermeasures implemented.  Initial countermeasures indicated a significant reduction of defects. It was noticed about 80% reduction in occurrence  rate of defects, based on the data collected since July 2017, when the countermeasures were initiated, until November  2017.    Keywords  Automotive Manufacturing, DMAIC, Sequencing Mode, Six Sigma, Steering Gear     Introduction   Recently, most automotive original equipment manufacturers (OEMs) produce vehicles in Just -in-time (JIT) system  bases which is pull system that enables the overall supply chain to be efficient. Similarly, suppliers deliver parts to  OEMs on time when they are needed to avoid unnecessary inventory cost. Line feeding is internal logistics process to  ensure perpetration and delivery of parts from storage areas where parts and subassembly parts are located upstream  the assembly line to the Border of the Line. There are different line feeding modes. Line stocking is the most common  feeding mode used in automotive assembly industry. The other line feeding modes such as kitting and sequencing are  more used. However, the efficiency of them compare to line stocking is not proven yet (Sali & Sahin, 2016).   The objective of this study  was to improve handling damage of parts in automotive industry specifically to  prevent any damage of Steering gears during sequencing to board of assembly line and during assembly process. It is  mainly focused on automotive industries, especially for comp anies that use sequencing mode as part feeding system.  In the company, when a sequenced Steering Gear is found with some handling damage at assembly line, the line must  be stopped to get replacement parts. The issue gets worse if it is detected downstream after the part assembled which  could cause rework and enormous downtime that affects over all products deliver to customer.  The project was chosen ",utc.edu,University of Tennessee at Chattanooga,United States,35.0459,-85.2953
44,DESIGN OF A VISUAL BOARD FOR A MANUFACTURING CELL,@gmail.com,"Design, Manufacturing Cell, Visual Board, Visual Management, PDCA ","Inadequate flow of information leading to poor communication is an indication of an existence of waste in  communication similar to a waste of waiting in a manufacturing process. This issue remains as a real problem for  organizations including firms with ex perience in lean practices.   Therefore, designing visual management (VM)  boards embedded with a continuous improvement framework is key to ensure long- term use and a better flow of  information.  Applying the PDCA (Plan- Do-Check-Act) cycle as a design metho d as well as root cause tools to  communication problems in a manufacturing cell can be a great help as a comprehensive analysis and several iterations  for the right approach are possible and encouraged in this type of methodology. However, there is limited information  in the literature on designing VM boards using the PDCA cycle. This project addresses the use of visual management  in manufacturing by exploring the design, implementation, and use of visual boards for a production cell.   The  objective is to i mprove information flow and promote continuous improvement within cells in the manufacturing  process.  Active management involvement is recommended as key element to sustain the use of the board over  time.  The study also indicated that including the target-audience into the design process makes the continuous use of  the board an attainable goal even with little involvement from management.    Keywords  Design, Manufacturing Cell, Visual Board, Visual Management, PDCA    Introduction  In most manufacturing facilities, the term “Flow” is one of the biggest concerns, specifically regarding the material  flow and how products move through a process (Suzaki , 2012).  In today’s competitive world, information flow has  become as important as material flow since it r epresents a significant hurdle that many organizations face, whether  they are implementing lean manufacturing or not (Harris and Harris, 2008).  Managers spend up to 80% of their time  communicating and their inability to do so effectively can potentially a ffect their business’s interests (Raina, 2010).   Quite often, poor information flow in a manufacturing facility becomes a manager’s nightmare as it results in  significant absenteeism and lower quality and productivity (Raina , 2010).  Therefore, the ability  to provide the right  information to the right people and at the right time allows employees to have the tools they require to succeed.    Currently, many organizations from manufacturing and service sectors, resort to the use of visual tools to  provide information to employees ( Eaidgah, Arab -Maki, Kurczewski  & Abdekhodaee , 2016).  A visualization  approach involves posting milestones in the work area for everyone to see. It leads to higher levels of involvement  among crews and to track deadlines.   Visual Management boards are one of the many tools that enables management to develop an optimal control  over the production and therefore effectively serve all customers’ needs (Suzaki , 2012).  The use of visual boards  allows exposure to problems than can slow progress by providing management the means to take appropriate actions  and provided solutions.  By implementing VM boards through manufacturing facilities, organizations are able to ",gmail.com,,,46.3144754,11.0480288
45,A PROPOSED TAXONOMY FOR THE SYSTEMS STATISTICALENGINEERING BODY OF KNOWLEDGE,@odu.edu,"Body of Knowledge, Systems Statistical Engineering, Taxonomy ","In the ASEM-IAC 2012, Cotter (2012) identified the gaps in knowledge that statistical engineering needs to address,  explored additional gaps in knowledge not addressed in the prior works, and set forth a working definition of and body  of knowledge for statistical engineering.  In the ASEM-IAC 2015, Cotter (2015) proposed a systemic causal Bayesian  hierarchical model that addressed the knowledge gap needed  to integrate deterministic mathematical engineering   causal models within a stochastic framework.   Missing, however, is the  framework for specifying the hierarchical  qualitative systems structures  necessary and sufficient for specifying systemic causal Bayesian hierarchical models.    In the ASEM -IAC 2016, Cotter (2016) specified the modeling methodology through which statistical engineering  models could be developed, diagnosed, and applied to predict systemic mission performance.    In the last research  update, Cotter (2017)  proposed revisions to and integration of IDEF0 as the framework for developing hierarchical  qualitative systems models.  In that work, Cotter  noted that a hierarchical causal Bayesian socio -technical modeling  body of knowledge was yet to be  developed, validated, and peer reviewed .  This paper reports research  into  development of a core taxonomy for the systems statistical engineering causal Bayesian socio -technical modeling  body of knowledge.    Keywords  Body of Knowledge, Systems Statistical Engineering, Taxonomy    Introduction  Cotter (2012) documented the evolution  of statistical engineering from its conceptual origins over last 40 years and  offered its definition  as  “… the integration of statistical theory with technical, engineering, information systems,  managerial, financial, and economic knowledge to solve applied complex organizational and societal problems that  involve elements of risk or uncertainty in their outcomes.”   Cotter (2018) expanded the research term  to “systems  statistical engineering” (SSE) to recognize that complex organizational and societal problems exist only in systems of  interconnected processes .  As a research domain, however, the question arises as to what conceptually  is systems  statistical engineering?  What is its ontological foundation?  As a first step toward building the systems statistical  engineering ontological foundation, this research compiled  all prior works on statistical engineering and  Cotter’s on  systems statistical engineering and applied exploratory concept analysis to build an initial concept map taxonomy.    Concept Extraction Methodology  The corpus consisted of five seminal articles discussing the state and future of statistical thinking and first proposing  statistical engineering published in Quality Engineering 22(3) , six seminal articles debating the concept of statistical  engineering published in Quality Engineering 24(2), and five ASEM -IAC papers by Cotter (2012, 2015, 2016, 2017,  2018) laying the foundations for the systems sta tistical engineering body of knowledge.   The 2010 discussion of the  state of statistical thinking was the culmination  of two decades of research into and publication about statistical  thinking by Roger Hoerl and Ron Snee (Hoerl, et. al., 1993).  Over the two-year period from 2010 to 2012, statistical  thinking evolved into the concept of statistical engineering.  With little further development in the statistics  community, Cotter evolved the concept into systems statistical engineering with the objective of establishing a causal  Bayesian hierarchical modeling approach necessary and sufficient for complex socio -technical systems engineering.  The core SSE  concept map was use build using the definition of statistical engineering set forth by Cotter  (2012).  The concept map was refined  from review of the corpus literature  using the hierarchy of semantic relations  methodology set forth by Madsen (2001, 2002) and Nuopponen (2005, 2006).   The general core concept mapping  process flow was:  1. Establish the purpose and delimitation of the concept analysis. ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
46,SYSTEMS STATISTICAL ENGINEERING – SYSTEMS HIERARCHICALCONSTRAINT PROPAGATION,@odu.edu,"Causal Bayesian Hierarchical Models, Hierarchical Systems, Systems Statistical Engineering  ","Cotter (ASEM-IAC 2012, 2015, 2016, 2017): (1) identified the gaps in knowledge that statistical engineering needed  to address and set forth a working definition of and body of knowledge for statistical engineering ; (2)  proposed a  systemic causal Bayesian hierarchical model that addressed the knowledge gap needed to integrate deterministic  mathematical engineering  causal models within a stochastic framework ;  (3) specified the modeling methodology  through which statistical engineering models could be developed, diagnosed, and applied to predict systemic mission  performance; and (4)  proposed revisions to and  integration of IDEF0 as the framework for developing hierarchical  qualitative systems models.  In the last work, Cotter (2017) noted that a necessary dimension of the systems statistical  engineering body of knowledge is hierarchical constraint propagation to assure that imposed environmental economic,  legal, political, social, and  technical constraints are consistently decomposed to subsystems  , modules, and  components and that modules, and subsystems socio- technical constraints are mapped to systemic mission  performance..  This paper presents systems theory, constraint propagation theory, and Bayesian constrained regression  theory relevant to the problem of systemic hierarchical constraint propagation  and sets forth the theoretical basis for  their integration into the systems statistical engineering body of knowledge.    Keywords  Causal Bayesian Hierarchical Models, Hierarchical Systems, Systems Statistical Engineering     Introduction  Cotter (2015, 2016, 2017) developed the quantitative causal Bayesian hierarchal model as,    Min YTotal = f(w′(Ypred – T))          (1)  s.t.  Y = F(pai, uxi)β + F(paj, uzj)γ + ε  LBX ≤ F(pai, uxi) ≤ UBX  possibly                   LBZ ≤ F(paj, uzj) ≤ UBZ    and demonstrated the applicability of this model to hierarchical propagation of mission outcomes versus requirements  topologies to sub- system, module, and component functional requirements.  SSE  is designed to model dynamic  systems stochastic-causal functions-to-mission topologies through the integration of general and complex sys tems  governance, the cybernetics of Stafford Beer’s Viable System Model,  control theory, constraint propagation , and   causal Bayesian hierarchical regression.  The core of Systems Statistical Engineering (SSE) is to be a true systems  product and process  design and  performance improvement methodology.  As such, SSE extend s the current Six Sigma CT -drill-down improvement  methodology to a broader systems framework in which the problem and its solution effects on systemic performance  and mission outcomes are modeled holistically.  General systems theory (Ashby 1956, Bertalanffy 1968, Boulding ,  Rapoport 1986) provides the general  systemic modeling principles.  Complex systems governance (Keating 2014,  Keating and Katina 2015, 2016) specifies a reference model of the performance functions necessary to maintain  complex system viability.   SSE is designed to operate within the Metasystem s Two (M2) Information and ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
47,ADVANCING CYBERSECURITY FROM MEDIEVAL CASTLES TOSTRATEGIC DETERRENCE:A SYSTEMS APPROACH TO CYBERSECURITY,@gmail.com,"Homeland Security, Cybersecurity, Cyber Deterrence ","On a near -daily basis, the news media informs the public of the latest company to succumb  to the onslaught of a  cyberattack and having its sensitive data records breached. Although considerable research has been conducted to  improve cybersecurity from the perspective of bolstering defenses as one would do to fortify castles in the Middle  Ages, this report analyzes whether or not a strategy of deterrence can be employed to bolster cyber defense. Drawing  heavily from the defensive paradigm that resulted from the nuclear deterrence strategy of the 20th Century, this paper  investigates how the US can achieve a more long -term equilibrium solution for cyberspace through a number of key  principles inherent within deterrence. In doing so, this paper posits that cyber deterrence can be an effective strategy  for US defense in the 21st Century. By employing a systems thinking approach to this complex problem, this research  argues that there are 3 key attributes that help create effective cyber deterrence: improving international partnerships,  diversifying both cyber and non-cyber weapons capabilities, and promoting the correct international narrative on cyber  violations. This research finds that an effective cyber deterrence bolsters cybersecurity  by dis-incentivizing threats  and adversaries from perpetrating cyberattacks.    Keywords  Homeland Security, Cybersecurity, Cyber Deterrence    Introduction  Despite advances in cybersecurity, the threats in t hStee cyber realm  (or cyberspace)  continue to outpace the US  military’s ability to defend against all vulnerabilities and the growing cyber threats . With the accelerating advances  of the information age, the cyber realm is increasingly becoming a key priority for all countries today because it  connects and informs everyone across the world.  For example, t he c yber realm encompasses layers of critical  information and has ties to physical infrastructure like power grids and monitoring systems for power plants. As  technology advances, cyber will continue to play a key role not only in protecting US interests, but also in providing  for US defense in the 21st century. Currently, the size of cyberspace is infinite and it is accessible by essentially anyone.  Unfortunately, because of these characteristics, there are many vulnerabilities and threats within the system. If  breached, it can result in the disclosing of classified information as well as damage to phy sical infrastructure. In  addition, the private sector ’s utilization of cyber-tools is becoming more dominant in the military (Snow 2011). For  both of these reasons, the US Department of Defense, specifically the US military’s Central Command (CENTCOM),  has inquired whether or not it is possible to strengthen the nation’s cybersecurity posture through an effective cyber  deterrence strategy. This research finds that a key  challenge for such a cyber deterrence strategy is to strengthen and  secure the cyber env ironment while not unintentionally exploiting friendly vulnerabilities  and creating additional  cyber threats.       Research Focus   The research began with the intent to develop a framework for cyber deterrence. However, after the team’s preliminary  background research on the cyber realm and existing cyber deterrence strategies, the researchers determined that even ",gmail.com,,,46.3144754,11.0480288
48,ASEM 2018 INTERNATIONAL ANNUAL CONFERENCE PAPERBUSINESS CYCLES: A STATISTICAL PROCESS CONTROLPERSPECTIVE,@ttu.edu,"Business cycles, Statistical process control, Control charts, Change Points. ","Since the late 1920s economic recessions have been considered part of an overall business cycle analysis with  unknown, and variable, amplitude and frequency. The National Bureau of Economic Research formed a Business  Cycle Dating Committee in 1978, and sin ce then there has been a formal process of announcing the NBER  determination of a peak or trough in economic activity.   Over the year’s researchers have focused on predicting the onset of recessions as well as future values of  leading indicators of economi c performance. A lack of definition of the business cycle has led to variable  interpretations of observed economic performance and verbal gymnastics when economists describe the current state  of the economy.   A number of authors have used Markov Switching, CUSUM charts, Logistic Regression, and Support Vector  Machine algorithms to try and predict the onset of a recession. Another extensive area of research has focused on  using models to predict economic indicators. These prediction models include: Auto -regression, Factor  (Principle  Components) Augmented models, and Factor Augmented Auto-regression models.   The authors envision a future where economic analyses start with a statement as to whether the economy is  operating under common or special cause variatio n. Once this is determined analyses that are appropriate for the  current status can be applied. This paper reviews the state of the art on Business Cycles, how they are quantified  currently, define opportunities for partitioning economic indicators into sp ecial and common cause variation, and  explore the impact of this change and its effects on the research into this area.   Keywords  Business cycles, Statistical process control, Control charts, Change Points.  Introduction  Knowledge of the state of a countries current and future economic performance is critical to decisions made by its  businesses and citizens.  The drivers of economic performance are equally important to a governments ’ effort to  manage their economy.  Is the economy currently growing or in recession, what is the forecast for the future, what  measures can be taken to accelerate or stabilize growth?  Answering these questions requires : a model for the ",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
49,"MULTIPLE TEAM MEMBERSHIP, TURNOVER, AND ON-TIMEDELIVERY: EVIDENCE FROM CONSTRUCTION SERVICES",Missing,"Multiple Team Membership, Turnover, Fluid Teams, Project Management ","Firms who want to compete in dynamic markets are finding that they must build more agile operations to ensure  success. One way for a firm to increase organizational agility is to allocate employees to multiple project teams,  simultaneously - a practice known as multiple team membership (MTM). MTM allows for the potential of improved  project performance through additional flexibility and learning, however, there is also the possibility of negative  performance effects from MTM due to overwork, coordination neglect, and problems with resource blocking and  starving. In this paper we theorize about these conflicting predictions prior to building and testing an empirical  model that draws on a unique dataset consisting of 1,503 construction projects in the Europe District of the United  States Army Corps of Engineers (USACE). Although USACE is a government entity, it operates similar to for-profit  construction services companies. We find that MTM shows an inverted U-shaped relationship with on-time project  delivery whereby it is first related to improved performance and then later related to worse performance. To extend  our exploration we examine whether MTM makes teams more fragile operationally. We do this by investigating  whether teams that experience turnover are more susceptible to the negative effects of MTM. Our empirical results  support this proposition and deliver additional insight that the effect is driven by unanticipated turnover. Our  findings provide understanding into the benefits and the difficulty in building a more agile workforce.     Keywords  Multiple Team Membership, Turnover, Fluid Teams, Project Management    Introduction  Firms face dynamic and uncertain markets and so building agile project management is a key determinant of  organizational success (Fisher, 2010; Girotra, 2014). In many contexts, this need for agility has led to an increasing  use of fluid, project teams (Edmondson, 2009; Huckman, 2009; Reagans, 2005). In a fluid team, employees with  potentially diverse experiences are brought together to execute a project and then the team is broken up and  individuals move on to the next project. The constant assembling of the right talent at the right place permits  organizations to respond more nimbly than might be possible with an organizational -level response. However, a  standard model of fluid teams with individuals fully dedicated to one team (Huckman, 2011), may prove inefficient.  In many situations projects must be completed in a structured sequence and so there may be lag time between steps  or there may not be enough work at each phase of the project to ensure full utilization of the team. As a result,  organizations have responded by staffing individuals to multiple teams simultaneously, a practice known as multiple  team membership (MTM). This study defines MTM as how firms allocate employees to multiple project teams,  simultaneously. Firm usage of MTM is growing and although MTMs have received theoretical attention (O'leary,  2011), their operational implications have received little study and so it is important to understand these outcomes  from both a practical and theoretical perspective.   There are compelling reasons to expect positive and negative performance outcomes from MTM. The  deployment of MTM may aid operational performance in three ways. First, MTMs may build volum e flexibility  (Goyal, 2011; Kesavan, 2014), permitting any given team to scale its effort in response to the actual work demands.  Second, MTMs may augment individual learning since there are greater opportunities to see entire start -to-finish  project cycles (Pisano, 2001; Reagans, 2005), as well as more chances to work with others and thus learn vicariously  (Bresman, 2010). Finally, with MTM utilization, employees see a greater variety of ideas and may be able to bring  these ideas from one project to the next, thus aiding performance (Hargadon, 1997; Huckman, 2011).   Despite these potential benefits there are also compelling reasons to predict a negative relationship between  MTMs and project performance. First, when team members are engaged in multiple teams simultaneously, they may  grow overworked and their performance may suffer (Staats, 2012; Tan, 2014). Second, as individuals’ work across  many teams then coordination may suffer – resulting in coordination neglect that may lead to declines in operational ",Missing,,,46.3144754,11.0480288
50,DEVELOPING STUDENT LEARNING OUTCOME METRICS FORMAKERSPACES: A STEM PILOT COURSE,@uta.edu,"Makerspaces, Curriculum Design, Learning Outcomes, STEM Higher Education, Assembling Teams, Digital ","Preparing undergraduate engineering students with the competencies needed for future work environments is a central  objective of college engineering programs. Recently, access to 3D printers and other digital fabrication technologies  in academic makerspaces has increased opportunities for students to engage with people and tools essential for  improving their engineering and design competencies, and has led researchers to explore how to increase and measure  student learning in these spaces. The literature reveals interests in and the need for exploring how makerspaces affect  undergraduate student lear ning outcomes, but few universities are actively engaged in this type of research. The  University of Texas at Arlington (UTA) Libraries’ FabLab has endeavored to integrate their makerspace into the  undergraduate curriculum and measure the learning that tak es place when students engage in making. Toward this  goal, a list of eleven transdisciplinary makerspace competencies, each with multiple dimensions, was proposed and  tested across a diverse range of undergraduate courses between 2016-2018. This paper summarizes results of a senior- level Engineering Project Management course that participated in the program in Spring 2018. Competencies  examined in this course were assembling effective teams and demonstrating understanding of digital fabrication  processes. Homework-based interventions for both competencies were designed and integrated into a semester -long  makerspace project. Mixed -methods including pre - and post -self-assessments, project rubrics, team member  evaluations and oral presentations were used to assess and measure student learning. Preliminary results indicate that  students gain competency in assembling effective teams and demonstrating their understanding of digital fabrication  processes by completing projects in makerspaces.    Keywords  Makerspaces, Curriculum Design, Learning Outcomes, STEM Higher Education, Assembling Teams, Digital  Fabrication, Engineering, Project Management    Introduction  Preparing undergraduate engineering students with the competencies needed for future work environments is a central  objective of college engineering programs. Recently, access to 3D printers and other digital fabrication technologies  in academic makerspaces has increased opportunities for students to engage with the types of people and tools that  are increasingly essential for gaining engineering and design competencies, as well as soft -skill competencies like  teamwork, communication, ethics, and others.   Ease of access to makerspaces has led researchers to explore how to increase and measure student learning  in these spaces. Hira, Joslyn & Hynes (2014) provide a concise and current review of the history of makerspaces in  education, theoretical foundations , implications for pedagogy, and relevance of makerspaces to national science  standards. For a deeper dive, Andrews (2017)  provides an exhaustive literature review on the state of the art in  “Making Literacies,” and Rosenbaum & Hartmann (2017)  provide a meta -analysis that distills recent literature on  educational makerspaces into discrete areas of research. The literature reveals an interest in, and the need for, exploring  how makerspaces affect  student learning outcomes, but, as Rosenbaum & Hartmann are quick to point out, few  universities are actively engaged in this type of research.  As Koh & Abbas (2014) proclaim, the current literature  focuses mainly on 1) history and models of makerspa ces; 2) case studies or informal reports of how specific  makerspaces were founded; 3) advice and resources for how to start a makerspace; 4) suggested technology and sample  projects; a nd 5) i ssues related to funding, staffing and programming . Some studies on measuring impact, student  engagement and learning outcomes are beginning to emerge. The f ollowing are some examples of related research  taking place, particularly for engineering-related courses.  A University of Ottawa study reports an increase in confidence in communication and teamwork skills (80%),  engineering and problem solving skills (60%), and design skills (90%) . Most students ( 75%) reported that the ",uta.edu,University of Texas at Arlington,United States,32.728471299999995,-97.11202127009975
51,MODELING A WATER MARKET AND FOOD FREE-TRADE ASINSTRUMENTS FOR GOVERNANCE OF AGRICULTURAL WATERDEMAND IN ARID AND SEMI-ARID REGIONS,@virginia.edu,"Food Trade Regulation, Partial Equilibrium Models, Middle East and North Africa (MENA), Zayandehrud Watershed, ","  In water scarce regions, such as the Middle East and North Africa (MENA), the agricultural sector consumes most of  the freshwater resources. This makes irrigation water demand management an issue of critical concern, especially  since it involves trade-offs between water security and food security. Market features like water markets, trade policies  and land-use subsidies are common tools that policy makers use to influence. Yet these instruments can lead to water  market failures and unsustainable outcomes . Current research tends to study the impact of  water markets under  different agricultural trade regimes, including protectionism and free trade policies, especially in resource constrained  regions where the trade-offs are stark.   This study offers a partial equilibrium economic model to explain the influence of water market and food  trade policies  on the agricultur al sector and the effects on  farmers’ welfare and ecological  sustainability. The  Zayandehrud, a critical watershed in central  Iran, was chosen as a case study to test this model. The result s portray  that an intra-national water market in a closed economy can yield increased utility, though it may not lead to less  irrigation water demand. In contrast, free trade conditions coupled with water markets can decrease pressure on water  resources, offer more consumer utility and better conditions in terms of unauthorized extraction. T his research could  be instructive for sustainable economic development, strategic planning, and optimal resource allocation in semi-arid  regions. Future research will explore  the framework via a dynamic stochastic general equilibrium  (DSGE) model to  get more clear and accurate forecasts.   Keywords  Food Trade Regulation, Partial Equilibrium Models, Middle East and North Africa (MENA), Zayandehrud Watershed,  Agricultural Productivity, Irrigation Efficiency, Water Resource Management  Introduction  Due to the complexities at the intersection of food security and water security, agricultural water demand management  is a major global challenge. This is especially the case for arid and semi-arid regions, like the countries of the Middle  East and North Africa (MENA), where water resources are extremely limited and in decline due to factors like climate  change and rapid population growth. Currently, over 85% of available fresh water in this region is used for crop  irrigation, yet many nations  cannot completely satisfy the food requirements of their population (Bucknall, 2007) .  This condition put s extra pressure on both renewable and non -renewable water resources , i.e. surface and deep  groundwater, which results in the overexploitation of water resources in most MENA countries. Moreover, declining  water tables have led to high concentrations of contaminants such as heavy metals, nitrates and phosphates in aquifers.  Overexploitation of groundwater is also causing land subsidence and soil degradation which negat ively affects the  farming yield (Madani, 2014). There are many cases where this has caused irreversible damages to the ecological and  socio-economic sustainability.     The village of Ashan is quite emblematic of many arid regions in Persia and the MENA region, more broadly.  The dire consequences of groundwater overexploitation can be seen in  the dry and empty qanats , which are ancient  hand dug underground tunnels that distribute water. For centuries qanats conveyed groundwater from mountainous to  low-lying arid regions like Ashan. A single qanat network was used to provide water to all the  residents, farms and  orchards of this small village for hundreds of years. However, contemporary diesel-electric pumps allowed farmers to ",virginia.edu,"University of Virginia, Charlottesville",United States,38.0448061,-78.5166906
52,"THE PERVERSION OF CERTAINTY:  CHOICE ARCHITECTURE,DIGITAL PATERNALISM AND VIRTUAL VALIDATION IN THEATTENTION ECONOMY",Missing,"Attention Economy, digital choice architecture, attitude certainty ","The Attention Economy  applies the principle of scarcity to attention as a commodity, which  represents the limits  inherent in the “neurophysiology of perception”  and the social asset  of available time.  Within this new economy,  human decision-making is being engineered through choice architecture algorithms and digital paternalism, which  provide abridged subsets of information narrowing focus and reducing objectivity.  Personal risk assessment is based  largely on attitude certainty – the strength with which someone holds a belief.  Technologically driven generations are  immersed within the attention economy, which artificially enhances the sense of certainty through user -legitimized,  virtual validation.  Attitude certainty is now being developed through online manifestations of traditional mechanisms.   This paper explores these online manifestations focusing on how and why they have become so influential.  Beyond  the awareness of the validation paradigm shift and the obvious technological deterministic implications, management  impacts will be significant as online development of attitude certainty becomes pervasive and as a result, risk  perception and subsequent risk behavior is less pr edictable.  A brief analysis of the impact of these risks is presented  to assist engineering managers and researchers in the on-going development of these issues.    Keywords  Attention Economy, digital choice architecture, attitude certainty    Introduction  Melvin Kranzberg famously stated in the first of the Kranzberg Laws, “Technology is neither good nor bad; nor is it  neutral” (Kranzberg, 1986).  Kranzberg was not a technologi cal determinist but his law has a deterministic focus if  we interpret it to mean simply that technology is not indifferent —it cannot be ignored.  Today, information and  technology are almost synonymous; and together, the impetus of our economy.  Time and attention have now become  commodities so rare and so scarce that we have built an economy around them.  The Attention Econo my seeks to  influence information (the consumer of our time and attention ), for financial gain.  And because time and attention  are at a premium in this digital environment with a seemingly endless supply of information and innumerable  organizations seeking to control it, decisions have assumed a more immediate nature.    There is a perfect storm developing —one that threatens to alter how humans process information, make  decisions and assess risk. In the business of controlling our attention for economic viability, e-commerce sites, fueled  by big data acquisitions from Online Social Networks (OSNs) , are continually improving virtual, behavioral  engineering techniques.  The field of Recommendations Engineering is using algorithmic filtering and personalization  techniques, neural networks, choice architecture and digital paternalism to increase website dwell time, limit  objectivity and influence decisions.  The inevitability of a progressively technologically immersed society is shaping  economic structure, le gislation and resource allocation.  Internet proficiency, literacy an d fluency are becoming  necessities as economic and social inf rastructure race to provide avenues for expansion for web -enabled commerce  and communications.  Homo economicus is still a myth—humans continue to make irrational decisions without a keen  appreciation of statist ical probability.  We remain subject to heuristics that inform and affect our judgement under  uncertainty especially when de cisions are time -sensitive.  And  we have not yet escaped inherent bias which often  obscures rationality and objectivity.  Attitude certainty—the confidence we have in our beliefs; has always been ideally  developed through independent, objective psychological appraisals of information .  Now, the key tenets of certainty,  fundamental to navigating OSNs , are intrinsic to online behavior especially in digital natives.  These evolved online  behaviors coinc ide with , and enhance proven, certainty-building mechanisms and appear to result in faster ",Missing,,,46.3144754,11.0480288
53,A CASE STUDY FOR SUSTAINING A STANDARDIZED HEALTHCAREPROCEDURE,@uta.edu,"Healthcare process standardization, process sustainability, sustainability plan. ","This paper describes the sustainment plan of a project that standardizes a healthcare procedure and provides training  of this procedure to physician trainees. The procedure discussed in this paper is the insertion of central venous internal  jugular triple-lumen catheters using ultrasound guidance. This project was the first attempt to standardize a medical  procedure that has significant risks and its associated training across six different departments at the University of  Texas Southwestern Medical Center (UTSW). Multiple stakeholders were involved beyond the six departments  including representatives from UTSW’s Simulation Center and Quality, UTSW’s Safety and Outcomes Education  Department and the University of Texas at Arlington’s  (UTA) Industrial, Manufacturing, and Systems Engineering  Department. At this point in the project , the medical procedure has been standardized and the training process and  educational support materials have been developed  necessitating a sustainability plan to achieve long -term goals.  Many potential challenges exist  including continued adherence to the standardized procedure and the associated  training curriculum. The sustai nability plan provides a support framework that helps ensure the standardized  procedure, the training and the associated deliverables are  continued well into the future . The paper also presents  preliminary results from the application of a sustainability model.     Keywords  Healthcare process standardization, process sustainability, sustainability plan.    Introduction   Process standardization is gaining traction in healthcare (Leotsakos et al., 2014 ). Process standardization refers to  removing the variation in the performance of a specific process to ensure that the process is conducted in a uniform  manner (McGrath et al., 2008 ). There are multiple benefits for standardizing processes  and include improving  efficiency, reducing errors, improving process quality and consistency across users, and reducing costs. In healthcare  organizations, for clinical procedures, standardization has the added benefit  of reducing patient harm ( Leotsakos et  al., 2014). When a larger organization standardizes processes, the organization can distill the individual organization  department or unit policies and procedures into organization level processes and procedures, reducing the overall set  of documents. Process standardization in healthcare has not been as widely used as it has in other industries (Leotsakos  et al., 2014 ). However, standardization is considered an effective way to implement process improvements ( Health  Quality Ontario, 2013). ",uta.edu,University of Texas at Arlington,United States,32.728471299999995,-97.11202127009975
54,EXPLOITING ONTOLOGY- UNDERSTANDING A DOMAIN OFDISCOURSE IN ENGINEERING MANAGEMENT,@odu.edu,"Ontology, Domain of Discourse, Engineering Management.  ","When we hear the word “ontology,” the first thing comes in mind is the philosophical perspectives. Philosophical  thoughts and explanati ons hardly bring excitement to engineers who thr ive for practical applications. Do we really  need an ontological analysis to understand a domain of discourse? Especially, as an engineer, when we always rely  on the facts, proofs and experiments; how an ontological study can help us to solve a problem? Even though it is well  received and used in the field of philosophy and subsequently in the medical domains, we the engineers, have rarely  leveraged ontology and its full potential in the field of engineering. Part of the blame goes to our misunderstanding  about the term itself. The purpose of this paper is to clarify the concept of ontology and address the pressing needs of  ontological studies in the area of engineering management . The three key questions - what, why, and how - will be  answered to justify the reasoning of leveraging ontological studies. The researchers will also present two recent  ontological research in engineering management.    Keywords  Ontology, Domain of Discourse, Engineering Management.     Introduction  Ontology originally derived and then evolved from the domain of philosophy. As a philosopher, we can go above and  beyond our thoughts, often not necessarily relating it to pragmatism. Conversely, engineers thrive for practical  outcomes and app lications. Applicability and practicality thus are prominent field of concerns for engineers.  Therefore, does it ma ke any sense to utilize ontology  in the field of engineering management? Do engineering  managers or practitioners really need to understand ontological aspects of a domain of discourse? Why should a  philosophical thought provoking “ontology”  be studied to understand a domain of discourse ? Let’s say we, the  engineers, are enlightened by ontological discipline and somehow get to the point to unde rstand why we should use  it. The next question then automatically comes is how we can exploit ontology? A misunderstanding and/or little to  no knowledge about ontology resulted in not availing the potential of this hidden gem. The purpose of this paper   therefore is to decipher the importance of ontological studies/understanding by answering three basic questions- what,  why, and how; supplemented by two active research exploiting ontology in engineering management.     Understanding Ontology: What, Why, and How?  What is an Ontology?   Let’s first elaborate what an ontology is. Tracing back to philosophy, ontology derived from two Greek words- ontos,  meaning “being,” and logos, meaning “logical debate.” Just by the word itself, ontology means the understanding of  existence; knowing the meaning of beingness. Thomas Gruber (1995) defined ontology as an explicit specification of  conceptualization. This definition allows us to think deep down to understand the meaning of ontology. B y carefully  analyzing this definition, we can find three keywords of importance- explicitness, specification, and conceptualization.  Thus, ontology is not just to define a domain of discourse with a formal definition, rather going further to identify the  terms/classes/categories for that domain of discourse, meaning what terms directly or indirectly constitute the domain.  Then finding their (terms) relationships, functions, axioms, and instances depending  on the level or type of ontology  one is developing. For the sake of the paper length, t his discussion will not touch the details of various levels of  ontology, however will point out  supportive evidences  on why ontological studies are important for engineers,  especially for engineering management.  ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
55,MANUFACTURABILITY AND THE PRODUCT DESIGN:  CASE STUDYEXPLORATION OF MANUFACTURABILITY ASSESSMENTS ACROSSA PRODUCT LIFE CYCLE,@cavse.msstate.edu,"Manufacturability, Assessment, Metric, Life Cycle, Taxonomy, Design for Manufacturability, Product Design ","The challenge in the workplace with delivering high quality products to market in a timely manner has become  increasingly more intense due to changing customer demands and expectations.  The importance of incorporating  manufacturing input earlier in the design cycle has evolved into a more common place concurrent engineering practice  in order to minimize risk to performance, cost, quality, and schedule.  However, there still remains the challenge of  how to evaluate significant numbers of design alternatives during early stages of the life cycle acquisition process.   There is continued interest by the Department of Defense (DoD) on the assessment of manufacturability a s  part of the Engineered Resilient System (ERS) effort as they develop new military systems.  Prior research has led to  the development of a Manufacturability Assessment Knowledge -based Evaluation (MAKE) which is a methodology  developed to assess the manufacturability of a product at various stages of a product’s li fe cycle.  At the foundation  of MAKE is a taxonomy based on a matrix-style assessment designed to capture the impact of design decisions on the  manufacturability of a product. This research paper discusses the development of this matrix, and its evolution and  application in the manufacturability assessment process.  Three case studies will be used to demonstrate the application  of the taxonomy at several different points in a product’s life cycle, as the research moves “to the left” toward an early  concept phase where design fidelity is extremely limited.    Keywords  Manufacturability, Assessment, Metric, Life Cycle, Taxonomy, Design for Manufacturability, Product Design    Introduction   Development of products for both industrial and government applications is fraught with challenges.  One of those  challenges involves the ability to identify manufacturing issues early in the product life  cycle prior to finalization of  a design that may have inherent concerns that negatively impact the cost of the product over its life cycle.  This  challenge is of particular interest to the DoD, where investment is high and product life cycle can extend over a period  of decades.  Understanding of how to mitigate the manufacturability risk early in the design development is key to the  Engineered Resilient Syste m (ERS) Tradespace efforts involving research of the “ilities”  and development of  predictive methodologies, algorithms, or models to understand how these “ilities” impact the overall life cycle cost   and contribute to the resiliency of combat systems.   The traditional approach to design for manufacturability (DfM) revolves around a few central points.  The  need to design products with the manufacturing environment in mind, resulting in products with low manufacturing  cost, high quality and that meet the cust omer demands.  To this end, there is an abundance of DfM information  available to design engineers in the form of guidelines, checklists, data sheets, software programs, etc . (Bralla, 1996;   Anderson, 2014; Boothroyd & Dewhurst , 1994).  All of which are focused on providing design engin eers with the  best practices or rules-of-thumb necessary to design parts and assemblies with a focus on manufacturing.  Rather than  design engineers basing their decisions on traditional form, fit and function, DfM adds ano ther layer of complexity  for trade off analyses associated with the design.  Most of the information available in the DfM community is focused on the DfM analysis of individual parts  and assemblies by identifying characteristics of the parts or assemblies t hat may cause issues during the intended  manufacturing process (i.e. machining, casting, welding, assembly, etc.).  In most cases it does not extend to other  functional areas of manufacturing that have impact to manufacturing cost, such as the labor involved with training for  new or specialized processes, environmental, health and safety (EHS) concerns that may drive the need for special  equipment or training (McCall, et al., 2018).   ",cavse.msstate.edu,Mississippi State University,United States,37.9537,-91.7756
56,IDENTIFYING VULNERABLE STAKEHOLDERS FROM DEPENDENTRELATIONSHIPS IN CALIFORNIA’S WATER SYSTEM,@virginia.edu,"Stakeholder Analysis, California Water Resource Management, Matrix Analysis ","Years of drought in California have led to degradation of water quality, declines in surface and groundwater reserves  and land subsidence (CA Water Science Center, 2018). As this system becomes increasingly strained and expands, so  does the complexity of stakeholder interactions. Each stakeholder has a value system use d to guide their choices. A  network of stakeholders’ relationships is a compilation of interdependencies among stakeholders. Assessing the role  of a single stakeholder in the system can be measured by the stakeholder’s power in the system and dependency on   the system. The methodological approach in this case study maps California’s stakeholders, ranked relative to other  stakeholders, to create a power-dependency graph. The purpose of a stakeholder analysis is to identify both vulnerable  and powerful stakeho lders. California’s farming industry emerged as a vulnerable stakeholder. Recognizing this  vulnerability can incentivize the farming industry to increase their power through political lobbying. Alternatively,  powerful stakeholders, such as the State Water Board, can better protect vulnerable parties now that their position  within the system is identified. Evaluating stakeholders’ preferences leads to improved decision making and system  designs (Bhatia, 2017). This paper expands that assertion to reach roles  and relationships of stakeholders within the  system.    Keywords  Stakeholder Analysis, California Water Resource Management, Matrix Analysis    Introduction  California’s Water Systems  California has a population of thirty-nine and a half million residents. The natural resources of the state are threatened  by the impending consequences of climate change, which are theorized to exacerbate already strained water resource  concerns (Meisen, 2011).  Anthropogenic climate change is projected to increase the frequency and severity of  droughts, which has dire implications for the large agricultural economy in California ( Diffenbaugh, 2015). Indeed,  agriculture has shaped the political climate  in California, which  is reflected in the fact that its agricultural industry  produces highly water intensive crops such as fruits and nuts, resulting in irrigation being the biggest consumer of  water resources in the state (Johnson, 2015).  Water in California comes from the state’s surface and groundwater reserves  as well as  watersheds that  extend across eight states (The Nature Conservancy, 2012 The result is that water resources management in the state  of California has a large, complex group of stakeholders. There are users that are dependent on the consistency and/or  quality (depending on usage purposes) of the water supply. Groundwater and surface water are often regarded as two  entities; however, their codependent ecosystem structure is strained during the abuse of either (Winter, 1998). Despite  environmental codependence, the collection and treatment of water is drastically different for each source. Water  resources are preserved through nongovernmental organizations (NGOs) and government programs and laws. The  treatment and delivery of surface water includes water treatment facilities and water distributors as stakeholders. Water  collected after use is sent to wastewater treatment facilities for treatment and returned to nature. All parties that interact  with water are subject to regulations and standards set by state and federal governance.  ",virginia.edu,"University of Virginia, Charlottesville",United States,38.0448061,-78.5166906
57,INFLUENCE OF REWARD SYSTEMS ON MOTIVATION-PROS ANDCONS BASED ON CURRENT LITERATURE,@odu.edu,"Motivation, reward systems, performance ","Motivation of an employee is both intrinsic and extrinsic.   Every worker's compensation is linked in some way to  their performance.  Intrinsic motivation is more responsive  to less quantifiable work environment factors such as  making tasks that stimulate our intellect and creativity.  A similar factor involves giving the workers more autonomy  in their efforts as seen in participative management initiatives.  A limitation of money as a reward is that it can lower  intrinsic motivation in some cases (Deci, 1973).  Today’s workers are more likely to have jobs that require  creativity, initiative and “thinking outside the box”.   Similarly, companies that want to bring these qualit ies out of  their employees need to evaluate their reward systems.  Pink (2009) posited that there is a disconnect between what  science knows about motivation an d what businesses typically  do to motivate their employees and he suggests  paying a fair wage and using non-monetary approaches to improve workers intrinsic motivation.  Scientific research  shows extrinsic rewards have a neutral or negative effect when intrinsic motivation is high (Arnold, 1976).  Do  workers really focus on their income when excretin g effort on behalf of their company?   Where does intrinsic  motivation come from?   Do some extrinsic reward s decrease intrinsic motivation?  Are non -financial extrinsic  rewards more effective?   What controversies exist in the research area of motivation?     Are current managers aware  of what science says about rewards and motivation? These questions are discussed in this paper  based on the current  literature.    Keywords  Motivation, reward systems, performance    Introduction  Reward systems are made up of two  components.   The first component consists of extrinsic rewards such as salary,  benefits and incentive payments.  The second component is made up of intrinsic rewards such as self -satisfaction,  recognition and other rewards given to employees that are not  monetary based.   Both of these reward components  are commonly used for increasing the motivation and performance of the employee.  From the employees’ vantage  point, motivation of the employee could be two ways as well; intrinsic and extrinsic.   Every worker's compensation  is linked in some way to their performance.  Intrinsic motivation is more responsive to less quantifiable work  environment factors such as making tasks that stimulate our intellect and creativity.  A similar factor involves giving  the worker more autonomy in their efforts as seen in participative management initiatives.  A limitation of money as  a reward is that it can lower intrinsic motivation in some cases (Deci, 1973).   An anecdotal story is often used to emphasis how extrinsic rew ards can reduce intrinsic motivation.  In this  story, some children start to play a ball game in a man's yard.  The man does not want them in his yard and devises a  plan.  To the children's surprise, he offers to pay them for each game they play.  They are  delighted to play their  game and get paid for it.  After a few weeks, the man stops paying them.  The children announce that without  payment, they will not play anymore on his property.  Their focus was altered by the extrinsic reward of money.   The clever man achieved his goal of getting the kids to stop playing in his yard (Mawhinney, 1990).   In 1995, one of the  today’s leading technology company set out to publish an encyclopedia  on CD discs  that would be vast in its scope and linked to an online versio n.   They called this product Encarta.  Thousands of  professional writers were paid to develop rich content for this ambitious project.   Years later, an online  encyclopedia called Wikipedia would be developed that had articles written by thousands of people for free.  No one  was paid for content.  Although the usefulness of Wikipedia as a resource is debatable, there is no debate on the ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
58,(BLOCKCHAIN CONCEPT),@ou.edu,"Blockchain technology, Decentralized systems, Centralized systems, Multi-Criteria Decision making ","Blockchain technology is an emerging technology which is known to enhance the security and interoperability of any  system while it needs more powerful infrastructure and cost of maintenance inside the organization. In this project,  we are trying to apply A HP method of decision -making to analyze whether decentralization of the IT system inside  an organization is a good decision or staying with the centralized system is better. Based on the uncertain consequence  of the adoption of the blockchain technology in  the competitive market, businesses should have a robust decision  framework. The framework will clarify the utility, which is gained from complete adoption, partial adoption, or  withdraws the adoption of blockchain technology.     Keywords  Blockchain technology, Decentralized systems, Centralized systems, Multi-Criteria Decision making    Introduction  Demand for decentralization of the organizational processes is strong throug hout the world (Prud'homme, 1995) .  However, depending on the structure of the organization and the entity of the processes inside the organization, the  need and the efficiency of the decentralization may differ. For both centralized and decentralized IT systems, there  are some advantages and disadvantages. M easuring the level of benefits that decentralization will have for a specific  organization will help the managers to decide whether they should keep their system centralization or not (Zdravkovic,  Rychkova, & Speckert, 2014).  One way of implementing a secure decentralization is to take advantages of the Blockchain technology. It is justified  that Blockchain technology will enhance the security and interoperab ility of the system. lack of intermediary parties  has made blockchain an inspiring database structure for the security (Wall & Malm, 2016). However, the establishment  of blockchain technology as a decentralized  system in enterprises  needs more powerful infrastructure and cost of  maintenance inside the organization, since instead of central databases we will have several databas es. On the other  hand, it will diminish the most part of central authority power as everybody in the system can make changes in the  system. However, the consensus concept and fast data transaction will increase the interoperability of the system and  therefore it may increase the efficiency of the system. Still, decentralization of the system has some bad effect s such  as increased disparity, decreased stability and the efficiency and in some cases the corruption use cases.  It can be like  a drug or medicine which although it has some beneficial effect to solve some challenges, it can have some side effects  as well (Prud'homme, 1995).  In this paper, a decision framework consisting of important factors for decentralization and centralization is developed.  The factors are chosen based on the advantages and disadvantages of decentralization and centralization. After  developing a hierarchy model for the problem, we implement the model for a dataset gathered through questionnaire  from researchers of blockchain technology, using the Super Decision software and try to validate the result based on  the literature review.         ",ou.edu,University of Oklahoma,United States,35.1959878,-97.44570827599858
59,,@iastate.edu,"Acquisition, DoD, Systems Engineering, Complex Systems, Cost Overruns  ","Defense and Aerospace Systems Acquisition projects, just like any other Large -Scale Complex Engineered Systems  (LSCES) experience delays and cost overru n during the acquisition process. Cost overrun and delays in  LSCES are  due, in part, to high complexity, size of the project, involvement of various stakeholders, organizations, political  disruptions, changes in requirements and scope. These uncertainties,  due to the exogenous factors,  have cost the  federal government billions of dollars and delays in completion of the programs. Cost estimation of federal programs  is usually based on previous generations of systems produced and almost all the time the costs  are underestimated.  Underestimation of the cost of the programs is an endogenous factor, which results in cost overrun for any program,  the behavior of the cost escalation is pre -forecasted to be norma lly distributed, but due to the cost overrun, the cost   escalation curve may be skewed. In this paper, the authors will be studying the cost escalation and time delays of  the  Advanced Extremely High Frequency (AEHF) , a DoD’s space acquisition program. The distribution of the cost and  time can aid in understanding the effects of endogenous factors influencing the cost overrun and the effect of change  in requirements during  the acquisition process. This data will serve as  a foundation for further research to create a  framework, which will be used, in better forecasting of the cost of the acquisition of the programs.    Keywords  Acquisition, DoD, Systems Engineering, Complex Systems, Cost Overruns     Introduction  A system, which in complex in nature with many stakeholders, interacting and coupled system is a Large-Scale  Complex Engineered System (LSCES) (Deshmukh & Collopy, 2010) . These systems are associated with high cost  and high risk due the complexity and numerous interactions with people spanning across the world working in  numerous geographic locations and organizations (Lewis & Collopy, 2012; Shapiro & Lorenz, 2000). One such system  is the Defense weapon acquisition system or Major Defense Acquisition Programs (MDAP). There are various  complexities in defense system such as technological complexity, managerial or organizational complexity, business  system complexity, coupling of the systems, cognitive complexity , number of parts, lines of code and many more  (Bloebaum, Collopy, & Hazelrigg, 2012) (Spero, Bloebaum, German, Pyster, & Ross, 2014).   The defense acquisition programs incl ude weapons, aircrafts, ships, space acquisitions and so on.  It is so  often seen that such highly complex systems have higher costs and schedule overrun exceeding to more than 40% of  their initial costs (Deshmukh & Collopy, 2010). For example, a comparison of complexity, in terms of number of parts  and lines of code, with the schedule time in Exhibit 1 by former director of the DARPA Tactical  technology office  Paul Eremenko shows the escalation of schedule for aerospace industry (Eremenko, 2009).  The exhibit also shows  the escalation comparison of the aerospace industry  with the automobile and integrated circuits industry. The  aerospace industry’s cost increases by 8-12% every year whereas the automobile industry and the integrated circuits  industry’s costs increase by 4% and 0.1% respectively. The increase of cost in the aerospace industry every year causes  the cost to grow at least twice the estimated  costs by the end of the program due to longer schedules  and high  complexity of the system produced (Eremenko, 2009).  ",iastate.edu,Iowa State University,United States,42.0279608,-93.64473746093857
60,MANAGING CONFLICT TOWARDS IMPROVING TEAMPERFORMANCE-WHAT WE KNOW SO FAR,@odu.edu,"Team performance, conflict, trust, management ","Team conflict is a state of disagreement amongst people collaborating in gr oups, working together; caused  by either  actual or perceived opposition towards each other or an action they intended.   Conflict takes various forms, when it  comes to teams there are three established types of conflicts: task, relationship (personal) and process conflic ts  (Thompson, 2011).  A team which carries out a particular task may have a disagreement regarding the content of the  task, which could include conflicting viewpoints, opinions and perspectives.   Task conflict occurs when team  members disagree over ideas and opinions abou t the task being performed (Jehn, 1995). In contrast, relationship  conflict refers to disagreements based on the personal issues amongst the team members unrelated to the task at  hand.  Process conflict involves disagreement about delegation and logistical  matters and are conceptually different  from task and relationship conflict.   Over time, emergent states develop such as trust, respect and cohesiveness and  these may dull the effects of conflict types on the team’s viability and group outcomes.   The liter ature on team  conflict indicates it can be healthy or counterproductive in completing teams’ assignments.   Teams with no conflict  may not explore all the alternat ives to complete the job, secondary to reduced brainstorming and the challenging of   ideas.  Managers must orchestrate a delicate balance with the teams working for them to encourage the right mix of  conflict and cohesion to ensure that best ideas get heard while ensuring that one or two disagreeable team members  do not suffocate the team with pointless arguments.    Keywords  Team performance, conflict, trust, management    Introduction  Differences in interests, perceptions, information  and preferences are unavoidable, especially in teams that work  together closely for a lo ng periods of time.   This causes conflict (Thompson, 2011).  Earlier studies in this subject  point out that conflict takes various forms.  A team carrying out a particular task may have disagreement regarding  the content of the task which could include conflicting viewpoints , opinions, and perspectives (task conflict).  In  contrast, relationship conflict includes personal issues amongst the team members unrelated to the task at hand  because of different working styles and personalities (Jehn 1995).  Process conflicts involve disagreemen t about  delegation and logistical matters and is conceptually different from ta sk and relationship conflict (Jehn, Grees,  Levine,& Szulanski, 2008).  Task conflicts occur when team members disagree over ideas and opinions about the  task being performed such as the appropriate information to include in an annual report.  Over time, emergent states  develop such as trust, respect , and cohesiveness.  These may dull the effects o f conflict types on the team’s viability  and group outcomes  (Jehn, 1995).   Is conflict always bad?  Literature on team conflict indicates it can be healthy or counterproductive in  completing a team’s assignment.   Task conflict does not support the status quo and seeks alternative ideas.   Therefore it promotes and triggers creativ ity and creative thinking amongst team members  and gives birth to new  ideas for reaching the end goal by solving the problems  (Gilson, & Shelly, 2004 , West 2002,  Tjosvold, 1985 ).   Teams with no conflict may  have not  considered all the altern atives possibl e to get the job done  and in this  environment, innovation is unlikely.  Some teams have members who manufacture conflicts for no particular reason  and cause the team to work slower and often  results in less than optimal results.   Managers must orchestrat e a   delicate balance within the team under them to encourage the right mix of conflict and cohesion to ensure the best  ideas get heard while ensuring that one or two “disagreeable” team members do not suffocate the team with  pointless arguments (Behfar  and Thompson, 2007).   Later studies by De Dreu and Weingart(2003) failed to find ",odu.edu,Old Dominion University,United States,36.8862699,-76.30972478839735
61,INTEGRATED PRODUCT DEVELOPMENT AND INDUSTRY 4.0: A CASE STUDY INAN AUTOMOTIVE COMPANY,@pupr.edu.br,"Product Management Process, Industry 4.0 and Integrated Product Development. ","New technologi es to transfer and process information have promoted connections from different systems and  languages. This technological evolution, combined with interactivity, has played a significant role in organizational  environments. The gap is to investigate whethe r computerized solutions, supported by the 4.0 industry's innovative  features, serve to reduce barriers to effective collaboration and integration between product and process engineering.  The proposal is to apply the methodology in a case study in the Department of Process Engineering at an automotive  manufacturer. The findings are that technologies, such as cloud computing, can be a key to combine integrated  management with project deliverables in a powerful way. The availability of a common environment th at includes  analysis of needs, technical and economic feasibility, parameterizations, product and process simulations until final  implementation, can transform the engineering of products and processes. The contribution of this research is to review  the topic of integrated product development, identifying which computerized solutions can faster communication,  achieving an effective association between deliveries and product development. In conclusion, new technologies allow  more transparency and support for business decisions. Finally, the steps, once isolated in a serial development process,  are integrated and positively impact the product life cycle and cost savings.    Keywords  Product Management Process, Industry 4.0 and Integrated Product Development.    Introduction  Information are produced and available in enormous quantity and speed, but not always in the quality and time  expected. New technological resources, such as cloud computing and social media, have transformed the transfer and  processing of information. They are now also within corporate environments. Systems are being adapted to fit  corporate languages and to support the decision-making process to make them more efficient.  The development of integrated products advanced greatly in the 1990s  (FACHINELLO and DA CUNHA,  2004). However, the high cost, capacity and size of the files were limiting factors , according with Zancul and  Resenfeld (2008) . Revisiting this topic under the light of new technologies is important to understand how these  constraints have been addressed and to assess if integration and collaboration of stakeholders are better addressed.   Digital evolution has provided solutions and changed the way people treat different situations (HERMANN,  PENTEK and OTTO, 2015). At the level of creating a false sense that everything can be solved through technology.  For an innovation to be considered as an alternative in a given condition it needs to be tested before, but it requires  planning so that it can be replicated under known conditions in other places.  In a manufacturing environment this is a reality. There is a lot of internal and external information, many  innovative features and a need to act quickly to promote new products, all to keep the business of the organization at  a high level of competitiveness. From this point of view, an automotive industry wants to investigate the opportunity  to improve project management processes. The objective is to identify the current way of project management and  experiment a computerized system that promot es greater collaboration and integration between systems and  stakeholders in the projects of the final assembly engineering sector of this organization. The question is how to make  integration and collaboration between systems and stakeholders effective th rough innovation resources from 4.0  industry.  In this way, the initial step is to verify the current pattern of project management in this sector of the  organization, which documents, procedures and systems are used in daily practice. Afterwards, assess th e level of  adherence of this scenario to the new digital resources and confront it if it responds to the managers' expectations.  ",pupr.edu.br,Polytechnic University of Puerto Rico,United States,18.4223,-66.0559
62,MUNITION DEMILITARIZATION: MAINTENANCE PROGRAMIMPROVEMENT,@usma.edu;,"Systems Decision Process, Value Focused Thinking, Modeling, Demilitarization, Maintenance Plan ","As US munitions stockpiles grow, the importance of rendering these munitions inactive and secure from a human and  environmental perspective creates tremendous and important challenges.  Certain steps must be taken to address the  final lifecycle stage of th ese munitions and make them safe.   Currently the Army uses incineration as the principle  means by which to demilitarize these munition stockpiles.  The APOE 1236 incinerator is the primary means used by  Program Executive Officer for Ammunition (PEO -Ammo) to demilitarize munitions and which are currently located  at four facilities in the US and one in Japan.  PEO -Ammo has a challenge with consistent throughput of munitions  which results in higher costs and ultimately greater munition stockpiles.  The primary  source of the inconsistent  throughput is the maintenance program of the APOE 1236.  This project conducted a systems analysis on the current  APOE 1235 maintenance program and developed an efficient and effective preventive maintenance program.  The  systems analysis provided data which showed an inconsistent and distinctly different maintenance programs for every  APOE 1236 location.  The resulting preventative maintenance program unified the five locations into one  comprehensive program which is efficient, accurate and automated.  The PEO-Ammo demilitarization program is now  able to control the downtime of the APOE 1236 which provides for increased reliability and throughput of munitions.    Keywords  Systems Decision Process, Value Focused Thinking, Modeling, Demilitarization, Maintenance Plan    Introduction  Demilitarization is a complex and critical stage in the life -cycle of munitions. The U.S. Army faces a significant  challenge in addressing its demilitarization needs. Demilitarization includes the complete r ange of processes that  renders weapons, ammunition, and explosives unfit for their originally intended purposes. An array of demilitarization  methods exist to address munition stockpiles; however, the most industrialized demilitarization method is the use of  incineration. Open Burning and Detonation (OB/OD) is the first generation method for demilitarization and is still  used today as a supplement to incineration. Unlike OB/OD, incineration provides a controlled environment for  destroying and disposing of munitions while enabling operators to monitor pollutant emissions regulated by the federal  government. While incineration provides an effective alternative to OB/OD, Program Executive Office – Ammunition  (PEO-Ammo) must address the unique challenges that accompany the maintenance of low density equipment. The  Joint Munitions Command (JMC) under PEO -Ammo operates the APE 1236, which is one of JMC’s primary pieces  of incineration equipment. Despite the high volume of munitions processed through the incinerator s, only a handful  exist and are currently in operation. In order to address the growing munition stockpiles, these few incinerators must  be effectively maintained to support operations and the needs of the army.   The focus of this study  is to assess the ma intenance program currently in place at JMC depots today and  develop a new program which uses analysis to facilitate an effective preventative maintenance program. JMC currently  operates on an inconsistence response maintenance program which does not effectively address its maintenance needs.  The APE 1236 requires a preventative maintenance program based on data analysis in order to provide sustainable,  long-term support for the APE 1236 and to facilitate more efficient operations in the future. Achieving t hese goals  comes with some difficulties and limitations. First, the current structure of JMC’s operations do not include adequate ",usma.edu;,United States Military Academy West Point,United States,41.3915,-73.956
63,CRITICAL SUCCESS FACTORS for the IMPLEMNTATION ofRELIABILITY CENTRED MAINTENANCE,@gmail.com,,"Power utilities are faced with a challenge of sustaining the continuity of supply by reducing the frequency and  durations of power interruptions and outages. There is therefore an increasing responsibility to meet the required levels  of reliability of supply to secure current customers and to attract new ones . Reliability Centred Maintenance (RCM)  has been implemented by various industries to promptly identify and mitigate equipment failure with the aim of  improving overall system reliability. Studies have been conducted on the implementation and application of the RCM  program with emphasis on the analysis process steps. This study focuses on the critical success factors to be considered  for a successful implementation of the RCM program and the impact of a successful RCM program on performance  indicators.   This paper explores multiple published research case studies  to identify best practices  in implementing a successful  RCM program and the benefits of the RCM  to the organization. The reviewing of multiple case studies provided an  in-depth knowledge on various approaches towards the RCM implementation by various organizations as well as the  results attained.   The results indicate  five critical success factors for  a successful implementation of the RCM program  namely:  stakeholder support, training, RCM team competency, culture change  and the use of a pilot study . These factors can  be adopted by the organization as a strategic guideline to promote the success of the RCM implementation. The results  further prove that the RCM program is a cost-effective maintenance strategy that can offer an improved and efficient  plant performance.    Key Words  Reliability Centred Maintenance, Critical Success Factors, Reliability Centred Maintenance Impact.  Introduction  Power Utility X is a South Africa’s company which serves to generate, transmit and distribute electricity to customers  in the industrial, mining, commercial, agricultural and residential sectors and to redistributor s (Thwaits & C, 2013) .  The utility  is facing several  challenges including a decline or stagnant demand of supply from Key Industrial  Customers (KIC’s) due to a reduction in output or closure of plant as a result of a difficult economic environmen t.  KIC’s comprise approximately 38% of local sales and  33% of local revenue and  therefore the utility has a  responsibility to ensuring the reliability and availability of power supply in order to secure the current revenue from  the KIC’s and to support the economic growth of the country.  Reliability improvement is increasingly becoming one of the most crucial key performance measures  amongst various global leading utilities. Reliability Centered Maintenance (RCM) is an analytic technique that has  been applied by various prominent organisat ions where reliability and availability improvement is  paramount  (Walliman, 2011) (Campbell, Jardine, & McGlynn, 2016).  RCM focuses on the condition of the equipment and also  considers the reliability analysis activities such as failure mode, effect and criticality analysis (FMECA) and fault tree  analysis(FTA) which focuses on the identification and classification of equipment failures  and also serves as inputs  into the RCM process (Nicholls, Lein, & McGibbon, 2011) . Various researchers around the world have investigated  the implementation and application of the RCM strategy in a quest to understand the contributing factors towards both ",gmail.com,,,46.3144754,11.0480288
64,M,Missing,,,Missing,,,46.3144754,11.0480288
65,M,Missing,,,Missing,,,46.3144754,11.0480288
66,"COST OPTIMIZATION DECISION-SUPPORT BASED ON FUZZY LOGICAPPLICATIONS, ADVANCING INDUSTRY 4.0",@gmail.com,"Business processes, Cost optimization elements, Decision-making framework, Fuzzy logic system, Industry 4.0. ","Corporate functions of large multinationals are globally executed based on business processes. Numerous business  process variables have impacts on the execution of b usiness functions. A representative subset of business process  variables, specifically cost associated, is explored in this research. Cost optimization in the execution of business  processes is essential in ensuring corporate sustainability and competitive ness. To develop and implement a cost  optimization framework, global best practice cost optimization levers are explored. This research demonstrates the  prospects of developing a cost optimization decision -support paradigm based on the fuzzy logic system. Decision - support is an intricate aspect of any business unit. The decision -support framework relative to optimizing cost is  explored based on selected cost optimization levers aligned with benchmark fuzzy sets. The fuzzy logic technique is  efficient for optimization of business process variables offering effective resources when investigating variables that  are not precise. A framework supporting uncertainty and vagueness of business process variables are considered. The  research delivers a flexible thin sl ice decision-making framework of the solution with testing of cost optimization  levers on business processes. This presents an overview of benchmark measures supporting enterprise practitioners  relative to developing a cost optimization paradigm as an enhancement to the current design. An assessment  framework effective in substantiating relationships between cost optimization levers, business processes, and  corporate performance is developed.   Keywords  Business processes, Cost optimization elements, Decision-making framework, Fuzzy logic system, Industry 4.0.    Introduction  The dynamic nature of corporate operations has resulted in business entities seeking optimal measures towards   ensuring business competitiveness  and performance. Enterprise functions are hi ghly dependent on the execution of  business processes ( Becker, et.al, 2013). Several business process variables inclusive of cost which impacts on  business functions exist (Medoh & Telukdarie, 2017). Optimizing these business variables is a prerequisite to   attaining corporate sustainability. This research focu ses on exploring cost as a business process variable impacting  on business processes. Literature review indicates cost as an essential business process variable with  consequential  impacts on business p rocess optimization  (KPMG, 2008). Developing a decision -support Industry 4.0 model  for  optimizing cost without compromising  on quality is an essential strategic decision for every large multinational to  ensure global sustainability and competitiveness (Man deep, et.al, 2015). Developing a cost optimization decision - support framework can be conducted by considering a number of cost optimization elements based on distinct  business objectives.  Selected cost optimization elements are ranked, observed and a concl usion inferred based on  fuzzy logic techniques. The developed decision -support framework and fuzzy logic approach deployed are efficient  and significant for enterprise practitioners relative to optimizing cost.   Corporate entities are challenged when exploring measures to reduce operation cost in a sustainable manner  (KPMG, 2008).  This research aims to present a document for exploring cost optimization elements  aligned with  performance indicators on corporate functions based on fuzzy logic methodologies.  The fuzzy logic techniques have  been effectively adapted across several disciplines including business management, Engineering, structural designs,  information systems, structural modelling, optimization, military establishments, process , and automation cont rol  (Pamučar, et.al., 2011 and Adam, 2003).  The research is initiated by firstly  identifying and  defining best practice cost optimization benchmark  elements influencing the execution of business processes. The research identifies and reviews four best practice cost  optimization elements imperative in optimiz ing the execution of business processes. These are a combination  of  “Operational efficiency”, “Working capital management”, “Procurement” and “Supply chain” (Li, et.al., 2014; ",gmail.com,,,46.3144754,11.0480288
67,REVISITING DEMOLITION WASTE MANAGEMENT,@uaeu.ac.ae,"Construction Demolition, Waste Management, Recycled Concrete Aggregate.   ","Waste arising from the construction, repair, maintenance and demolition of buildings, structures and roadways is  generally termed as Construction and Demolition waste or CDW.  A large quantity of demolition waste is produced  every year and this creates a ma jor environmental concern. On the other hand an even larger quantity of concrete is  produced every year and this consumes a lot of natural aggregates. This creates a concern to preserve natural  aggregates. Recycling concrete and masonry waste into recycled  concrete aggregate (RCA) and using it in new  construction is a natural way of addressing both these concerns. But existing practice uses RCA only as the base  material which is low in value. This is due to the fact that the quality of the concrete debris, the raw material for the  current recycling process, is of varied quality. This is a Technology Management problem that needs development of  better technology. After reviewing the literature this paper proposes a process where the concrete debris is analyse d  using spot samples and the raw material is categorised before the production process begins. The categorisation is  confirmed with continuous sampling during production. This evidence based categorisation of the RCA produced,  permits the RCA to be used for structural and non-structural concrete with confidence. The paper proposes the findings  from the literature and the proposed process in detail.    Keywords  Construction Demolition, Waste Management, Recycled Concrete Aggregate.      Introduction  Waste can be described as materials that are not prime products, for which the generator has no further use  and has to discard. Waste arising from the construction, repair, maintenance and demolition of buildings, structures  and roadways is generally termed as Constru ction and Demolition waste or CDW (Wang et al., 2016). According to  the European Commission, the amount of CDW generated is estimated as about 25 -30% of the total waste in the  Europian Union (EU) countries ( Ferriz-Papi and Thomas, 2017) . The main reasons f or the generation of CDW are  changes of purpose, structural deterioration, rearrangement of a city, expansion of traffic directions and increasing  traffic load, natural disasters (e.g., earthquake, fire, and flood), and wars. Huge deposits of CDW are creat ed and  consequently they become a special problem of environment pollution. The most common method of managing this  material has been through its disposal in landfills. While on the one hand concrete and masonry waste form a  substantial portion of CDW, rapidly increasing production and utilization of concrete results in increased consumption  of natural aggregate, on the other. By the year 2020, production is expected to increase to more than 2.5 billion tons  per year and this situation leads to a question a bout the preservation of natural aggregates sources ( Malešev et al.,  2010). Developed nations understood the problem and are taking steps to recycle and use concrete and masonry waste.  As a mandatory regulation, the EU requires all member states to achieve at least 70% of non-hazardous CDW recovery  according to European Waste Framework Directive ( European Parliament, 2008 ). The UK, for example, achieved  89.9% in 2014 (DEFRA statistics) and already complies with this mandatory regulation of the EU.   Recovery and reuse of CDW can be in two forms: backfilling and recycling. Backfilling applications use the  recovered concrete as the base material for roads and for flooring in buildings. It is considered downcycling because  material application is of low value. On the other hand, in recycling, the CDW is transformed into a new material for ",uaeu.ac.ae,United Arab Emirates University,United Arab Emirates,24.19718435,55.680583440887546
68,M,Missing,,,Missing,,,46.3144754,11.0480288
69,REDUCING UNPLANNED BOILER TUBE FAILURES IN COAL FIREDPOWER PLANTS,@uj.ac.za,"Boiler tube failures, Thermal power plant, Generation fleet. ","Power plants are a complex and expensive asset to acquire and maintain. The corrective and preventive maintenance  costs of plants are significant. Enterprise practitioners seek ideal best  practice measures towards reducing escalating  maintenance costs and power plant downtime. The cost of corrective maintenance, due downtime, relative to power  generation is exorbitant. The result is a loss of electricity generation capacity. Numerous studies present literature  exploring measures towards enhancing the availability and reliability of electricity generating plants. This paper  focuses on reducing downtime in electricity generation plants, by reducing failures associated with boiler tubes. Best  practice measures towards enhancing the reliability levels and availability of electricity generating units are  considered. This research specifically explores boiler tube failures in coal -fired electricity generating plants. The  research reviews literatur e relative to global best practices, employed by boiler operators, against high  performing utilities. The research results are analyzed  and highlight correlations between selected variables,  Boiler Tube Failures (BTFs) and identified dominant failure mecha nisms. This paper recommends critical  benchmarks towards reducing downtime of unplanned boiler tube failures resulting in forced outages, in coal - fired electricity generating plant    Keywords  Boiler tube failures, Thermal power plant, Generation fleet.    Introduction  Globally, electricity can be classified as a critical basic human need. Africa is still termed “the dark continent” due to  challenges experienced with the availability of electricity throughout the continent. Saghir (2005) asserts 1.6 billion  people in developing countries do not have access to electricity. On the African continent alone, there is an estimated  population of 1 billion  people that use only 4% of the world’s electricity (Ledgard, 2011). Electricity demand is on  an upward trend on the  African continent and is projected to  increase at a rate of 3%  per annum over the next 20  years, overtaking economic growth. It is critical to have reliable generation and supply of electricity to ensure socio - economic growth of the Afric an continent (Mkhwanazi, 2003). Interruptions in electricity supply can have adverse  effects on t he socio -economic welfare of a continent. To ensure sustainable and reliable electricity generation,  appropriate maintenance practices need to be implemented.    Literature  This research focuses on analysing critical concepts in literature that are relevant to the proposed study and presents  a comprehensive analysis of important points of knowledge relevant to maintenance strategies, associated with boiler  tube failure cause-effects. Furthermore, this research presents international best practices on failure mechanisms and  the impending. A review of literature regarding maintenance strategies and implementation in industry is presented  and different maintenance tactics are concisely reported and discussed.    Power plant energy generation  Power plants are designed to generate electricity by converting chemical, thermal and kinetic energy into  electrical energy, as depicted in Exhibit  1. There are many forms of power plants that exist,  such as fossil fired  plants, nuclear plants, hydroelectric plants and solar plants all utilizing  different forms of energy to produce  electricity. Coal is the most prominent source of energy utilised in electricity generation (Agarwal and Suhane,  2017). ",uj.ac.za,University of Johannesburg,South Africa,-26.18493745,27.99979246435022
70,AWARENESS OF TRENCHLESS TECHNOLOGIES IN THE UAE,@uaeu.ac.ae,"Questionnaire Survey, New Construction, Rehabilitation, Trenchless Technology, Underground Utilities .   ","The population of the United Arab Emirates (UAE) is growing and major cities are becoming increasingly crowded.  Billions of dollars have been allocated to replace and maintain the existing infrastructure. This includes the installation,  inspection, repair, and replacement of water supply, sewer, storm water, power, and telecommunication networks.  Conventional open-trench methods to install new and rehabilitate existing utilities are expensive and lead to traffic  disruption including road closures, traffic delays, detours resulting in noise and loss of access to homes and businesses,  particularly in congested urban areas like Dubai and Abu Dhabi emirates. The UAE government is, therefore, looking  at alternative methods for the installation, replacement, and maintenance of aging underground utilities.  This paper will present the results of a quest ionnaire survey conducted among construction  practitioners in  the UAE representing municipalities, contractors, and design firms to elicit the current level of awareness of trenchless  construction methods and their popularity as emerging technologies among construction practitioners across the  country. Survey results provided an indication of the current and expected future trend in the application of trenchless  technologies including the types  currently employed. It also provide d an idea on the percentage of projects that  employed trenchless technologies or are willing to use these technologies in the future. A comparison of trenchless  technology utilization now and in 2007 is also presented. The survey results revealed the current and potential future  growth in utilizing trenchless methods and the expected average government expenditures for the construction of new  utilities and the rehabilitation of existing ones.    Keywords  Questionnaire Survey, New Construction, Rehabilitation, Trenchless Technology, Underground Utilities .      Introduction and Background  While the UAE  construction sector has recently come under substantial pressure due to the prolonged slump in oil  prices, it remains resilient, particularly in Dubai and Abu Dhabi Emirates, with its planned real estate and infrastructure  projects continuing to progress.  The UAE construction sector is projected to grow at a rate 6.5% through 2019  (Export.gov 2017). Investment in infrastructure projects are expected to fuel this growth. Billions of dollars are spent  every year in new facilities to improve the infrastructur e in the UAE. Despite subdued oil prices, immense sums of  money are still being poured into mega projects in the UAE, and the list of new developments coming on stream, from  malls and commercial complexes to villas and skyscrapers, just keeps on growing. A s of October 2017, more than  11,600 projects worth $805 billion are being built across the UAE, up by 16.3% from the 10,002 active projects under  construction in 2016, which were valued at $776.9 billion. That’s roughly $28 billion increase in  expenditures in just  one year (Lookup.ae 2017).  The World Expo 2020 will be held in Dubai (UAE) between October 20, 2020 and April 10, 2021 over a site  of around 438 hectares located midway between Dubai and Abu Dhabi (BIE 2017). Recently, Dubai has made major  investments, which are all set to start by Expo 2020 (Gulf Business 2017). The Dubai Expo 2020 also would see a rise  in the GDP as predicted by the International Monetary Funds. In an article published online on May 2016, Dubai  scored 89% on happiness index  for government services. This “Dubai Happiness”  initiative has 16 programs under  four themes that sums up 82 projects to be set in the city with an aim to make the city the happiest by 2020 (Arabian ",uaeu.ac.ae,United Arab Emirates University,United Arab Emirates,24.19718435,55.680583440887546
71,A METHODOLOGY FOR COMPARING RISK BEHAVIORS IN THEVIRTUAL AND NATURAL WORLDS,@ttu.edu,"Risk, Virtualization, Behavioral Economics, Poker, Kelly Criterion, Geometric Mean Criterion ","This paper outlines a  proposed methodology for measuring and comparing risk behaviors between two similar  populations interacting with functionally identical systems in virtual and natural (or ‘real world’)  environments.  Ultimately, such a methodology may be deployed to understand how and to what degree the structure of systems may  influence the behavior of human system actors by comparing two functionally identical systems (online and live play  poker games) and identifying incidents where the apparent magnitude of risk averse  or risk seeking behavior among  system actors differs from one play environment to the other . Poker serves as a useful test system by being an easily  understood system where economic decisions are made based upon incomplete information . Unlike many other real- world systems, the rules and structure of poker are as easily reproducible in a virtual setting as a live play environment.  However, the value of this research extends far beyond the doors of the casino and truly isn’t about poker at all . This  research is born at the intersection of engineering, psychology, and economics and is built upon insights from all three  fields. Understanding whether, how, and to what degree human behavior may be altered by  replicating a system  virtually may lead to valuable insights into ecommerce, social interaction, telemedicine, distance education, or perhaps  any other system with an internet-based or virtual equivalent.     Keywords  Risk, Virtualization, Behavioral Economics, Poker, Kelly Criterion, Geometric Mean Criterion    Introduction and Background  Much of the future of systems engineering and engineering management may include designing and implementing  virtual systems or systems which have a virtual component. However, a great deal of our understanding of how system  users interact with systems is based on experiences in a non -virtual world. Engineers and managers may therefore be  forgiven for assuming that similar groups of people are equally rational (or irrational) online as they are in the physical  world, but sufficient research has not yet been conducted on this subject . Literature already exists to su ggest that  rationality is malleable based on circumstances or conditions  (Kahneman, 2011; Thaler, 2015) , but there has been  comparatively little work to support or quantify the notion that similar effects may be observed by replicating a system  in the virtual world . At this time, there is no known convenient methodology to do so . This paper will propose a  methodology to derive a comparative baseline for comparing two similar systems, one virtual and one natural.     Decision Under Uncertainty  Prevailing theories of cognitive and social psychologists in the field of behavioral economics would imply that human  decision making is not always rational. Prospect theory in particular suggests that individuals predictably and regularly  make counterintuitive decisions which appear to be suboptimal in terms of both value and utility , and may even be  easily manipulated via framing or other methods  (Kahneman & Tversky, 1979) . Studies conducted in an imal  populations, such as with wolves and dogs  (Marshall-Pescini, Besserdich, Kratz, & Range, 2016) , bonobos and  chimpanzees (Heilbronner, Rosati, Stevens, Hare, & Hauser, 2008) , and different variet ies of bird from the Paridae  family (Kawamori & Matsushima, 2012)  might suggest an evolutionary predisposition or ecologically induced  predisposition towards certain risk tendencies. Although criticisms and revisions to the rule have since been published  (Lim, Wittek, & Parkinson, 2015) , the most prevalent description of risk behavior in animals s uggest the “energy ",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
72,SMART HOME REQUIREMENTS: IMPLEMENTATIONS ANDAVAILABILITY,@gmail.com,"Internet of Things, Smart Home, Smart Products, Old Population Management Service, Protocol. ","Technology evolution introduced the IoT paradigm, resulting in the Smart Home Environment for home services.  Most Smart Home services are currently wide ly available globally. Services such as smart metering has the highest  implementation rate around the world and services for old population management of their daily activities and health  at home is identified as a potential for South Africa (SA). A total of 1936 published articles are retrieved from Science  Direct and Taylor Francis databases. The inclusion criteria contained “smart home services, smart home products and  smart home protocols “.  After the exclusion protocols, a total of 74 articles met all the criterions and are used for data  analysis and results investigating the feasibility of Smart home services in SA together with local network capability.  The smart home requirements include home automation, home monitoring, home security and life conve nience are  assessed and are a determining factor to deploy the service. Network readiness to offer smart home services in SA is  accessed. This paper is able to conclude, on the international standard protocols, products availability and services for  deploying a smart home environment. Aspects such as home automation, home monitoring, home security and life  convenience are a determining factor for the smart home. The feasibility of Smart Home service for the old population  category is reported as a use case scenario to evaluate readiness in SA. This considers the architecture design readiness,  smart home requirements and Smart Home products availability.     Keywords  Internet of Things, Smart Home, Smart Products, Old Population Management Service, Protocol.    Introduction  South Africa (SA) faces challenges when it comes to new technology implementations. The challenges evolve around  Information Technology (IT) architecture design, security and interoperability  (Chen, et.al., 2017; Hui, et.al., 2017;  Buraji, 2017).  Such challenges can be observed in adoption of services  such as home Automation, home Monitoring,  home security and life c onvenience where successful deployment is dependent on architecture design, security  mechanisms and interoperability of systems. Services such as home automation, home monitoring, home security and  life convenience constitute a smart home solution.    Smart home refers to home that is technological capable and have sensors, actuators integrated into infrastructure to  serve the purpose of having everything connected and accessible from anywhere  (Alaa et al, 2017, Shini, 2017).  In  adopting a smart home solution as an IOT Technology  the need to answer the key question that is what is the  international best practice and what would constitute a su itable network design and service availability for IoT  technology deployment in SA?   This research paper aims to review international smart home services available and  compare with South Africa’s current and propose new services considering technology challenges that SA faces today.    Literature Review  An extensive literature review is used for this research. The architecture evolution merges with the technology of  today. There is a difference between smart and intelligent things. Smart things are considered as things that can be  electronically controlled  (Dovydaitis & Jasinevicius, 2015) and on the other hand intelligent things is where there is  human intervention by modelling most features (Dovydaitis, Jasinevicius, 2015).   The Internet of Things (IoT) access communication category includes the Wide Area Network, Global  Positioning Systems (2G), General Packet Radio Service (3G), Long Term Evolution (4G), Local A rea Network  (Wimax) and Personal Area Network (Wifi, Near Field Communication, Z -Wave, Zigbee, IEEE802.15.4)  (Silva, ",gmail.com,,,46.3144754,11.0480288
73,"STUDY ON SAFETY MONITORING SYSTEM OF CONSTRUCTIONCRANES IN GUANGZHOU, CHINA",@163.com,Construction cranes; Safety; Monitoring system; Project management; Economic effects. ,"Today China is in the stage of rapid urbanization development . More and more advanced technology and  sophisticated equipment are commonly requ ired to meet the needs of large  construction projects. This has led to   many challenges in the project managements. One of the most cha llenges is safety of construction cranes used  widely in construction sites. However, accidents might happen from time to time and even cause severe casualty due  to improper operating construction crane system as a result of serious adverse social impact and economic losses.  Hence, it is urgent to develop a safety monitoring system including the construction cranes control, risk warming   alarm, and remote monitoring management to ensure the system detect effectively and report possible unsafe  issues  and restrict improper operation . The system was awarded twice the prize of the science and technology by the  Association of Civil Construction of China, Guangdong. The re are 7 sections  in the paper : Demand Analysis,  System Design, Database of soil nail pullout tests, Application & Impact, Economic effects.    Keywords  Construction cranes; Safety; Monitoring system; Project management; Economic effects.    Introduction  1. Demand Analysis.   In construction projects, the vertical transportation of materials usually relies on the construction  lifting machinery.  Various types of construction cranes ha ve been widely used in recent years. Because of all kinds of reasons such as  tight construction schedules, limited budgets for contractors,  unit leasing, equipments  installation, lack of safety  awareness, and different level of technical staffs, cranes accidents occur from time to time, and even lead to trauma  casualties during the construction. Construction enterprises undergo huge economic losses.     According to the circular from the general office of the Ministry of housing and urban -rural construction, there were  487 accidents nationwide and 624 poeple dead in the building constructions and municipal projects for 2012. Of the  29 major and above accidents, 121 were died,of which 10 were cranes accidents, accounting for 34.48% of the  larger accidents and 52 died.It accounts for 42.98% of the total number of major accidents.   Therefore, it is very necessary to carry on the safety management of construction cranes through the establishment  of information system.    After analyzing the aggregation of construction cranes accident s in different places, we have developed Guang zhou  Safety Monitoring System of Construction C ranes to achieve a construction cranes on -line monitoring, data  transmission, monitoring data statistics and decision analysis with computer technology, automatic control  technology and communication technolog y based on  the requirements of the norms, procedures, standards,  regulations and other relevant documents of China.      ",163.com,,,46.3144754,11.0480288
74,ASSESSING SCORING DIFFERENCES BETWEEN AWARD WINNERSAND NON-AWARD WINNERS FOR THE MALCOLM BALRIGENATIONAL QUALITY AWARD,@uah.edu,"Discriminant Function, analysis, classification, Malcolm Baldrige National Quality Award Model. ","The Malcolm Baldrige Performance Excellence Model has proven to be a valuable tool for use by organizations  interested in performance excellence. A form of it is used by almost all industrialized countries including Japan to  improve on performance excellence in vari ous sectors of the economy. The purpose of this research is to use the  scoring data from a state program to investigate which categories show significant differences in scoring between  those who win the award and those that do not. Fisher’s Discriminant analysis is used to show that there is significant  difference at a 0.05 significance level , between scores for all 7 criteria for award winners and those who do not win  the award. The classification function resulted in a strong correlation coefficient of 0. 82, with 87.5% and 100% true  positive and true negative respectively. The result shows that scores for all 7 criteria play a significant role in winning  an award.    Keywords  Discriminant Function, analysis, classification, Malcolm Baldrige National Quality Award Model.    Introduction  The influx of Japanese automobiles and electronics into the American market in the mid- 1980s created a great  awareness and burden on quality professionals in the US to seriously drive the performance excellence agenda into  American businesses (Townsend & Gebhardt, 1996).  To help address this, t he Malcolm Baldrige National Quality  Award (MBNQA) was instituted in 1987 (Steeples, 1994) by the US congress to award U nited States organizations  that have attained some level of performance excellence based on an assessment by independent examiners.   Since its inception the award has gained a lot of publicity and literature shows that a form of the Baldrige  model has been rolled out in most industrialized countries including Japan (Council, 2014) (Townsend & Gebhardt,  1996). Unlike the MBNQA model which looks at the entirety of enhancing performance excellence in the organization  as a whole, models like the Shingo Prize and Capability Maturity Models focus more on some specific aspects of the  organization; like operational, software processes etc.   There are undoubtedly numerous benefits of the performance excellence award to the US economy. In 2011,  Link et. al, showed that the economic and social benefit to cost ratio had increased considerably from 207 -1 for  an  earlier study they did in 2001 to 820 -1 (Link & Scott, 2012) . The importance and usefulness of the model further  resulted in the emergence of local and state quality award programs which  ensured that applicants attained some  maturity before applying at the national level.   The award procedure involves examiners giving scores using submitted applications and eventual site visits  to applicants that are shortlisted. The scores are given based on the applicants’ achievement and improvement in 7  areas known as the Baldrige Criteria for Performance Excellence.  The 7 criteria are 1. Leadership, 2. Strategy, 3.  Customers, 4. ‘Measurement Analysis and Knowledge Management’ (MAKM), 5. Workforce, 6. Operations, and 7.  Results (ASQ, 2018). The categories are weighted with maximum attainable as: 140, 100, 100, 100, 100, 100 and 360  for each of the categories 1, 2, 3, 4, 5, 6 and 7 respectively, total maximum attainable score of 1000. Examiners award  all scores on a scale of 0 -100 and the total score for each category is calculated by multiplying the percentage score  by the total attainable for each breakdown. The award winners are announced based on the scoring for all applicants.  The issue then is what is the takeaway for non-winners? How far are they from winning? Can they have some feedback  about their performance in relation to the goal of winning the award?  ",uah.edu,University of Alabama at Huntsville,United States,34.7252,-86.6405
75,A REVIEW ON EFFECTIVE MAINTENANCE STRATEGIES ANDMANAGEMENT FOR OPTIMIZING EQUIPMENT SYSTEMS,@uj.ac.za,"Availability, Corrective Maintenance, Downtime, Preventive Maintenance, Reliability-Centered Maintenance ","The manufacturing, process, mining, technical, and engineering industries share a common goal of sustainability in  delivering the output product. Achieving t his goal requires effective maintenance systems with the objective to  maintain high reliability for production, high availability, and adequate standards of safety and quality compliance.   This paper presents a review of maintenance strategies and analysis of failure data obtained from a metal crushing  plant. Downtime data indicates the dominating equipment with failure affecting the system’s availability. The study  researches corrective and preventive maintenance strategies with supporting policies such as the Reliability-Centered  Maintenance (RCM), Condition -Based Maintenance (CBM), and Time -Based Maintenance (TBM). This includes  failure cause analysis methods and steps for finding the root cause and factors involved. Focus is on the methods  applied to opti mize performance of equipment and systems, increasing equipment availability, and minimizing  maintenance costs. The study further develops quantitative and qualitative analysis using mathematical and statistical  modelling tools for failure data. This aids with evaluation and data interpretation when performing analysis. Results  obtained indicate reliability parameters with an increasing failure rate over the recorded period. Probability functions  depict varying average time between failures on monthly inter vals. Weibull functions provide graphical  representation of the failure distributions and failure rate. The function also depicts the wear out and constant regions  of the distribution.    Keywords  Availability, Corrective Maintenance, Downtime, Preventive Maintenance, Reliability-Centered Maintenance  (RCM), Root Cause Failure Analysis (RCFA), Weibull Distribution.    Introduction  Maintenance of equipment and systems in engineering organizations has become increasingly popular in recent  decades. Due to the natur e of commercial competition within the industry, optimizing production and operational  systems is of utmost importance. Even though maintenance is largely neglected, it has a great impact in solving most  of organisational problems. Literature indicates var ious constraints, including  the complexities involved with  maintenance activities (i.e., stopping production to pe rform maintenance tasks) and related  intangible significance  (Mechefske & Wang, 2001) . Apart from maintaining the plant, functional effective maintenance systems are  essential in achieving the following goals:  • Improving plant/equipment efficiency and its systems  • Reducing maintenance costs.  • Improving reliability and availability.  Cost of maintenance in most large industrial operations accoun t for at least 40% of the annual operational  expenditure (Dunn , 1998) . With this consideration, best maintenance practices and strategies are required to  mitigate the loses and achieve organizational goals. Various strategies are classified in different types:  corrective  maintenance, preventive maintenance, Condition -Based Maintenance (CBM), Reliability -Centered Maintenance  (RCM), Time-Based Maintenance (TBM), Total Productive Maintenance, etc. Exhibit 1 below illustrates the sub-",uj.ac.za,University of Johannesburg,South Africa,-26.18493745,27.99979246435022
76,ENHANCING STRATEGIES TO IMPROVE THE PURIFICATIONPROCESSES AND SYSTEMS OF WASTEWATER CARE WORKS INGAUTENG,@uj.ac.za,"Wastewater facilities, East Rand water care, Wastewater purification.  ","Water usage for transportation  of human and other waste constituents cannot be overstated. There is a  need for water purification , prior to u tilization, at a wastewater facility.  Wastewater facilities collect,  treat and dispose wastewater generated from several sources. Wastewater facilities serve as sites for  wastewater purification and treatment. This paper focuses on the Wastewater facilit ies based in the  Southern pa rt of Africa . This research applies engineering management practices to direct enhancing  strategies towards improving the purification processes and systems of East Rand wastewater care. This  research aligns with global best pract ice benchmarks relative to wastewater treatments and purification.  East Rand wastewater facility is currently challenged with developing an effective sludge disposal  system. Employing conventional best practice purification benchmarks with high operating a nd  maintenance costs to deal with the rapid population and industrial growth is a major limitation. This  research seeks to identify existing gaps in the East Rand wastewater care zone, relative to global best  practice measures. A mixed methodology is adopt ed based on interview sessions, critical review of  literature and observation of the East Rand wastewater facility . The research results are analyzed ,  presenting current limitations and recommending critical benchmarks towards improving the purification  processes and systems.    Keywords  Wastewater facilities, East Rand water care, Wastewater purification.     Introduction  The facility under review is a wastewater purification and treatment organization . The facility offers  clients necessities ranging from opera tional, technical to management wastewater solutions. The facility  is experiencing limitations in compliance  especially in comparison with global best practice wa stewater  purification. These are inclusive of escalating operation and maintenance cost, old and overloaded  wastewater infrastructures. The overcapacity of wastewater distribution is as a result of rapi d growth of  population and industries in the Ekurhuleni region a suburb in Johannesburg. The facility under study   employs activated sludge processes  for biological nutrient removal together with biological filtration.  There exist challenges in the existing sludge process techniques. This is inclusive of the presence of  sludge bulking which negatively affects the quality of the final effluent distribut ed to the streams. The  sludge disposal system currently involves burying the sludge. Due to space limitations, this is not a best  practice approach. There is a necessity to seek alternative methods towards handling the sewage sludge.  ",uj.ac.za,University of Johannesburg,South Africa,-26.18493745,27.99979246435022
77,MULTI-PROJECTS SYNERGY ANALYSIS OF THE PROCESS OF NEWURBANIZATION IN CHINA,@126.com,"New Urbanization, Multi-projects, Synergy ","New urbanization is a complex project system, there are many subsystems in the process, and these subsystems are  basically driven by different level projects. Therefore, it is of great significance to study the multi -projects system  synergy of new urbanization to improve the quality of new urbanization. Based on the literature review, we selected  three critical programs including residential buildings, municipal public utilities and public facilities in the process of  new urbanization to analyze the system synergy. By using the survey data of  different regions whose urbanization  rate ranked in the top ten in China by the end of 2016, the paper analyzed the relationship among projects of different  level in the process of new urbanization, and then applied the order parameter method to construct the model of multi- projects coupling coordination degree so as to analyze the system synergy of new urbanization. The results have shown  that the multi -projects synergy is very important to the success of the new urban ization. The paper provided some  suggestions for the management practice of new urbanization from the perspective of multi -projects coordination.    Keywords  New Urbanization, Multi-projects, Synergy    Introduction  China’s urbanization has been developing at a high speed. From the end of 1996 to the end of 2016, China's  urbanization rate has increased from 29.37% to 57.35%. However, the long -term rapid development has also been  accompanied by some problems. Blind expansion of urban fringe, population explosion , the huge loss of arable land,  deteriorating ecological environment and other problems are common. In order to solve these problems, new  urbanization in China needs to shift the focus from the speed to the quality (Qiu, 2013). New urbanization is a complex  system which involves economic, social, ecologi cal, cultural and other aspects. Its development is supp orted by  different levels of projects. In fact, the internal synergy of new urbanization is the synergy among subsystems, which  is embodied in the syne rgy within a  project and among projects.  At present, most researches on the quality of new  urbanization focus on sustainable urban development  (Yigitcanlar & Teriman, 2015; Trillo, 2016; Yang, Xu, & Shi,  2017) and measuring the urbanization synergy level of different scales from the multi-dimensional index system (Niu,  Du, & Li, 2013; Cao, 2014; Zhou, Xu, Wang, & Lin, 2015), which are on the system level, such as the synergy among  the four modernizations  (Liu, Deng, & Ran, 2 017; Wang, Yao, Yang, & Chen, 2017; Li, Xu, & Dong, 2017 ), the  synergy among different subsystems  (Liu, Xu, & Wang, 2014; Li, Liao, Yang, Zhuang, & Shi, 2015 ),etc. Rarely  researches have involved the synergy within the subsystem.  With the continuous develo pment of the new urbanization process, a large number of rural people move to  the town, resulting in a large demand for housing, road, health care and other construction facilities. As the foundation  of the new urbanization system, the quality of the urban  construction subsystem needs to be paid att ention to. Thus,  this paper chose three major programs including residential buildings, municipal  public utilities and public facilities  in the urban construction subsystem as the research objects and built a model of internal coupling coordination degree.  Finally by taking the ten provinces with higher urbanization level s as an example, we analyze d the internal synergy  of their urban construction subsystem s. We aim  to provide reference s for the research on the qu ality of  new  urbanization and to promote the steady and rapid development of new urbanization.    ",126.com,,,46.3144754,11.0480288
78,HEALTH 4.0 AS A CATALYST TO A DYSFUNCTIONAL SERVICEDELIVERY: THE CASE STUDY OF JOHANNESBURG PUBLICHOSPITALS.,@gmail.com,"Health, 4.0, Hospital, Information, Operation, Service Delivery. ","Service delivery is a dynamic unique input -output-outcome process tha t changes, as the Stakeholders’ requirement  changes; which functions range from quality care delivery, operational performance efficiency, organizational  support, monitoring and management of resources. Dysfunctionalities stem from burden of diseases, waste, and abuse  of resources, inadequate procurement practices, population surge, theft, poor decision -making, fraud, managerial  incapability to match service with demand, un -noted expiry, and counterfeited drugs, ineffective referrals and  unreliable tracking systems, long waiting times and excessive length of hospital stays. Current surveys in healthcare  boundaries indicate results, in service delivery, with dissatisfaction level of 76%.  This research investigates the  performance cause-effects of hospitals underpinned by a lack of integrated information and communication systems.  To achieve best healthcare service practice and quality delivery, integration of the facets requires the inclusion of  healthcare 4.0: an electronic data exchange within entire value chain that eliminates inefficiency, and ineffectiveness.  This paper presents Health 4.0 concept as an inevitable seamless modular accelerant that interoperates, virtualizes,  decentralizes and converges data across services to ensure personalized healthcare outcomes. Using the concept design  principles, the paper further pinpoints Service orientation, real -time capability, cost reduction and efficiency  leveraging the entire health value chain.    Keywords  Health, 4.0, Hospital, Information, Operation, Service Delivery.    Introduction  Improving the strategies of health Service delivery is a functional process (driven by effective and integrated  information and communication technologies) that involve personalization and individualization as a service to the  client, patients’, and the health team, assuring the triad a deliverable outcome. The evaluated and assessed challenges  of poor quality of services outcomes faced by Gauteng hospitals that resulted to loss of confidence in these public  healthcare facilities, at tributed mostly to the information bottlenecks is paramount importance to hospitals’  management, the government and to the patients, (Nwauka & Weeks, 2013).   The primary objective of this paper is to identify these service delivery problems linked to data applications,  and to conceptualize Health 4.0 as catalytic support to virtualize, interoperate and revolutionize the service orientation  and delivery strategy; optimize the real-time capability, reduce cost, and increase efficiency of the entire health value  chain achieving deliverables through Health 4.0 design principles application. Revisiting these service delivery  problems will assist in ascertaining the need for Health 4.0 in this revolution trend and ensure that the delivery of  healthcare services is planned, monitored and managed appropriately to ensure reduction of waste (Nwauka & Weeks,  2013).    Literature Review  This section reviews both the service delivery cum integrated information system currently existing amongst state  hospitals in Johannesburg within the same geographical and demographic area in Gauteng province.     ",gmail.com,,,46.3144754,11.0480288
79,OPTIMAL SITE SELECTION USING MULTI-CRITERIA DECISIONANALYSIS AND OPTIMIZATION,@gmail.com,"Simulation,  ","The selection of a location for any public center among alternative locations is a multicriteria decision-making problem  with a signficant impact on the society ’s welfare. The conventional approaches for location problem tend to be less  effective in dealing with the vagueness nature of the linguistic assessment since it includes both quantitative and  qualitative criteria. Under many situatio ns, the values of the qualitative criteria are often imprecisely defined for the  decision-makers. In this research insights for finalizing the location of a city park in San Jose is provided by comparing  the results with different multi-criteria decision-making techniques. For this purpose, a short comparative analysis of  the methods and a numeric example of each approach are given.  This study examines AHP, SAW, TOPSIS, and  Optimization approach along with Sensitivity Analysis to evaluate which of the sites is best suited to build a city park.  The six alternatives are infill site in the center of the city, vacant site in city outskirts, adjacent to a supermarket, beside  an elementary school, in an economically disadvantaged neighborhood, remodeling and expa nding an existing park.  The four criteria are cost, population density, area of the center and the duration of the project completion.  The  alternatives are ranked and a set of compromising solutions  are explored. Based on the results,  two best alternatives  are proposed to  the decision makers. The first alternative is the infill site in the center of the city, and the second  alternative is remodeling or expanding an already existing park.    Keywords: Optimization, Multi Criteria Decision Making , AHP, TOPSIS, Sensitivity  Analysis, Monte Carlo  Simulation,     Introduction  Parks are essential elements of cities and neighborhoods that are primarily places for recreation, community gathering,  and natural open space. Parks can come in all shapes and sizes , from linear parks that may incorporate portions of  greenways or trails, to large wooded passive parks, to urban playgrounds, to pocket parks and shaded urban squares  such as dot Savannah and London. Numerous studies have shown that access to parks attrac ts residents, increases  property values, enables sports and recreation tourism, and spurs investment as community members appreciate safe  and easy access to natural outdoor areas to relax and enjoy the outdoors. Additionally, as many communities have  turned their attention to planning for healthy communities, parks provide opportunities for active recreation, fresh and  clean air, and space for trees and open space, which contributes to improved air quality and climate resilience.   Like any type of land use, the building and preservation of parks are controlled by a city, town, or county’s future  land use map, zoning code and map, and development and subdivision regulations. The amount of parkland and open  space that is needed in a given community is typically based on population and ease of access. It also takes into account  existing facilities, population dynamics, natural resources protection, and public input to set a course for the park  system’s future.  The layout and the location of public sites have lar ge impacts on quality of life and welfare of citizens. For  instance, Vahdatzad and Griffin (2016) showed that the design of hospital layouts may increase the quality of care by  providing efficient services to the patients and reducing timeliness of care. Various researchers have suggested both  quantitative and qualitative methods to approach public site selection . Mostly, research in ideal location has been ",gmail.com,,,46.3144754,11.0480288
80,A SYSTEMS ANALYSIS OF THE U.S. ARMY’S HUMAN DIMENSIONSTRATEGY,@usma.edu,"Enterprise Transformation, Systems Analysis, Value Path  ","The Army Human Dimension Strategy (AHDS) underscores  the importance of  optimizing individuals  as a hedge  against the future world challenges. While the AHDS provides a framework for improving human performance as part  of Army’s Force 2025 and Beyond (F2025B) and Army Operating Concept (AOC) , the AHDS is challenging to  implement as written. The AHDS presents a complex problem that requires multiple stakeholders at various echelons  to align their organizations, processes, metrics, and values down to the lowest level . This paper applies a systems  approach to holistically analyze the Army’s Human Dimension Enterprise (HDE) as a “complex system” and provides  recommendations for the current enterpri se transformation. The ability of the HDE to provide “optimized human  performance” depends on the interactions and alignments of numerous stakeholders across three levels –  enterprise,  organization, and individual.  The research traces the value path from the enterprise d own to the individual level to  identify misalignments, opp ortunities, and critical gaps. Research  methodology includes a literature review,  stakeholder value network, and systems analysis of the HDE. Key findings indicate that the human dimension strategy  lacks clarity and a well-defined metrics for a successful enterprise transformation. The human dimension strategy is   achievable with the alignment of the enterprise goals, system level requirements, o rganizational programs, and   individual stakeholder needs. The research provides recommendations to address  enterprise challenges identified  through the research.     Keywords  Enterprise Transformation, Systems Analysis, Value Path     Introduction  The world is becoming more complex, interdependent, and quickly evolving. In 2015, the U.S. Army developed a  vision to “win in a complex world” as part of the Army Operating Concept (AOC) called the Army Human Dimension  Strategy (AHDS). The AHDS elevates the importance of optimizing individuals as a hedge against the future  operational challenges.  This paper  analyzes the AHDS enterprise transformation efforts applied to the U.S. Army  Intelligence Center of Excellence (USAICOE) as the enterprise within the larger Human Dimension Enterprise (HDE).  USAICOE is one of the eight centers of excellence (COE) responsible for training, education, and future force  development for Army intelligence under the Training and Doctrine Command  (TRADOC) (TRADOC, 2016). To  navigate the challenges of the human dimension transformation, the author conducts a systems analysis to better  understand the current state of the enterprise and makes recommendations for the current enterprise transformation .   The research maps the high -level AHDS down to the individual level to identify misalignments, opp ortunities, and  critical gaps. The primary objective of the research is to determine - How effective is the Army Human Dimension  Strategy (AHDS) delivering value to all three levels of the enterprise?    Background   The origins of the human dimension rese arch traces back to the TRADOC p amphlet (TP) 525 -3-7-01, initial  capabilities document for U.S. Army human dimension (2012), and interim c hange recommendation document 11  July 2013 (U.S. Combined Arms Center  (CAC), 2014). The AHDS highlights the importance of understanding the  cognitive, social, and physical componen ts of the individual soldiers.  As such, the ultimate goal of the AHD S is to  optimize human performance by “applying knowledge, skills, and emerging technologies to improve and preserve the  capabilities of the Department of Defense personnel to ex ecute essential tasks ” (TRADOC, 2014). Relevant to the  human dimension areas of research, “cognitive dominance” (Matthews, 2013) and “grit” (Duckworth, 2016) advocate  ways to increase an individual’s human performance. As shown in Exhibit 1 , the AHDS provides a framework for  how the future Army must focus its resources on increasing the human performance of soldiers (TRADOC, 2014). ",usma.edu,United States Military Academy,United States,41.3927227,-73.95986305044411
81,THE IMPACT ANALYSIS ON GROUP CONSENSUS FOR EMERGENCYDECISION-MAKING OF LARGE-SCALE PROJECTS: ANEXTREME RISK PREFERENCE PERSPECTIVE,@163.com,"Emergency Decision-making of Large -scale Projects, Extreme Risk Preferences, Risk Preference Influence Model, ","Reaching high-consensus of decision-making in a short period of time is crucial in  an event of emergency during  large-scale projects. However, the pressure of time and complex risk preferences of decision -making members make  it difficult for h igh-consistency decision making in real p rojects. In order to study the influence of extreme  preference members on group consensus in emergency decision-making of large-scale projects, this article takes the  Tan Jian -shan Gold Mine expansion project  in China as an example  and takes viewpoint dyna mics theory, group  decision theory and opinion leader t heory as research methods. Then the risk preference influence model, the risk  preference similarity model  and the risk preference evolution model  are constructed to analyze the case data. From  the case data and model analysis  in the perspective of risk preference, we can see the structure of decision -making  members in large-scale infrastructure projects formed by two types of decision makers: extreme preference members  and non-extreme preference members . Finally, through big data simulation, we find that the acceptance degree,  the  influence of duration and other  parameters will affect the power of extreme preference members, and further  will  influence the c onsensus time. The influence  of extreme preferen ce members on group consensus is positively,  negatively or even reversely affected in an event of emergency during large-scale projects. This study plays a certain  reference in leadership of specific groups on group consensus in large- scale projects and provides a feasible method  for risk decision-making in the emergency event of large-scale projects.    Keywords  Emergency Decision-making of Large -scale Projects, Extreme Risk Preferences, Risk Preference Influence Model,  Group Decision Making Consensus.    Introduction  Emergency events of large -scale projects have the characteristics of randomness, rapid diffusivity, derivative,  transmission variability, huge destructiveness, high time pressure and so on. Once the events occurs, decision - making members will make  rapid identification and assessment of risks in a limited time according to their own  knowledge, experience and so on. And the emergency plan with low risks will be made under high time pressure by  decision-making groups. However, in the actual process of  large-group emergency decision-making, it is difficult to  form a highly consistent decision plan in a short time due to the influence of time pressure and risk preference of  decision-making members.    Literature review ",163.com,,,46.3144754,11.0480288
82,AN EXPERIMENTAL ADAPTIVE TEACHING PRACTICE,@mtsu.edu,"Experimental, Adoptation, Motivation, Clarity,Teaching Practice. ","In this paper the authors reflect and explain the challenges  during the lead author’s teaching experience over the past  few years and also provide adapta tion of teaching practices in order to improve students’ learning experiences.  The  lead author experimented with a combination of  methods and best practices adopted from his past instructors and  from the  literature.  Each course needs its own method of teaching, therefore method adaptation is of utmost  importance. Best teaching practice factors not only depend on  the course subject, but also to the teaching institution,  level and background of students, and cultural factors. This paper reports on the va rious methods that the author has  employed in different sections of a particular course  in order to analyze and identify the most effective factors that  impact student success. The findings should provide assistance and guidelines for new educators to imp rove their  teaching practices. The results of this research have been compared to the best practices in the literature which  emphasize: 1- student motivation, 2 - adaptation,  3 - clear explanation of applicability of the subject matter and its  connection to real world problems, 4- clarity and organization of the course syllabus and course requirement, 5- timely  returning of the graded assignments,  6 - starting each class with an overview  of the subject matter before diving into   the subject details, and 7- emphasizing the importance of the subject matter and the benefits drawn from learning the  subject. Authors hope that this paper will initiate meaningful discussions on  evaluation of var ious teaching methods   which could result in the improvement of the overall teaching quality and student success in technical fields.     Keywords  Experimental, Adoptation, Motivation, Clarity,Teaching Practice.    Introduction  Teaching is a sophisticated and multidimensional activity striving for effective and efficient principles to be followed.  Good teaching is an art to master and is not an accident. The major factor for effective teaching and promoting student  learning is understanding the students and the ir learning processes, as quoted by Herbert Simon, one of the founders  of the field of cognitive science “Learning results from what the student does and thinks and only from what the  student does and thinks, the teacher can advance learning only by influe ncing what the student does to learn.”  (Ambrose, Bridges, DiPietro, Lovett, & Norman, 2010)  Bedekar, Lee, Spearot & Malshe,  (2013) reported the mentor and mentee relationship using a teaching  scholars pilot program for a sophomore level course. In this stu dy, the authors reported how a new engineering  educator can benefit from the experience of a mentor while developing their own teaching pedagogy. Several factors  such as building relationship, course and pedagogy development were explored by the participan ts for their  professional development as well as course redesign.   Teaching evaluations have been widely used as a performance measure and teaching effectiveness. Although  there are arguments in the literature regarding the effectiveness of  teaching evalu ations as means of evaluating the  performance and quality of the teaching. One of the main factors is some types of questions that may have strong  biases with regards to  the teaching effectiveness (Boring, Ottoboni, & Stark, 2016). However, teaching evalu ations  are still one of the useful tools for  evaluation of teaching techniques and for providing feedback for instructors.    The objective of this paper is to analyze and adopt the factors which could help in creating an environment  that would support stu dent learning. These factors have been investigated by combining the theory of the teaching  effectiveness from the literature and empirical teaching experiences. William James (James, 1983) investigates the  importance of the science of applying psychology to education and importance of understanding the learning process ",mtsu.edu,Middle Tennessee State University,United States,35.84860085,-86.36090357044624
83,M,Missing,,,Missing,,,46.3144754,11.0480288
84,SYSTEMS THEORY,@sdsmt.edu,"Systems Science, Game Theory, Cost Allocation ","The Federal Highway Administration (FHWA) and state Department of Transportation (DOT) agencies conduct  Highway Cost Allocation (HCA) studies to divide the total highway cost of construction, rehabilitation and  maintenance among the various vehicle classes using it. The purpose of this study is to consider the HCA problem  from the viewpoint of systems theory. W e suppose that a highway facility is a transportation system where the road  users classified as vehicle classes are elements deriving benefits out of using this system. Such a system has a well - defined boundary in the context that vehicle classes travel on  highways only and not in any other infrastructure  environment. More importantly, these elements or vehicle classes comprise of varying attributes whilst remaining  interdependent. For instance, certain elements require thicker roadways or pavements such as trucks, while other have  a demand for thinner roadways but more number of lanes such as passenger cars. However, these elements or vehicle  classes with their unique characteristics form coalitions or subsystems with membership of two or more vehicle classes  recognizing that cost responsibilities can be lowered by shared usage of highway facility. Therefore, cost allocated to  an element for utilization of the system should not exceed an amount the element would have paid for participating in  a subsystem. Systems behavior postulates maximum benefits can be accrued by elements choosing to be a member of  the main transportation system inclusive of all elements or vehicle classes. This study will seek to identify whether  elements are exhibiting a symbiotic relationship.    Keywords  Systems Science, Game Theory, Cost Allocation    Introduction  The objective of highway cost allocation (HCA) studies is to apportion total expenditures made by the state department  of transportation (DOT)  agencies and Federal Highway Administration (FHWA) on infrastructure assets such as  highways and bridges among road users. Typical examples of road user groups include passenger cars,  motorcycles,  trucks and busses. HCA studies conducted by the FHWA  and state DOTs  serve to verify equity between user fees  such as gas taxes paid by vehicle classes, and the corresponding cost imposed by them. Essential expenditures made  by the highway agencies for upkeeping include project costs undertaken with respect to construction, rehabilitation  and maintenance. This paper will focus on allocation of cost of constructing a new highway facility. The criteria used  by HCA practitioners  for assigning cost  to various classes of vehicles  is grounded on a principled approach to  fair  cost-division and centered on a measure of utilization of the highway facility. Multitude of HCA methods exists that  facilitate attribution of highway costs among road user groups . However, among many such available methods, the  procedure associated with modeling of HCA as a cooperative game  being played among st players is analogous to  defining the allocation problem in the context of system sciences.   The objective of this paper is to  establish a preliminary foundation  for the HCA problem to be viewed as a   system comprising of interconnected elements that are behaving in a strategic and cohesive manner. The motivation  behind the study conducted in this paper  is to take advantage of  a problem-solving approach in the area of  general  systems theory developed by Bertalanffy (1968). The author stresses analysis of a research  problem as a synergistic  process as opposed to in isolation. Examination of a given problem as a system allows researchers to develop an  effective blueprint based on which  an appropriate mathematical tech nique can be identified for finding a robust  solution. The most widely used procedures implemented for  HCA can be broadly classified into  incremental and  proportional methods. A significant modeling shortcoming of incremental and proportional  methods lies in  their  ability to detect the inter-connectedness and inter-dependencies present within the HCA problem. Hence, the intent of  this paper will be to demonstrate suitability of assessing an HCA problem from the standpoint of a systems thinking.  ",sdsmt.edu,South Dakota School of Mines and Technology,United States,44.0731,-103.2063
85,A FRAMEWORK TO INTEGRATE PERFORMANCE MEASUREMENTSYSTEMS WITH DATA ANALYTICS,@pucpr.edu.br,"Performance Measurement Systems, Data Analytics, PMSDF, Performance Analytics, IoT, Industry 4.0, Big Data. ","Performance Measurement Systems (PMS) have been adopted as a management support tool in several industry  segments. The data obtained through these systems bring knowledge about an organization’s operations and must be  interpreted to support decision making. The use of data analytics methods may guide this transformation of data into  useful and insightful information that provides knowledge. However, current PMS do not embed data analytics tools  for data processing and visualization in a straightforward manner. This paper presents a procedure for the evaluation,  design, and implementation of a PMS integrated with a data analytics process. The use of this procedure is illustrated  through two application cases in maintenance and service operations: the earlier deals with maintenance management  of an automobile manufacturer, while the latter is used to analyze the performance assessment surveys of a university’s  certificate program courses. Results associated with the utility, usability, and feasibility of the procedure are discussed.  The framework is not restricted to specific industries or operations and can be used in organizational units, such as  departments or branches, independent processes or self-contained projects.    Keywords  Performance Measurement Systems, Data Analytics, PMSDF, Performance Analytics, IoT, Industry 4.0, Big Data.    Introduction  The performance management systems (PMS)  are a precious resource to provide the company with information for  data-driven decision making.  It relies on the  performance measurement , that is  the proces s of quantifying the  efficiency and effectiveness of an action, and on the process measure, the metric used to quantify this efficienc y or  effectiveness (Neely, Gregory, & Platts, 1995).  Operations (like manufacturing plants, supply chains, extended enterprises, service companies) may be modeled as  input-output processes, with internal/external clients and value chains. These physical models are managed, creating  results, that may be measured as performance. The management process also needs inputs (performance information)  to create the output (decision), that works in two loops: operation management, assuring the operation is  working  as ",pucpr.edu.br,pontifícia universidade católica do paraná,United States,-25.441105,-49.276855
86,MODULUS OF RESILIENCE: AN ISOMORPHIC APPLICATION FORCRITICAL INFRASTRUCTURES,@ttu.edu,"Resiliency, Critical Infrastructures, High Impact Low Frequency (HILF), Reliability, Electric Power ","The term resilience often appears in contemporary litera ture to qualitatively describe systemic responses following  disaster.  A search in the literature did reveal the quantitative construct of resilience with respect to materials  science.  Additionally, review found the application of stress, based on materia l science constructs, to quantitatively  represent resilience in generic system terms.  However, literature review reflected an absence of quantitative  measures of resilience specifically related to power systems.  The maximum energy that can be absorbed per unit  volume without creating permanent distortion is the Modulus of Resilience (U r).  The Modulus of Resilience is  calculated by integrating the stress -strain curve from zero to the elastic limit.  This paper proposes an isomorphic  application of the Modulus of Resilience to quantitatively express resiliency of electric infrastructure.    Keywords  Resiliency, Critical Infrastructures, High Impact Low Frequency (HILF), Reliability, Electric Power    Introduction  The increasing electrification of society’s functions decreases the public’s tolerance for protracted electric outages.   As more functions require electricity, contemporary societal systems are subjected to a single point of failure.  Traditional r eliability metrics have focused more on recurring common cause events ; conversely, quantitative  metrics for major event resiliency have been lacking.  “[L]iterature frequently uses the concept of resilience to imply  the ability to recover or bounce back to normalcy after disaster occurs” ( McEntire, Fuller, Johnston & Weber , 2002)   A significant challenge in justifying investment to increase High Impact Low Frequency  (HILF) event resiliency is  the inability to quantify expected gains.  A method of expressing the expected value is utility , which is defined as  the expected satisfaction from an action or item.  Decision Makers (DMs) receive satisfaction through positive  outcomes resulting from mitigation actions.   While utility can be qualitative or quantitative , the use of quantitative  utility can prove influential in cases involving high -levels of uncertainty.  Therefore, this research aims to encourage  optimal resiliency investment by providing a metric which supports the cardinal representation of utility.     Quantifying Resiliency  As the term resiliency becomes more prevalent in the discussion of electric outage response, a need to transition  from qualitative to quantitative measures will help evaluate performance  of systems , establish goals and justify  investment.  System design and enhancement is aided by a  clear aim based on tangible expectations.  Scenario  analysis can then be utilized to assess gaps between e xpected and desired outcomes.  Although a  number of  traditional reliability metrics currently exist, new considerations are required as hazards and threats associated with  HILF events evolve.   Without quantification of the expected utility from resiliency efforts, under -investment in  infrastructure may result.  Economic utility can be expressed using either cardinal or ordinal methods.  The cardinal method utilizes a  numerical approach with units of “utils”. The use of cardinal methods enables interval or  ratio scales to be  employed, where interval scales indicate both order and difference in value and  ratio scales indicate order,  difference and utilize an absolute zero. “Utils” are assumed to be equitable to varying levels of satisfaction  correlating to different types of goods, services, etc.  The quantitative approach supports the mathematical derivation  of total, average, and marginal utilities.  Total utility can then be used to represent the sum of utility received from  successive units of the item consu med or acquired.  Average utility is the total satisfaction received divided by the  corresponding number of units.  Lastly, marginal utility is the utility gained by one additional unit of an item.  The  use of cardinal utility requires acceptance of basic assumptions, including: a person can express utility in ",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
87,THE PRACTICE OF SEEKING SIMPLICITY IN RISK DECISIONS UNDERUNCERTAINTY,Missing,"Ockham’s razor, Pareto Principle, Prospect Theory, Decision-making under uncertainty, Reasonability, Complexity, ","When making discrete risk decisions, people intuitively seek problem simplification to reduce the quantity and  complexity of available options , a concept that is often anathema to many in engineering  disciplines. While the  adequacy of a ship’s structural design in high seas is clearly understood and can be assured though application of  simple safety margins, or when modeling doping of silicon wafers for chip produc tion requires only the first term of  an infinite Taylor series , the same understanding among engineers cannot be assumed for assessing risk decisions.  This paper will explore the application of Ockham’s razor (ontological parsimony) and the Pareto Principle (80/20  rule) to risk decisions  with the objective of reducing complexity and increasing simplicity through reasonability in  decision-making. To simplify the interactions between factors considered in risk and hazard analys is, several  behavioral economics  studies which counter established utility and rational decision theories will be used in  developing a model which applies these two axioms.    Keywords  Ockham’s razor, Pareto Principle, Prospect Theory, Decision-making under uncertainty, Reasonability, Complexity,  Simplicity.    Introduction  There exist two views of human decision -making: first, that the human mind is brilliant and worthy of emulation;  second, that “the human mind is an imperfect reasoning device” (Jaynes, 2003).  The classic decision theory tools are  predicated upon a firm definition of rationality, which is based upon maximization of some objective criteria or utility  (Bertalanffy, 1968) and typically appears to make perfect sense when thoughtfully considering the matter. Conversely,  a decision which does not maximize utility b y selecting the highest return on an investment or the lowest -risk  engineering design would be considered irrational. However, as noted by Ludwig von Bertalanffy, human behavior  and decision-making do not always comport with rationality (Bertalanffy, 1968). Therefore, we are left in a quandary:  how can irrational humans collectively be expected to make rational decisions according to established definitions of  rationality? To address this question, this paper explores the notion of reasonability by use of Ockham’s razor and the  Pareto Principle.    Furthermore, the fundamental models of expected utility which provide the conventionally-accepted definitions of the  rationality of decisions are themselves flawed. Consider , for e xample, the cost-benefit analysis performed by Ford  Motor Company when they chose to forego a mere $11 per vehicle modification to the Ford Pinto which would have  prevented 180 deaths and national scorn. As documented in an internal memorandum, Ford safety  engineers applied  standard cost-benefit analysis tools, consistent with practice and then-current government standards, to determine that  the total cost of applying the improvement ($137M) far exceeded the benefit ($50M), result ing in the choice to not  make the design changes (Grush & Saunby, 1 970)—an apparently rational engineering decision. Following public  outcry subsequent to Mother Jones Magazine’s breaking of the Pinto story (Dowie, 1977), the actual cost to Ford, in  terms of lawsuits and company reputation, far exceeded their estimates and they joined other tragic corporate failures  in the formation of what is now the field of product liability. How is it that a rational decision process yielded an  irrational (incorrect) an swer? Did the intuition of the four Ford engineers who signed off on the Pinto analysis run ",Missing,,,46.3144754,11.0480288
88,"THE INDUSTRIAL INTERNET OF THINGS MODELS, CHALLENGESAND OPPORTUNITIES IN SUSTAINABLE MANUFACTURING",@d.umn.edu,"Industrial Internet of Things (IIoT), Cyber-Physical Systems, Sustainability Manufacturing, Industry 4.0. ","The Industrial Internet of Things (IIoT) has brought a novel technological paradigm that is rapidly changing the  modern manufacturing systems, economic landscape and business infrastructure. The ad -hoc nature of the network  architecture and the low power connectivity features of the IIoT help observe, model, optimize and control processes,  machines as well as the whole manufacturing enterprise remotely and cost-effectively, which means more integration  with the real world and less human intervention. The IIoT interface helps people and system components communicate  at the right time and place at a reasonable cost, from product conception to service providing. For industries, the IIoT  has revolutionized the fields of machine learning, big data technology, sensory networks and machine -to-machine  (M2M) communications. That has improved the overall manufacturing system performance and made the m more  sustainable than ever, by monitoring their power consumption and waste. In this paper, a brief introduction of the  current development of IIoT is presented focusing on the digital factory, the cyber -physical system and the Industry  4.0 paradigms. Th e current challenges and limitations of IIoT and cyber manufacturing systems, such as economic  impact, architectural patterns and infrastructures are discussed. In addition, the future opportunities of IIoT integration  in sustainable manufacturing are inve stigated, showing the gaps in the technology and model s to achieve the triple - bottom-line (TBL) of sustainability while getting closer to the full potential of the IIoT implementation in the new  Industry 4.0 paradigm.    Keywords  Industrial Internet of Things (IIoT), Cyber-Physical Systems, Sustainability Manufacturing, Industry 4.0.    Introduction  The Industrial Internet of Things (IIoT) is an enabler to the smart industrial revolution (Metallo, Agrifoglio, Schiavone,  & Mueller, 2018 ) as it has improved the total management system of the manufacturing, transportation and  consumption of goods as well as services (Urquhart & McAuley, 2018). IIoT is the network of a multitude of devices  connected by communication technologies that results in connecting together smart machines, advanced data analytics  (cloud computing)  and; intelligent transportation throughout the manufacturing system delivering new insights  resulting in an unprecedented economic profit (Digital, 2017; I. Lee & Lee, 2015; Posada et al., 2015; Winnig, 2016).  With the help of IIoT’s features companies can easily maximize profit gaining by providing real-time production data,  reducing the downtime, improving asset performance, reducing wastages and so on. Even the interlinked components  like cyber manufacturing system (CMS) , advanced analytics, and machine learning to opera tions can open potential  springboards for new business op portunities. Adopting IIoT in the real -time information and communications  technology (ICT) researchers have estimated that  12 billion devices will be connected by 2020 for the flexibilities of  Cyber Physical System (E. A. Lee, 2008) (Cooperation, 2013). Organization like Ericsson, has its vision of connecting  18 billion devices by the end of 2022 (Cerwall et al., 2015). In overall, IIoT will create a super profitable global market  growing from 16 billion in 20 14 to 50 billion in 2020 harnessing remotely controllable or constantly connected  physical objects embedded in local set tings (Weinberg, Mi lne, Andonova, & Hajjat, 2015 ). Day by day, IIoT  is  creating new manufacturing systems and hardware components  for improving its economic margin and making  industries more sustainable than ever.  The term Industry 4.0 is synonymously used with the IIoT where Industry 4.0 was launched by the German  government in Hanover Fair (Drath & Horch, 2014; Kagermann, 2015; Shrouf, Ordieres, & Miragliotta, 2014). After  that, many other re gions have created their own variants of the same concept. In the US, the Industrial Internet  Consortium was set up to encourage greater use of the IIoT in the American manufacturing sector. So far, IIoT features  have created a vision where recent developments in information technology are expected to enable entirely new forms  of cooperative engineering and manufacturing with the key idea of intelligent products and machines bringing more ",d.umn.edu,University of Minnesota - Duluth,United States,46.8203898,-92.08527412081386
89,AN ANALYTICAL EXPLORATION OF THE THEORIZEDRELATIONSHIP BETWEEN TOTAL PRODUCTIVITY AND CASHCONVERSION CYCLE,@ttu.edu,"Total Productivity Management, Working Capital Management, Cash Conversion Cycle. ","The cash conversion cycle (CCC), a proxy metric u sed for the measurement and management of Working Capital  (WC), has long been established in previous research as being crucial to organizational profitability. The same holds  true for the measure of productivity. Intuitively, synergies between CCC and productivity are obvious: both influence  profitability. This paper explores analytically this intuitive relationship between Total Productivity (TP) and CCC that  is lacking in literature. The concept of TP is derived from the theory of Total Productivity Mana gement, which  acknowledges and includes in its framework the impact of non -operational factors on operations and as such states  that WC influences productivity and thereby profitability. The results of such an analysis will assist engineering  managers to u nderstand this critical relationship in addition to providing the platform for further research into the  predictability of health/profitability of an organization using either or both of TP and CCC.     Keywords  Total Productivity Management, Working Capital Management, Cash Conversion Cycle.    Introduction  That the primary objective of an organization is profitability is well established (Goldratt, Cox, and Whitford, 2004).  Assuming entification of an organization is permitted, its health as a whole then is determined by several measures,  all of which combined ensure profitability (Dean, 2015) . Sink and Tuttle (1989) discussed that high performance,  which leads to profitability, is determined by seven factors: effectiveness, efficiency, quality, productivity, q uality of  work life, in novation, and budgetability . Kamali-Nejad and Beruvides’ (1998) work added an eighth factor, that of  environmental performance, as critical to organizational health. Based on the huge body of prior research, and even  instinctually, p roductivity, not just the operational kind, can be expected to determine the profitability of an  organization.     Total P roductivity Management (TPMgmt) is  a productivity measurement and management theory postulated by  Sumanth (1984) that seeks to determine productivity , not in silo o r singularly from an operational perspective but  instead through inclusion of factors extraneous to production itself that are equally critical to production and  productivity. For example, from an enginee ring manager’s perspective, conventional operational productivity (say,  units produced per man-hour) being the same, an organization that has on its books higher debt or interest repayments  is probably worse off in that it requires more resources to achiev e the same level of operational productivity, and the  higher debt payments imply lesser capital available  for say productivity or quality improvement initiatives. An  important feature of Total Productivity (TP) , apart from its intent of performing a system ic analysis of productivity,  is that working capital (WC) is central to its meaning and implications (Sumanth, 1984; Tiruvengadam and Beruvides,  2016a). That feature of TPMgmt is one of the primary factors underpinning this research.     The TP construct itself is a disarmingly straightforward ratio of total tangible output produced per unit of total tangible  input (Equation 1; Sumanth, 1984). The output and input are not the same as revenue and costs of production; rather,  revenue and costs of production are but a part of an array of measures that comprise the former as shown in Equations  2-3 (Tiruvengadam and Beruvides, 2016b) . Manifestly, the equations suggest a systems approach through the  inclusion of all factors, not just operational. Also, c ritically, this ratio is one of dollar output created per dollar input  expended by combining the vario us factors  based on their dollar value, yet again a simple way to overcome  dimensional inconsistencies between the factors themselves that might have otherwise prevented combining them all. ",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
90,A PROSPOSED METHODOLOGY FOR DEVELOPING SYSTEMSTHINKING LESSONS BY AND FOR NON-EXPERTS,@oregonstate.edu,"Systems Thinking, Systems Thinking Education, Systems Thinking Lessons , Human -Activity Systems , Systems ","Systems thinkers achieve expertise by learning from experts and reading existing systems thinking literature. This  method is typically successful, if the learner is driven; however, this method does not work for everyone. This explains,  in part, why systems thinking is not widely used. Many of today’s complex problems can be solved using a systemic  approach, but there are too many problems and not enough systems thinke rs. Therefore, the need to expand systems  education beyond the select few is critical. To foster widespread systemic thinking we must start reaching the next  generation of thinkers in primary and secondary education (K -12). Unfortunately, the number of exp erts capable of  teaching systems thinking is already small and the number of experts with the ability to teach K -12 students is even  smaller. Additionally, the lack of systems thinking curriculum suitable for K -12 students presents another challenge.  To ad dress these challenges, the authors propose a systemic methodology, rooted in engineering management  concepts, which will allow non-experts to create systems thinking lessons for K-12 audiences. The authors present the  results on using the proposed methodo logy during a multi -year industrial engineering Capstone project, and discuss  whether non-experts are capable of teaching systems thinking to other non-experts. The proposed methodology, while  focused on developing systems thinking -centric lessons, has the  potential to assist engineering managers with  developing training modules for their work teams.    Keywords  Systems Thinking, Systems Thinking Education, Systems Thinking Lessons , Human -Activity Systems , Systems  Literacy    Introduction  The typical approach employed to solve problems and study systems is known as reductionism. This approach breaks  a problem or a system down into its constituent parts to gain an understanding of those parts, and then works backwards  to understand the whole problem  or system based on the parts. Reductionism can be successful in some cases, but it  often fails when addressing complex problems in complex systems because a whole system is greater than the sum of  its parts (Jackson, 2003). The alternative to the reductionism approach  is a holistic approach. This approach views a  whole system as the emergence of the interactions and relationships between parts. Once a system has emerged, it  provides meaning to the parts and the interactions between those parts. This contrasts with the reductionism approach  which allows the parts to provide meaning for the whole system . Holism is still interested in understanding the parts,  but more attention is given to the network of relationships between the parts and how the interactions between thos e  parts give rise to the whole system. (Jackson, 2003).  Systems thinking embraces this  powerful holistic approach. Unfortunately, relatively few people have  achieved expertise in systems thinking compared to other disciplines. This is due, in part, to the process of becoming  a systems thinker. Similar to most other disciplines, a motivated learner achieves expertise  in systems thinking  by  learning from an expert and read ing existing literature on the subject to gain knowledge. Although this process is  effective at producing more systems thinkers, it lacks the profileration necessary to propogate systems thinking beyond  the select few who are driven to learn. Most disciplines, like physics or biology for example, have thousands of experts  available and learners (students) are exposed to these subjects during primary and secondary (K -12) education. The  systems thinking discipline has significantly less experts available to teach those students who are interested and most  students in K-12 education are not formally exposed to systems thinking content. To help solve complex problems the  world needs more peo ple who are  simply aware of systems thinking concepts  and who can use a holistic approach   when solving problems. Therefore, a new methodology needs to be d eveloped for teaching and learning systems  thinking that will help increase the awareness and use of systems thinking concepts; the methodology should facilitate ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
91,ROUTING ALGORITHM AND COST ANALYSIS FOR USINGHYDROGEN FUEL CELL POWERED UNMANNED AERIAL VEHICLEIN HIGH VOLTAGE TRANSMISSION LINE INSPECTION,@mst.edu,"UAV, Transmission line inspection, Cost model, Routing algorithm. ","With the rapid development of robotics technology,  robots are increasingly used to conduct various tasks by utility  companies. Unmanned Aerial Vehicle (UAV) is one of the efficient robots that can be used to implement high voltage  transmission line inspection. One bottleneck of the existing UAV technologies used in transmission line inspection is  the limited capacity of the lithium battery . The practitioners need to land the UAVs and replace the lithium battery  frequently to complete a certain range of inspection. In this paper, the team explores the feasibility of using hydrogen  fuel cell as the power to support UAV in transmission line inspection. A routing algorithm is proposed to guide the  motion of the ground team and the schedule of the  landing points of the UAV. The cost is analyzed and compared to  the scenario that traditional lithium battery is used. A case study is implemented  to validate the effectiveness of the  proposed routing algorithm and examine the performance in terms of cost effectiveness.    Keywords  UAV, Transmission line inspection, Cost model, Routing algorithm.    Introduction  The transmission line in power system is subject to usage deterioration and environmental corrosion. To ensure reliable  power supply, transmission line inspection should be one essential aspect of routine maintenance considered by utility  companies. In earlier years, transmission line inspection was done by linemen, which is not an efficient  method due  to low efficiency and dangerous working condition . Later, helicopter patrol was adopted by utility companies   (Whitworth, Duller, et al, 2001; Yan, Wang, et al, 2007; Earp, Eyre-Walker, et al , 2011). However, it’s still risky  when the pilots and the team members are close to the high voltage line.   Unmanned Aerial Vehicle (UAV) that has been widely adopted in many fields such as pollution monitoring,  filmmaking, and reconnaissance, has also been used for serving  as a promising approach for the transmission line  inspection (Wang, Han, et al. 2009 ; Wang, Chen, et al. 2010; Yang, Yin, et al. 2012 ; Li, Mu, et al. 2016). However,  lithum batteries, the prevalent electric power storage technologies used by UAV, cannot provide the required energy  for a long endurance. Most UAVs powered by lithium battery can only support a usage period of approximately 20  minutes, which is far below the endurance expectation of the transmission line inspection.   To improve the power endurance, fuel cell technology, a novel power system, has been proposed to serve as  an alternative to lithum battery as a potential power source for UAVs. The hydrogen fuel energy system is  a better  energy supporting method compared to fossil fuels (Veziroğlu, Şahin, 2008). Gadalla and Zafar (2012) investigated  the integration of a hybrid power system consisting of hydrogen fuel cell, photovoltaic panels, and battery for a small  UAV, which illustrates that the flight performance can be improved with the hybrid power system with an endurance  increase from 470 min to 970 min. Kim and Kwon (2012) described the design, construction, and flight test of a fuel  cell-powered small unmanned aircraft. The authors proposed a power system using a polymer electrolyte membrane  fuel cell combined with a hydrogen generator. The possibility for the utilization of a fuel cell in a small aircraft was  successfully validated. The fuel cell aircraft flew for 2 h without incidents in the fuel cell system.  For now, the study of the transmission line inspection adopting robots is mainly focused on technical issues  including data transmission, rout ing algorithm, video and image analy sis, etc., while the cost analysis is by largely  ignored. Compared to the cost analysis for the transmission line inspection using suspended robots whose 80% total  cost is from staff salary ( Nagarajan, Qin,  & Sun, 2017), inspection approaches using UAVs have the potential to ",mst.edu,Missouri University of Science and Technology,United States,37.9532435,-91.77426666814159
92,REVISITING THE EFFECT OF LEADER-MEMBER EXCHANGE ONCOUNTERPRODUCTIVE WORK BEHAVIOR: IN THE PERSPECTIVEOF CROSS-CULTURE,@tju.edu.cn,"Leader-member exchange, counterproductive work behavior, cross-culture ","Although extant literature has shown that leader -member exchange has an effect on counterproductive work  behavior, only a few articles identify different cultural groups when studying on the issue. This study aims to  construct a theoretical model of hypotheses about the relationship between leader -member exchange and  counterproductive work behavior in th e perspective of cross -culture. There exist two groups which are against local  and migrant cultural backgrounds separately in cross -border companies. In this paper, we respectively study the  effect of leader -migrant member exchange and leader -local member exchange on counterproductive work behavior  of local members. We conduct a systematical review of literature and theories, and use the method of semi - structured interview to propose some hypotheses about the main effect, mediating effect and moderating eff ect of  the relationship between leader -member exchange and counterproductive work behavior. These hypotheses finally  form the theoretical model. Overall, this study provides new insights on what theoretical model exists in the  relationship between leader-member exchange and counterproductive work behavior.    Keywords  Leader-member exchange, counterproductive work behavior, cross-culture    Introduction  With the deep development of economic globalization, more and more companies have started cross -border business  to take advantage of local resources to build new factories or develop new markets. Cross -border companies often  choose to employ local people to meet a large number of demands for labors due to low labor cost and restrictions of  local laws on the number of migrant workers. However, cases in recent years have proved that many cross -border  companies are plagued by counterproductive work behaviors ( CWB) of local employees, such as absenteeism,  sabotage and strike, which has significantly influenced the reputation of cross -border companies. Therefore, the  behavior of local employees has become a hot topic in the field of organizational management.   Early studies on CWB focus on the concept and effect on work performances, and study CWB at the  individual level. Now researchers pay more attention to identifying the potential antecedents of CWB and the  relationship with these antecedents. The researchers have gradually considered CWB as a group -level phenomenon.  However, current researches focus more on the effect of external antecedents on the CWB, such as cultural  differences, when they study CWB in the cross -culture situation. They neglect the effect of  internal antecedents,  which makes the existing literature lack a comprehensive model of CWB in the cross -culture situation.  The majority of researches into group antecedents have confirmed that the leader -member exchange (LMX)  is correlated with CWB. Some  investigate the effect of LMX on CWB, and some investigate the effect of LMX on  the group antecedents of CWB.  Considering that LMX is the relationship between leaders and employees, we can investigate the theoretical  model of the relationship between LMX and CWB by viewing LMX as an entry point and CWB as a consequence  variable. This model combines the study of main effect, mediating effect, and moderating effect.    ",tju.edu.cn,Tianjin University,China,39.107253299999996,117.17789778037583
93,BENCHMARKING CELLULAR TECHNICAL EFFICIENCYUSING DATA ENVELOPMENT ANALYSISOR - USAOR - USA,"@pdx.edu,","Data Envelopment Analysis, DEA, Mobile Cellular, Mobile Base Station, Telecommunication, Efficiency, Mobile ","Cellular technology infrastructure requires major capital spending with the rapid obsolescence requiring frequent  upgrades. This paper bridges engineering and management by using a robust and objective management tool for  benchmarking mobile cell efficiency with the important radio Key Performance Indicators (KPIs)  for evaluating the  technical efficiency of the mobile cells. Furthermore, reliability and usage cause challenges in evaluating mobile cells  to enhance the Quality of Services (QoS). Data Envelopment Analysis (DEA) is employed as a methodology to build  the evaluating model, and we identify a robust multidimensional benchmarking model of current performance that has  one input and multi-outputs. The approach was tested on data from a local mobile operator in North Africa. Moreover,  the set of references from the best practice point of view for the inefficient cells are defined. These results give network  engineers specific suggestions for future improvements based on other cells. Finally, the scope of further research is  provided along with some opportunities to enhance the model for new technology and other aspects in this application  area.    Keywords  Data Envelopment Analysis, DEA, Mobile Cellular, Mobile Base Station, Telecommunication, Efficiency, Mobile  Network Performance.    Introduction    Over the past decade, there has been a remarkable development in the mobile telecom sector, and the technologies  and competitive environment have had a significant ripple effect on this fast growing sector. Therefore, the mobile  operators in developing countries have to go through these developments to achieve the competitive advantage to stay  in the market (Casey, 2014) (Chavula, 2013)  (Beltaif, & Weber, 2018) . One of the most critical strategie s for the  mobile operators is to ensure customer satisfaction. As a result, mobile network performance and Quality of Service  (QoS) should be evaluated since most of the customer s are looking for network performance and quality (Kumar,  Anuradha, & Naresh, 2002) (Seytnazarov, 2010) . While the rapid adoption of 3G, 4G, and soon to 5G for mobile  operators, the mobile operators in developing countries are predominantly still based on the 2G  technology (GSM).  For example, in Nigeria, more than 98% of the cellular subscribers are using the 2G technology (Ilyas, Qazi, Rassool,  & Uzmi, 2016) . Therefore, in this paper, we focus on developing the model  using the 2G technology data . In fact,  mobile operators face many new challenges and opportunities as they shift their technologies, and they have to be  strategic to maintain customer satisfaction  in the developing countries  (Dabab, 2016) .  The primary condition for  achieving optimum cellular  services of mobile operators is the high performance of the mobile network. Therefore,  considerable efforts have been spent to develop a Mobile Base Station (BS) to provide better services . We still can  increase cellular network efficiency by determining the inefficient BS, which would be to evaluate their performance  based on multi KPIs. Performance management helps to check the performance of the network, and indications of  individual network elements or services that all KPIs are forming overall QoS (Alam, 2013) (Haider, Zafrullah, &  Islam, 2009) (Kyriazakos, Papaoulakis, Nikitopoulos, Gkroustiotis, Kechagias, Karambalis, & Karetsos, 2002)  To ","pdx.edu,",Portland State University,United States,45.5111,-122.6833
94,WATER INFRASTRUCTURE SUSTAINMENT VIA OPTIMIZATION: ASYSTEMS APPROACH,@ttu.edu,"Infrastructures, Isomorphology, Optimization, Sustainment, Systems, Water Resource Management ","The overall management of water infrastructure systems can have complexity due to the age of the system, various  funding sources and distribution throughout infrastructure, policy/governance, and types of short - and long -term  sustainment approaches.  A systems -level analysis would help in overall optimization where this, in some instances,  will require the su b-optimization of components within the system to achieve the desired state.  In this paper, the  authors define these states along with the identified constraints and obstacles of the system to help satisfy needed  improvements.  An optimized system is more resilient to stress, an important outcome for water resource systems.  In  performance of this analysis, the needs for effective resource infrastructure management would uncover potential  system improvements to aid in both short- and long-term sustainability.  Improvements in operations management and  resource policy would be gained and would help optimize the overall system.  This support s and contribute s to  obtaining an optimized and desired future state.  The approach outlined in this paper has implicati ons beyond water  resource management.  The insights generated from this analysis can be applied to the management of other commons  type resources as well as to other industry sectors.   In thinking beyond just one system type, and based on isomorphic  similarities, this type of systems approach can carry over to help improve and optimize other resource type  infrastructure systems.  It is anticipated that the commonality will improve other sustainment approaches used across  industries.    Keywords  Infrastructures, Isomorphology, Optimization, Sustainment, Systems, Water Resource Management    Introduction  There is an enhanced focus today on the improvement of information systems and the development of efficiencies,  but the core infrastructure systems that meet human needs, such as water systems are largely overlooked.  Water  Resource Management (WRM) and as sociated infrastructure system management can be more complex than may  seem due to the type and complication of the infrastructure, the compound demands to manage the overall system,  and the varying opportunities and challenges that come with overall infrastructure management.  Current systems in  place are old, inefficient, and not resilient nor sustainable.  As a result of new infrastructure implementations and  additions over time, the more elements there are in the system, the more opportunities for improvement and change,  but this also opens the door for more potential vulnerabilities.  Therefore, as general practice and as internal processes  become more complex, the management of the system and infrastructure must adapt with changing conditions.  Th e  development of a systems-level analysis is feasible in helping to determine overall optimization of the infrastructure.   This type of systems thinking approach and analysis aims to look at the system at a whole level for intra -component  optimization with better integration, to maximize system deliverables and results .  Understanding the entire system  (not just the sub -parts within) allows for one to better comprehend the impacts within the system and what can  negatively impact results as well as optimize.   Systems thinking is a set of skills used to help improve in identifying  and understanding systems, predicting their behaviors, and devising modifications in order to produce desired end - state effects (Arnold & Wade, 2015). These skills and components work together as a system and the improvement of  each can lead to overall system optimization.  In addition, and in some instances, the sub-optimization of components  within would help lead to a more desired and efficient end-state.  This system of components are working together to  accomplish optimization and in -turn, the accomplishing the overall aim of the system (Deming, 1994).  Effective  resource infrastructure management at a systems-level would allow for one to continuously uncover potential system  improvements with change to aid in both short - and long-term sustainability.  This is important as improvements in  operations management and resource policy are gained with the results of overall system optimization.  Taking this a ",ttu.edu,Texas Tech University,United States,33.59375255,-101.89959552302756
95,DESIGN OF THE MANUFACTURABILITY ASSESSMENTKNOWLEDGE-BASED EVAULATION TOOL,@cavse.msstate.edu,"Manufacturability, Tool, Subject Matter Experts, Design ","The early stages of manufacturability design play a major role in the life cycl e cost of a product, as such, there is a  need for the application of systems level thinking and input from subject matter experts (SMEs) to be incorporated  into early phase design decisions that will potentially impact life cycle costs. The literature show s that current tools  available focus mostly on cost, with none focusing solely on early manufacturability and design. The  Manufacturability Assessment Knowledge -based Evaluation (MAKE) is a tool created to aid SMEs in assessing the  manufacturability and early design of a product. This type assessment is being created for the U.S. Army Engineering  Research and Development Center’s Engineered Resilient Systems program as an input variable.   This paper will document the research and comparisons of other Design  for Manufacturability tools. The  outcome of this research provides the foundation for the need for a new tool and methodology. Additionally, this paper  will outline the research performed to build the requirements  for the MAKE Tool, discuss the lessons learned from  case studies that led to improvement in the tool, and present an overview of the MAKE Tool. It will also discuss plans  for integrating best practices from industry and concerns and recommendations from S MEs. New capabilities  are  planned for the tool as research continues.    Keywords  Manufacturability, Tool, Subject Matter Experts, Design    Introduction  Acquisition experts for the Department of Defense (DoD) perform trade-off analysis of various system attributes, such  as performance metrics, geometric information , and cost during the acquisition process.  The Engineered Resilient  Systems (ERS) program provides these experts with the capability to perform the analysis on much larger sets of  design alternative s or options.  In support of the ERS program, research is underway at the  U.S. Army Engineer  Research and Development Center (ERDC) to provide these experts as early in the acquisition process as possible  attributes that, historically, are not available un til very late in the process if at all.  The Design for M anufacturability  (DfM) was identified as one such attribute. There are many resources available to manufacturers and engineers such  as guidelines, checklists, data sheets, software programs, etc. that focus on providing best practices for design and  assembly of manufactured parts (Boothroyd, Dewhurst, & Knight, 2011). Most of the information available focuses  on the DfM analysis of individual parts but does not cover other functional areas of manufact uring that impact cost,  such as labor and ergonomics. These resources for manufacturing are also incorporated into a computer-based system  for ease of assessing products.  The research to create a new manufacturability tool stems from the lack of a computerized tool that  specifically looks at manufacturability and does not link directly  to cost . The Manufacturability Assessment  Knowledge-based Evaluation (MAKE) software application draws upon a taxonomy of manufacturability concerns,  based on functional areas of a manufacturing system. It serves to identify areas within each manufacturing system that  are impacted by characteristics of the design, as well as concerns that can be used to compare alternatives of a design.  (McCall et al., 2018). The MAKE methodology is designed to accomplish two things. The first is a metric or value  that represents the manufacturability of the product design and is the basis of evaluating or comparing various design  alternatives. The second is the assessment portion of the methodology which provides feedback on how to mitigate  concerns for future improvements to the manufacturability metric. Whereas the methodology has no direct correlation  that can be applied to the metric to generate an overall cost of manufacturability associated with a product design, it ",cavse.msstate.edu,Mississippi State University,United States,37.9537,-91.7756
96,A STATE-OF-THE-ART REVIEW ON RESEARCH ABOUT THEINCORPORATION OF EQUIPMENT OPERATING CONDITIONS INCONDITION-BASED MAINTENANCE,@hotmail.com,"Condition-Based Maintenance, variable operating conditions, statistical models.  ","Condition-Based Maintenance (CBM) represents one of the best philosophies for optimal maintenance -task  scheduling in industry, causing an impact in cost reduction due to maintenance time decrement and better spare parts  management, mainly. The characterist ic that makes CBM different to other maintenance philosophies consist in  monitoring the current degradation condition of the industrial equipment, with the objective to extend the equipment  useful life. Literature in CBM field mentions the need to consider  the equipment operating conditions, which are  variable throughout its lifetime in most of the cases; however, few studies have developed methodologies that  incorporate these conditions due to the cost that implies data acquisition when executing a run -to-failure experiment.  This work develops a revision of different methodologies found in CBM literature under variable operating conditions,  focusing on the statistical models used, the available data bases, and the decision making carried out based on this  information. The revision of literature comes to an end with gap analysis that allows determining future lines of  investigation in CBM.     Keywords  Condition-Based Maintenance, variable operating conditions, statistical models.     Introduction  CBM references to a maintenance strategy that implements monitoring the system’s present condition to specify when  to carry out maintenance acti ons. The main advantage of CBM methodology resides in performing maintenance  actions only when the monitored condition calls for  them, minimizing interruptions to the equipment normal  operations. Decision making in CBM falls into two categories: diagnostics and prognostics. The fundamental objective  of diagnostics is to detect deviations in the condition variable from the equipment normal state, meaning, diagnosing  a failed state. On the other hand, prognostics main goal is to estimate when the equipment will reach a failed state.  Peng, Dong, & Zuo  (2010), explain that diagnostics attends the detection, separation , and identification of failures  once the anomaly occurred; while prognostics aims to provide trusty predictions on degradation paths, and estimations  of the remaining useful life (RUL) of a component or system subjected to degradation.  Moubray (1992) explains the importance of considering the operating conditions that the equipment is  subjected to during its operating period, since temperature, speed, pressure, height, operators, among others, affect its  performance and, therefore, its degradation development. In this context, considering the environmental conditions  under which an equipment operates is fundamental, not only for the reasons previously explained, but also, for CBM  prognostics and diagnostics, as their omission might mislead maintenance decision making. This study develops a  literature review of studies and databases that consider or acknowledge operating condition variables for CBM ",hotmail.com,,,46.3144754,11.0480288
97,OPERATIONAL GOALS AND METRICS IN VIRGINIA AIRPORTSUSTAINABILITY MANAGEMENT PLANS,@purdue.edu,"sustainability, management planning, airport ","Sustainability is in the E ngineering Management Body of Knowledge (EMBoK)  and is an emerging concern in  engineering management. Airports across the US are creating sustainability management plans (SMP) either as  standalone plans or incorporated into airport master plans. Many airports have developed SMPs and the resulting plans  took different forms and different purposes. While the Triple Bottom Line uses social, economic and environmental  aspects, the FAA suggests the addition of operational aspects for airport sustainability. With 66 public-use airports in  Virginia, the Virginia Depar tment of Aviation (DOAV) has developed a statewide SMP framework and separate  guidance for each of three classifications of airports. These plans extend far beyond environmental management  systems. The DOAV asserts that sustainability is a “strategic appro ach to airport planning, development, asset  management, and resource protection”, uses the triple bottom line approach, and ranks operational needs for today  and the future. This view of sustainability closely aligns with the EMBoK. This paper explores the SMP guidance for  three categories of airports: commercial service, reliever and general aviation, and community and local service. Based  on the operational sustainability activities presented by the Sustainable Aviatio n Guidance Alliance (SAGA),  particular goals and metrics in the DOAV guidance are identified as affecting operational sustainability. This paper  compares operational goals and metrics suggested in the DOAV guidance for the three categories of airports , and  develops a new categorization of metrics for airport operational sustainability. These operational sustainability metrics  may be useful for engineering managers in developing plans and measuring performance of future projects , and in  improving the sustainability of their organizations.    Keywords  sustainability, management planning, airport    Introduction  Sustainability is in the Engineering Management Body of Knowledge (EMBoK) and is an emerging concern in  engineering management. Sustainability has become more and more important as a practical concern in business  strategic planning (Shah & Nowocin, 2015). Engineering management is often included  as a part of airport  management work. Airport management teams have initiated efforts on i ntegrating sustainability principles into  airports daily operations and long-term planning.  Transportation Research Board’s (TRB) Airport Cooperative Research Program (ACRP) published the  Synthesis Report 10, Airport Sustainability Practices. The research team surveyed 25 large airports across the world  and identified the  current and future drivers of implementing sustainability practices at airports, include “a irport  policies, corporate  responsibility, and stakeholder concerns/relations , climate change , federal regulations ” ( Berry,  Gillhespy, & Rogers, 2008, p.10). These drivers represent the external pressure. The Virginia Department of Aviation  (DOAV) identified the internal drivers for airport implementing sustainability practices which are “reducing operating  costs and extending federal, state, and local investment in airport facilities” ( DOAV, 2016e, p. 10). As mentioned in  the Section 3.5.13 of EMBoK sustainability should be incorporated into business strategies in the very beginning  (Shah & Nowocin, 2015).  Pressured by both the internal  and external drivers, a irports across the US are creating  sustainability management plans  (SMP) as standalone plans or incorporated sustainability into airport master plans.  Many airports have developed SMPs and the resultin g plans took different forms and different purposes.  With 66 ",purdue.edu,Purdue University,United States,40.430028,-86.92642114650494
98,EVOLUTION OF FIRM STRUCTURES IN MARKET AND THESUPPORTING LIFECYCLE LOGIC,@uqo.ca,"Market Lifecycle, evolution of firm, organizational structure, strategy, customer profile  ","The structural forms of companies on the market lifecycle change substantially as the lifecycle unfolds. These changes  are both structural and topological in concert with what is happening in the market. The firm shapes and substance  evolve in a fashion that suggests an environmental adaption dictated by the market or more precisely anticipated by  the firms as they read the market signals. This a rticle traces the logic that defines the modes and results of this  adaptation. The lifecycle itself has been demonstrated to be a powerful device by the principal author and colleagues,  in a series of articles, to be symmetrically predictive in time, forec asting both the future and the past. This predictive  capacity applies to structural evolution of the firm and the constraints that the distinct organizational shape of the  structure will place on the firm. Furthermore, knowing the present shape, one can de fine both the past and successor  structures and their fit to the enabling environment. The environment itself can be traced to market deep and surface  structures and the closely correlated strategy landscape dominant at each stage of the market.     Keywords  Market Lifecycle, evolution of firm, organizational structure, strategy, customer profile     Introduction  The deep structure of the market is defined by the underlying customer base that sends specific signals to the surface  structure captured by the lifecyc le , on how to address the needs of the customer base over which the particular firm   is  trending along the lifecycle (Vernon 1992).   In high tech markets (Pavitt 1984) the underlying customer profiles define the fundamental dynamics of the market  demand response, and this concept is easily extended to all markets that are characterized by a succession of demand  profiles as the firms drift along the  market lifecycle, which is the surface structure (Namdari and Li 2017).   The deep structure customer profile is distributed as a bell curve, consisting of innovators and early adaptors who set  the tone for the interactions between the entrepreneur suppliers and the early risk taking and curious client base.  Exhibit 1 captures the drift of the surface structure, the lifecycle over the deep structure, the customer base.     ",uqo.ca,Université du Québec en Outaouais,Canada,45.42226325,-75.73909824426678
99,M,Missing,,,Missing,,,46.3144754,11.0480288
100,UNDERSTANDING BOILER TUBE FAILURES AND THE LIFE CYCLEMANAGEMENT IN THE SOUTH AFRICA COAL FIRED POWERGENERATION INDUSTRY,@uj.ac.za,"Boiler tube failures, Coal fired power plants, Performance, Maintenance. ","South Africa depends significantly on thermal power generation from coal fired power stations. The high demand for  electricity has strained the power utility’s  ability to perform maintenance on the power plants. The postponement of  planned outages and the fact that most of th e power plants have reached  mid-life span , increases the risk of  performance failures of the power plants.   Boiler Tube Failures is one of the major re asons for forced outages in coal fired power plants. The paper focuses on  background information regarding boiler tube failures and the co rresponding failure mechanisms. The cost  associated with maintenance activities, due to downtime is power plants is preposterous. The paper further explores  factors that can reduce Boiler Tube Failures in power plants, hence a reduction in downtime. It is of utmost  importance to the engineering manager to reduce the downtime. The research methodology adopted in this study is  the review of industry data that is available within the South African power generation industry. The data is collected  directly from the power utili ty’s database for analysis.  The findings of the study illust rate that various continuous  improvement process methodologies can be initiated in order to reduce the number of boiler tube failures. There  were 16 9 boiler tube failures across 14  coal fired power stations in  2017. Boiler tube failures were  the second  highest contributor to plant breakdowns in 2015, contributing 33.53%  (1847MW) of the total megawatts lost due to  breakdowns. The power utility has a poor performing fleet of power stations.    Keywords  Boiler tube failures, Coal fired power plants, Performance, Maintenance.    Introduction  South Africa, as a developing country is facing numerous challenges. One of the challenges is the shortage of skills  in the engineering and  in the  project environment. The shortage of skills is perpetuated by the emigration of  experienced professionals for first world countries and the retirement of ageing skilled workers  (Mutshena, et.al,  2008).  All the above mentioned contribute to creating a void in the engineering and project environment. The result  is poor performance and poor quality as seen in the maintenance and service of thermal power generation industry.        South Africa, despite many years of independence still depends on technologies that have been developed elsewhere  in the world. The use of these technologies oft en brings a burden of added costs and most importantly lack of  understanding by the local community on the construction and use of these technologies  (Simelane and Mohamed,  2012). Technological transfer and absorption in any country plays a substantial rol e in the development of that  country. According to the commission on Growth and Development “learning something is easier than inventing it”   (World Bank and Itzhak, 2011).   Steam boilers play a significant role in power generation and other processes such as chemical and refinery  industries, in South Africa. Boilers are normally in operation for an extended period of time usually 200  000 hours,  this leads to water or steam tube failures due to various reasons such as corrosion and aging. In order to maintai n ",uj.ac.za,University of Johannesburg,South Africa,-26.18493745,27.99979246435022
101,UNDERGRADUATE LEADERSHIP PROGRAMS IN ENGINEERING,@oregonstate.edu,"Undergraduate Engineering Leadership Skills, STEM Leadership Development, University Leadership Curriculum, ","Engineers in the workforce work in groups  to solve  complex problems. Effective leadership and soft -skills are  necessary to coordinate group efforts  in industry. As a result, technical proficiency is no longer the  only mastery  expected from engineers  upon graduation. Unfortunately, most undergraduate engineering curriculums overlooks  resources to teach and allow students to practice leading. Literature on engineering development agrees that leadership  skills are an industry need for professional success, however it does not agree on a university teaching method. In the  past decade various styles of leadership programs have developed to fill this gap.  This synthesis paper will compare   effectiveness of current published methodologies that colleges have used to develop STEM leadership courses. Of all  programs analyzed , quantitative results show each method is successful in develo ping leadership skills of their  students. A high amount of support  exists for capstone de sign courses to improve leadership traits of engineers .  However, due to lack of standardization when rating effectivene ss of these programs it is difficult to draw a  definite  conclusion. Further research is needed to 1) understand what leadership qualities should be taught at the university  and 2) fairly compare leadership programs to determine their effectiveness.     Keywords  Undergraduate Engineering Leadership Skills, STEM Leadership Development, University Leadership Curriculum,  Capstone.    Introduction  The previous focus in engineering industry was  only technical mastery  (Kasamoto, 2000).  Consequently, most  undergraduate engineering programs have not attempted to develop  student leaders (Schell & Hughes, 2017). Many  organizations such as the Nationa l Academy of Engineering, American Society for Engineering Management,  the  Accreditation Board for Engineering and Technology (ABET), and the STEM Development Office have asked college  programs to provide leadership training before engineers enter the workforce (Knight  & Novoselich, 2017; Ozgen,  Sanchez-Galfore, Alabart, Medir, & Giralt, 2013).  Recently the international science, technology, engineering, and mathematics (STEM) community has sought  to improve students soft skills in academia (Seat, Parsons, & Poppen, 2000). There are common university leadership  building strategies classified by Graham , Crawley, and Mendelsohn  (2009) in MIT’s program review that included :  those based around leadership  and management theory, based around rigorous team projects, and those involving  industry projects where the program is funded through a company. It is unknown which method of teaching leadership  to engineers is best.  This paper will review STEM  leadership courses that are either lecture or project based, using   published methodologies within the past decade, to investigate  a student’s ability  to meet industry needs  after  completing a program.    Problem Statement  STEM field’s need for leaders has given rise t o college progr ams designed to improve student s’ ability to take  initiative, create vision, communicate, navigate project teams, and handle changes within professional engineering   (Graham et al., 2009). Pressure to develop young leaders has been marked by ABET accreditation requiring students  to demonstrate technical and soft skill  ability on teams (Dixon  & Kauffmann , 2011). Team based programs are  expected to become commonplace at universities. Between 1990 -2005 the National Science Foundation  (NSF)  donated $186 million to revise college programs and improve leadership characteristics of engineering students. NSF’s  donation was the largest investment in college reform for leadership programs (Froyd & Borrego, 2014).  Both indust ries and universities h ave begun developing programs to prepare student s. While leadership  practices have increased as part of the undergrad experience there is a lack of agreement on a method to achie ve  proficiency. Unlike other programs, l eadership courses for engi neers need t o be presented in a  technical format ",oregonstate.edu,Oregon State University,United States,44.56305595,-123.28392337694638
102,DESIGNING SYSTEMS FOR CYBER RESILIENCE,@gmail.com,"Cyber resilience, complex systems, engineering design, resilience engineering. ","Each year, the number of data breaches continues to increase in unprecedented num bers.  In 2017 alone, there were  1,093 cyber-attacks in the United States representing a 37 percent increase from 2016. One of the largest in 2017  involved a breach via a weak point in a company’s website software where personal information for 143 million   people was stolen including social security numbers, driver’s license numbers, dates of birth and credit card numbers.   Due to the continued threat of cyber-attacks, the need to engineer systems for resilience is vital to system security and  has become a critical requirement for developing new systems. This research describes resilience as involving  cognitive, behavioral, and contextual features that are utilized together to guard against environmental adversity within  each constituent system of the cyber ecosystem.  These features of resilience  are used  to highlight design  considerations across the constituent systems of the cyber ecosystem landscape using complex system methods.    Keywords  Cyber resilience, complex systems, engineering design, resilience engineering.    Introduction  With the continued increase in cyber breaches, the resulting monetary losses and reputational impacts, the need to  prevent and mitigate loss from cyber attacks has never been more important. Because of this, focus has turned toward  how to design cyber resilient systems that can adapt to different attacks such that breaches are prevented or loss  reduced.  Yet, while the idea of engineering resilient systems is not new, the definitions and approaches are still varied.   Through a synthesis of literature, this paper will outline key considerations for engineers to design systems for cyber  resilience based on the cognitive, behavioral, and contextual features of resilience. Rather than defining systems from  an information technology or cyber surveillance perspective , this paper provides  a broader view  defining the cyber  ecosystem as a combination of constituent systems (organizational, human, physical and information technology, etc,)  that operate together within the cyber space. Each ha s the ability to introduce vulnerabilities or contribute to overall  ecosystem resilience.   Additionally, this paper proposes a shift in engineering systems thinking from defensive  mechanisms to transformative resilience mechanisms that enable the ecosystem to stay ahead of the curve in the cyber  threat space.    The foundation for this discussion begins by establishing a set of definitions. Given the number of definitions  for “system”, “cyber”, “ecosystem” and “resilience” along with the combined terminology , this is an important first  step to understanding design considerations for cyber resilient systems.  The paper then provides a deeper review of  the features of resilience that need to be present in every system design that can affect the cyber ecosystem.  A review  of systems thinking, systems design, complex systems and resilience engineering will be bridged to these resilience  features to provide a holistic view to engineer for cyber resilience.  Recommendations for future modeling and research  opportunities will then be provided.    Definition for a System  The INCOSE Systems Engineering handbook defines a system as:  “...an integrated set of elements, subsystems, or assemblies that accomplish a defined  objective.  These elements include products (hardware,  software, firmware), processes, people,  information, techniques, facilities, services, and other support elements.”  (Walden, et. al. 2015,  p 5). ",gmail.com,,,46.3144754,11.0480288
103,DATA WAREHOUSING AND BUSINESS INTELLIGENCE WITH BIGDATA,@yahoo.com,"Big Data, Hadoop, Business Intelligence, Data Warehouse. ","Big data technologies came up with promises of handling and faster-processing of hundreds of terabytes of data with  parallel processing capability and a t a low cost. This paper provides an overview of big data technologies, best  practices from the standpoint of BI applications. We propose that conventional data warehousing and big data  approaches can co -exist to provide enterprise business intelligence (B I) community with maximum business  information capabilities. We elaborate how existing users of conventional data warehouses and enterprise business  intelligence can be benefited out of big data technologies. We highlight how business intelligence could be leveraged  with the help of emerging big data technologies.    Keywords  Big Data, Hadoop, Business Intelligence, Data Warehouse.    Introduction  Business organizations continuously make effort to improve their decision making capability using business  intelligence (BI). On the other hand, they want to improve the performance of BI tools and also achieve cost savings.  Business organizations can achieve that goal by taking advantage of the 21st-century computing technologies, software  engineering, data warehousing technologies, cloud computing, and lately with the emergence of mobile devices such  as tablet computers and smartphones. Organizations look  for making informed business decisions as things are  changing in their environments.  Operational data is good for online transaction processing purposes. This data is also analyzed to fulfill the  analytical needs of an organization. Typically, data is sto red in an enterprise data warehouse to perform those  analytical activities. These operational data needs to be pushed to data warehouses more frequently to enable  knowledge workers to access information faster, often as soon as it is created. To achieve su ch goals the database  engines need to have a faster processing capability.  Business organizations have been encountering a huge volume of data over a period of time. During the last  decade organizations stored all business information in data warehouses (Rahman, 2017). They are facing challenges  while data volume grows every year.  This has caused data storage and retrieval response time to increase gradually.  This means that data warehousing DBMS engines and business intelligence (BI) tools need to be more powerful with  parallel processing capability to expedite refreshing data warehouses and retrieve analytical information.   On the other hand, these days, data are generated outside an organization’s transactional data by virtue of the  Internet. Data come from many sources including mobile devices, sensor, and social media, RFID tags, query-logs for  websites, blogs, and product reviews (Chaudhuri et al., 2011). By using external data, such as the ones generated by  users (e.g., online product reviews) an organization can find valuable consumer insights (Bendle & Wang, 2016). By  combining both internal and external data an organization can develop business intelligence capabilities with a 360 - degree view of customer behavior, expectation, and business performance. There are examples of improved customer  experience and business performance gained by using data from external sources (Spiess et al., 2014). To bring in  social media data, which are mostly unstructured, special tools and technologies are needed.   The Apache Foundation provides a host of open source tools and te chnologies to build big data eco-system.  These include the Hadoop framework, NoSQL databases, and machine learning libraries, Hive, Pig, Impala, HB ase  to name a few (Rahman, 2017). Organizations might consider using a conventional data storage system, BI tools and  big data tools and technologies to ingest, store, process, and present data to knowledge workers and decision makers  in a meaningful way. This paper presents an architecture that provides  data flow and BI capability based on both  transactional and big data.   ",yahoo.com,,,46.3144754,11.0480288
104,TOTAL QUALITY MANAGEMENT & APPLE SUCCESS,@gmail.com,"Total Quality Management, Operations and Supply Chain Management, Apple Inc. ","This project investigates the relation between Apple’s success and Total Quality Management (TQM) tools’  implementation in an organization. Today, TQM has become one of the most important principles in management.   Any successful organization’s management team must work to continuously improve the quality of its products,   services, and/or work environments in order to be competitive; this can easily applied via a TQM principles & tools.   Apple is one of the world’s most successful companies. Although it has not implemented TQM principles &  tools verbatim as in TQM text books, it has been successful in part due to implementing TQM principles from the  start. This project focuses on presenting this company’s success and quality improvements in different areas, and their  relation to each other from the TQM perspective.  It discusses the evaluation of Dr. Deming’s five steps and fourteen  points about Total Quality in Apple. Also, it shows how Apple serves as an example in today's technology industry  by focusing on quality first. Finally, this project discusses the legacy Steve n Jobs’ focus of placing custom er  experience and quality first.    Keywords  Total Quality Management, Operations and Supply Chain Management, Apple Inc.    Introduction  While Apple may not have adopted all of the practices outlined in Deming's five steps and his fourteen points, there  are many parallels in their approach t o business and product design. From the moment a customer enters the retail  realm of Apple they are immersed in  a “customer first” experience.  The products created and services provided by  Apple at their retail outlets are arguably the standard when it comes to the modern day technological industry.  The  continual improvement on their designs is born of a tireless effort to refine the prior generation and continued  improvement through cutting edge research and development. The research and design teams at Apple are committed  to providing the user with the best possible product experience. In our society and modern age, it is rare not to find an  Apple designed product within our offices,  class roo ms, and homes.  Apple has developed success through a  perspective focused on Total Quality Management (TQM), whether they realized it or not. The goal of this project is  to investigate the application of TQM in Apple’s management system. It focuses on highlighting the success reasons  in the management methodology adopted by this company.    Literature Review  This project investigates a successful implementation of TQM. By looking for literature about TQM implementation  in Apple Inc., the researchers did not find any academic publication supporting the direct implementation. However,  they found some online articles and forums that show Apple to be a good example of a successful organization that  applied strategies similar to those of TQM. Thus, this study focuses on investigating this point by comparing Apple’s  strategies with those of TQM.    Total Quality Management (TQM)  Quality was defined as a dynamic state associated with products, services, people, processes and environments that  meet or exceed current expectations (Goetsch and Davis 1995). Thus, TQ M is an approach to doing business that  attempts to maximize the competitiveness of an organization through continual improvement in the quality of its  products, services, people, processes and environments (Goetsch and Davis 1995). TQM has nine key elements shown ",gmail.com,,,46.3144754,11.0480288
105,,@uta.edu,"Food security, logistics, optimization, food bank, nutrition-focused dietary. ","Food banks are non -profit organizations with a mission to alleviate food insecurity and hunger, both of which are  serious and growing public health concerns in low -income communities. On a daily basis, food bank managers are  faced with the challenge of balancing demand and supply to  maximize the fulfillment of nutritional requirements,  while operating with limited resources . The objective of this research is to develop an optimization model that will   engage all the available food bank and agenc y resources in food collection and distribution .  This model maximizes  the collection of food considering the type of food based on a recommended dietary guideline, subject to transportation  and warehouse capacity constraints. Accurate estimation of client demand based on a standard guideline encourages  food bank managers to collect more nutritionally valuable food items, leading to greater organizational food  distribution efficiency and effectiveness. Food bank performance using this approach is compared wi th current food  bank logistics, in terms of  amount of distributed food . The results  of implementation of this approach  show a  significant reduction in amount of uncollected food which means more valuable foods are available for people in need,  and less foods are wasted in the system.    Keywords  Food security, logistics, optimization, food bank, nutrition-focused dietary.    Introduction  The growing number of individuals who do not have reliable access to food, known as food-insecure individuals, is a  mounting problem in the United States. According to the 2016 poverty statistics provided by Feeding America, a  hunger-relief organization, 40.6 million individuals in the U.S. are food insecure (Feeding America, 2017). In addition,  many people in the U.S., particularly low-income individuals, suffer from diet -related diseases such as obesity and  diabetes, which is mainly due to the consumption of low-quality food (Ogden, Carroll, Kit, & Flegal, 2012).  To address t he issue of hunger, Feeding America provides food -insecure individuals with food through a  network that consists of three main entities: (i) food donors (i.e., suppliers), including supermarkets, retailers, and  grocery manufacturers; (ii) food banks (i.e.,  central hubs), and (iii) food agencies (i.e., customers). To address the  issue of the diet-related disease (i.e., obesity and diabetes), Feeding America uses a nutrition guideline, called MyPlate,  to allocate food based on individuals’ daily nutritional r equirements in five categories : grains, vegetables, fruits,  proteins, and dairy (Campbell et al., 2015).  Several non-profit organizations such as Food banks work as an intermediary in food aid supply chain to  collect and distribute donated food to food-insecure individuals, ideally in an equitable manner  (Orgut et al., 2016) .  As humanitarian non -profit organizations, food banks aim to fulfill food -insecure individuals’ demand for food,  considering both food quantity and quality, and minimizing food waste , which is particularly challenging because  both demand and supply in food aid supply chains are unpredictable. Deman d varies based on changes in  unemployment and household income over time. Supply is also unpredictable because there is no accurate information  about quantities, qualities, and frequencies of donated materials prior to donation (Davis, Jiang, Morgan, Nuamah, &  Terry, 2016). However, since demand is generally larger than supply (Davis et al., 2014) , the real challenge lies in  logistics improvement in the food aid supply chain network (Martins, Melo, & Pato, 2016).  One of the typical challenges that food bank managers are commonly face d with is the collection and  distribution approach of donated food. The major problem with the current food banks’ collection -distribution  approach is that it is highly dependent on the food bank's limited transportation resources, resulting in sub -optimal  performance in terms of demand fulfilment, food waste, and logistics costs. Typically, Food banks receive, inspect,  and temporarily store food provided by food donors. Food agencies then are responsible for distributing food and  groceries to food-insecure individuals (Davis, Sengul, Ivy, Brock III, & Miles, 2014) . The problem with the current  collection-distribution approach is that it is incapable of addressing the issue of capacity and transportation limitations  of food agencies. That is, in most cases, food agencies are not able to collect food from food banks because either they ",uta.edu,University of Texas at Arlington,United States,32.728471299999995,-97.11202127009975
106,UNDERSTANDING ENGINEERING IDENTITY INUNDERGRADUATE STUDENTS,@montana.edu,"Engineering identity, engineering education. ","The process of becoming an engineer is fundamentally an identity development process and students who identify as  engineers are more likely both to graduate and to enter the field upon graduation. Therefore an opportunity in  engineering education is provid ing undergraduates experiences that bolster their sense of identity as engineers. In  particular, experiences that offer authentic engagement in engineering work should be expected to promote  engineering identity.   This paper tests the relationship between collegiate experiences expected to promote engineering identity  formation with change in engineering identity in a national sample of 918 engineering students using data from the  2013 College Senior Survey (CSS). The CSS is administered by the Higher Educa tion Research Institute (HERI) at  UCLA to college students at the end o f their fourth year of college; d ata from the CSS are then matched to students’  prior responses on the 2009 Freshman Survey (TFS) to create a longitudinal sample. Engineering identity is measured  using a composite of items available in both surveys to assess change in engineering identity over four years, and  intention to pursue an engineering career is also tested. Results show participation in undergraduate research appears  to increase engineering identity, while participation in an internship increases likelihood of pursuing an engineering  career.     Keywords  Engineering identity, engineering education.    Introduction  National reports have called for increasing the numbers of engineers g raduating from American colleges and  universities to fill anticipated job openings and help sustain the nation’s economically competitive position globally.  As a result, colleges and universities have worked to identify opportunities and interventions that  will improve their  institutions’ engineering degree productivity. These opportunities have included expanded access to undergraduate  research programs, internship opportunities, and increasing the support for engineering related student organizations  to f acilitate progress toward degree attainment. Further, colleges and universities have focused recruitment and  retention efforts on groups historically underrepresented in engineering as these groups tend to represent some of the  fastest growing demographic groups in the United States.   That said, improving degree productivity will only partially address this problem as only about half of seniors  in engineering are fairly certain of their plans to pursue engineering as a career (Hughes & Hurtado, 2013; Sheppard  et al., 2010) . In other words, while a  bachelor’s degree is generally required to enter the fiel d of engineering,  completion of a bachelor’s degree alone does not ensure entry into the field. Fundamentally, the engineering formation  process is an identity development process, and scholars have turned to engineering identity as a possible explanatory  factor motivating students’ decisions regarding persistence in their degree programs and ultimate entry into practice  (Meyers, Ohland, & Silliman, 2012; Pierrakos, Beam, Constantz, Johri, & Anderson, 2009) .   The purpose of this paper is to explore the relationship between undergraduate experiences and the  development of engineering identity. This paper uses a national, longitudinal dataset to test the effect of experiences  intended to promote persistence to degree completion and development of an engineering identity on change in  engineering identity over four years  of college. While previous research has pointed to indicators to help measure  engineering identity and experiences that promote engineering identity, that work has been limited in terms of  sampling procedures, instrumentation, and cross-sectional design. This work begins to address that gap. ",montana.edu,Montana State University - Bozeman,United States,45.6638859,-111.07928704602077
107,ROADWAY THICKNESS IN HIGHWAY INFRASTRUCTURE ASSETMANAGEMENT,@sdsmt.edu,"Linear Programming, Infrastructure Asset Management, Cost Allocation ","The most important policy study conducted by the Federal Highway Administration (FHWA) and state Department  of Transportation (DOT) agencies in the area of surface transportation infrastructure management is known as the  Highway Cost Allocation  (HCA) study. An HCA study is  performed by state and federal highway authorities to  evaluate whether road users classified as vehicle classes are paying their fair share of highway cost responsibility. The  objective of HCA study is to distribute a highway cost among all vehicle classe s on the basis of their consumption of  roadway thickness (pavement) and traffic capacity (lanes). An HCA method must be able to allocate cost by taking  into consideration all primary categories of cost being imposed by road users. We propose to consider th e Equivalent  Single Axle Loads (ESAL) used to determine pavement thickness and the number of lanes for capacity are considered  as two types of players for which the discrete A-S values can be calculated. These values in turn allow the calculation  of the costs allocated to all the vehicle classes using a facility with known ESALs. The lanes are divided among vehicle  classes using Shapley Value. The implementation of discrete A -S value allows delineation of highway cost into cost  of pavement thickness and cost for traffic lanes. The purpose of this paper is to demonstrate efficacy of a new method  that generates a unique allocation by combining concepts of discrete A -S value into a least core based linear  programming model. The final cost allocation will be sho wn to hold a strong fair -division property that only one  solution can satisfy.    Keywords  Linear Programming, Infrastructure Asset Management, Cost Allocation    Introduction  A Highway Cost Allocation (HCA) study is a critical policy instrument used by the Federal Highway Administration  (FHWA) and state department of transportation (DOT) agencies for fiscal management of national and state  transportation related assets. This paper seeks to place renewed emphasis on a cost measurement system designed for  HCA that utilizes discrete Aumann-Shapley value in an optimization context as propositioned by Kumar Dubey and  Garcia-Diaz (2017).  In addition, t his paper will shed light on novel use of Linear Programming (LP) based cost  allocation model that permits integration of traffic lanes and traffic loading conditions via two game-theoretic concepts  of Shapley value and discrete A-S value. An important feature of this approach is its ability to  divide cost among  vehicle classes by being reticent to  impact on roadway  in line with  pavement deterioration and demand for traffic  lanes. Additional advantages of this procedure include constraints of the LP model ensuring satisfaction of economic  properties that promote equity and fairness. The Right-Hand Sides (RHS) of the model are obtained using discrete A- S values wherein pavement cost occasioned is quantified by 18,000 lbs. Equivalent Single Axle Load (ESAL), a unit  of pavement damage as per  American Association of State and Highway Transportation Officials ( 1993) Pavement  Design Guide. Traffic lanes are ascertained using Transportation Research Board’s (2010) highway capacity manual.  The technical classification of a road user in HCA is a specific vehicle class defined as per taxonomical nomenclature  established in FHWA (2013) traffic monitoring guide.    Conceptual Approach  The study first considers a set A consisting of all vehicle classes using a multi-user highway facility. Further, this  paper will use greek symbols of 𝛼, 𝛽 and 𝛾 to represent three broad classification of vehicle types namely,  Light,  Medium and Heavy vehicle classes. The proposed procedure functions under the assumption that vehicle classes are   making a rational choice to share a highway facility for the purposes of lowering their cost responsibilities. Working  under this theory, the procedure mandates identification of all strategic highway facilities that the three vehicle classes ",sdsmt.edu,South Dakota School of Mines and Technology,United States,44.0731,-103.2063
108,HOW SYSTEM DYNAMICS CAN BE AN EFFECTIVE MODELINGMETHOD TO UNDERSTAND ENGINEERING TRANSFER STUDENTPERSISTENCE,@vt.edu,,"As we work to meet the demands of a more diverse engineering workforce, the transfer student population becomes  an important piece of that conversation due to their rich diversity that includes underrepresented minorities and first - generation students. Research indicates transfer students arrive with pre -existing characteristics that impact their  persistence rates at a four -year institution, which is generally less than that of their native peers. Researchers also  affirm the responsibility of institutions to provide the necessary framework and support to help this population be  successful. However, as institutions create programming and set forth policies, they are often doing so non-holistically,  which results in counter-intuitive outcomes that are further preventing the success of engineering transfer students. As  the transfer educational pathway continues to grow, further research is need ed to identify effective ways to increase  their persistence. The purpose of this paper is to discuss how system dynamics can be an effective tool to more fully  understand the process undergraduate engineering transfer students go through, while considering the uniqueness of  this population and the various needs they may have.     Introduction  For this research a transfer student is defined as a student that begins at one institution and leaves that institution to  attend as a full -time student at another insti tution in their pursuit of a bachelor’s degree. Transfer students may be  vertical transfers, who move from a two -year institution to a four -year institution, or they may be lateral transfers,  who move from one four-year institution to another four-year institution. Persistence is defined by Pascarella, Smart,  and Ethington (1986) as actively working toward a bachelor’s degree within a nine-year period.  The National Center for Education Statistics (NCES) and the National Student Clearinghouse Research  Center (NCSRC) have recently begun reporting on transfer students and through their reporting we can begin to gain  an understanding of how many transfer students exist in the current U.S. undergraduate student cohorts. In terms of  vertical transfer, students mov ing from a community college to a four -year institution, reports show approximately  one-third of first -time freshmen are community college attendees (Snyder, de Brey, & Dillow, 2016), and just over  30% of students attending a community college will transfe r to a four -year institution (Shapiro, Dundar, Huie,  Wakhungu, Yuan, Nathan, & Hwang, 2017). Additionally, more than one-third of students attending a four-year public  institution transferred to another institution, known as a lateral transfer (Snyder, de Brey, & Dillow, 2016). This data  is represented in Figure 1.1 and shows transfer students make up a significant share of student s attending a higher  education institution. Additionally, from 2014 to 2016, the United States (U.S.) experienced a 5% increase in transfer  student applications, further demonstrating the number of students following these pathways continues to increase  (Clinedinst and Koranteng, 2017).      ",vt.edu,Virginia Tech,United States,37.22192675,-80.42728184013652
109,NETWORKS,@uah.edu,"Information Loss, Operations Research Decision Effectiveness, Organizational Theory ","This paper demonstrates how to determine where information is lost in an organization’s communications network  by incorporating variables associated with management theory and cognitive psychology. A model was constructed  that emulated a decision-makers effort to gather information from colleagues within the company, discuss the  information with team co-workers, then process the information in order to make a final decision. The model  captured the information lost due to transference, groupthink, and cognitive limitations of the human brain or  overload. The factors that caused variation in loss amount are perceived importance of the data, groupthink  influence, and information quantity. Each of these factors was varied and the model was run through several  iterations. The model displayed a negative correlation between perceived importance and transference loss . It also  illustrated a positive correlation between groupthink influence and groupthink loss. The model reveals that there is a  positive correlation between both low and high levels of perceived importance and overload loss, but a negative  correlation between perceived importance and overload loss when importance levels are average. Finally, the model  discloses that all three loss types are positively correlated with information amount fluctuations.     Keywords  Information Loss, Operations Research Decision Effectiveness, Organizational Theory    Introduction  Communication networks exist among employees in every organization . Some networks are developed by the  employees themselves, while others are based on the structure of the company (Katz & Tushman , 1979 ). These  networks exist for a very important reason . That reason is to help individuals gather knowledge, consult with fellow  workers, and make decisions that benefit the organization as a whole (Grant , 1996). Even if there are well -defined,  easily accessed, open channels of communication available to each and every employe e, information can still be lost  along the way . This information loss can hinder the decision -making process (Martin , Morgan, Joseph, & Carley,  2010).  To eliminate information loss, one must first know where along the lines of communication the loss is  occurring and to what extent. The study presented in this paper is part of a larger research project that will determine  where network deficiencies exist that cause information loss . This study will also determine the nature of the  information that is lost as  it relates to the inherent factors that affect information . To capture loss and understand  how the intrinsic factors influence this loss, a model was constructed that mimics the communication network within  a hierarchical structured  organization. The mode l follows the point -of-view of a decision -maker (DM), as she  gathers information from work associates, examines the information with intra -team members, then process the  information in order to make a final decision . As the information traverses through th e communication network, the  model will extrapolate how much information is lost due to the telephone effect associated with transference (Breck   & Cardie, 2004 ), the persuasive effects of groupthink (Leenders , Engelen, & Kratzer, 2003 ), and the cognitive  limitations of the human brain that lead to overload (Davis, 2011).   Once the model has calculated loss, the natural factors that influence the undulations in information will be  studied. These factors include perceived importance (Keller & Staelin , 1997 ), groupthink (Larey & Paulus, 1999 )  (Leenders, Engelen, & Kratzer, 2003 ), and quantity  of information (Davis, 2011) (O’Reilly, 1980). If these factors  and their consequences are sufficiently understood, we believe that it will help pave the way to informatio n loss  eradication.     Background  Using information loss as a tool to predict decision effectiveness is a novel idea . However, its foundations have been  explored over the years in management theory and cognitive psychology (Keller & Staelin, 1987)(Marois & Ivanoff, ",uah.edu,University of Alabama at Huntsville,United States,34.7252,-86.6405
110,UNDERSTANDING HOW SOLDIER LOAD EFFECTS SOLDIERPERFORMANCE,@usma.edu,"Human performance, soldier load, constructed scales, load effects ","Today’s American soldier carries too much weight. Soldier load is a classic tradeoff problem, and the present research  seeks to build a framework to measure and understand this tradeoff. Through the development of constructed scales  and newly developed direct measures, it is possible to gain improved understanding of the  effect of load on soldier  performance. When considering the effect of soldier’s load, it is helpful to consider the soldier as a system, with  critical functions and requirements. The measurement of these functions, and the impact of soldier load on the efficacy  of these functions, is not well understood.  A methodology for measurement opportunities for soldier performance is  proposed. Results include a discussion of constructed scales of an existing obstacle course and insights from a weapon  employment experiment that explored soldier load impact.  The application of this work extends into a variety of  human performance domains.    Keywords  Human performance, soldier load, constructed scales, load effects    Introduction  The United States Army extensively researches, develops, and tests new soldier equipment.  Individual  Soldier equipment facilitates protection, sustainment, mobility, lethality, and communication.  However, unnecessary  or excessive amounts of individual equipment overburdens the  soldier, resulting in degraded performance.   Throughout the history of warfare, humans have attempted to achieve an optimal equipment configuration that would  lead to some type of advantage.  Today’s technological advances make it harder than ever to choo se the optimal  equipment configuration.  This research is an effort to devise measurement techniques to support the se equipping  decisions.       A large amount of ongoing soldier performance research focuses on speed, maneuverability, and operability  of equ ipment.  In an effort to better understand how soldier load affects soldier performance, DoD researchers  integrated the use of the Load Effects Assessment Program - Army (LEAP-A).  LEAP -A is an obstacle course that  requires the soldier to maneuver obstacles  that replicate the array of possibilities one may encounter during ground  combat.  The intent is for the course to provide  quantitative measures that predict soldier performance, but currently  the course only collects data on the time to complete each obstacle without considering whether or not the obstacle  was completed using approved tactical movements. For example, individual movement techniques and proper muzzle  awareness are examples of grading criteria omitted in LEAP -A. Additionally, there are recognized areas of soldier  performance in which LEAP -A fails to measure.  The primary contributions of the research presented in this paper  focus on two known gaps in soldier performance measurement.    The first gap involves the creation of generic soldier performance measurement methods.  Traditional  experimental methods seek to use one or more natural, quantitative scale s to discriminate amongst alternatives or  conditions.  The present paper promotes the use of a constructed scale, a technique that uses careful language  and  criteria to translate an inherently qualitative phenomena into a quantitative measure.   The second gap addresses the  lack of lethality assessment with ongoing load -based research.  Soldier equipment load undoubtedly effects the  employment of weapons, however there are minimal studies or techniques that address this gap.             ",usma.edu,United States Military Academy,United States,41.3927227,-73.95986305044411
111,A COCHRANE METHOD SYSTEMATIC REVIEW OFUNIVERSITY TECH COMMERCIALIZATION RESEARCH,@winthrop.edu,"technology management, technology transfer, technology commercialization, systematic review, meta-analysis, ","Since 1980 universities have been able to commercialize inventions that their faculty researchers create as per the  1980 Bayh-Dole Act (P.L. 96 -517).  Research universities can now own and license these inventions to small and  well established companies.  Since 19 80, research universities have used tech commercialization to support their  regional economies with product development and sales, and academic entrepreneurship resulting in university spin - offs and start -up business formations.  This results in job creation.  The technology transfer offices (TTOs) which  were established at many research universities to manage this process have been studied quite extensively.  However,  the foundational elements that fuel successful TTO performance has not been studied compr ehensively. Instead,  there are numerous fragmented studies that date back to the early 1980s.  In addition, there is no agreed upon  common theory for studying university technology and how these elemental inputs related to performance outputs.   Thus, herein it is advocated that the resource-based view (J. Barney, 1991) and theory on environmental munificence  (Castrogiovanni, 1991, 2002) be used as a theoretical framework for researching university technology  commercialization.  Competitive resources in a more munificent  environment can make it easier for an organization  to survive and prosper.    Keywords  technology management, technology transfer, technology commercialization, systematic review, meta-analysis,  management of innovation, management of new technologies, project and innovation management, R&D  management, management of scientists and engineers    Introduction  Stanford cell biologist Lubert Stryer and U.C. Berkeley microbiologist Alexander Glazer researched the use of  phycobiliproteins found in marine algae as flu orescent markers.  Six (6) months later, Stanford licensed the  invention to two (2) companies which is now an important tool for cancer detection and blood screening.  And this  is only one among many such success stories.  In fact, for the past 30 years, universities have been in the business of  selling research results by licensing technological innovations to well established corporations, small businesses,  and/or to university start-up companies that they spin-off (Wiesendanger, 2000).  Since the Morill Act of 1862, established land grant universities have acted as research hubs.  In 1980,  Congress passed the Bayh -Dole Act ( ""Bayh Dole Act,"" 1980)  which allowed universities to obtain ownership title  to inventions created with government funded research an d established technology transfer offices (TTOs) to  manage the process of patenting and licensing these inventions .  Technology transfer is a subset of technology  management (C. Hamilton, 2017a , 2017b).  Because of the role of TTO s in enabling and commercializing  innovations, a growing number of researchers have examined for example: whether university policies and  structures have any effect on academic entrepreneurship (Seashore Louis, 1989) ; the features of universities that  generate the most spin -offs (A. W. Lockett, Mike 2005 ); factors that enhance university tech transfer ( Friedman,  2003); if internal and external factors explain the efficiency of university tech transfer (Siegel, 2003); the level of  efficiency that university TTOs in the UK exhibit (Chapple, 2005) ; the difference between for profit versus  traditional nonprofit TTOs, technology licensing for equity strategies and sponsored research licensing strategies  (G. D. Markman, Phan, Phillip H., Balkin, David B., Gianiodis, Peter T., 2005 ); the best incubation models for  academic spin-offs (Clarysse, 2007); the most efficient TTOs (Curi, 2012) ; what technology transfer specialists pay  attention to (C. Hamilton, 2015; C. S. Hamilton, David, 2016); and which TTOs are more likely to get better results  (González-Pernía, 2013). ",winthrop.edu,Winthrop University,United States,34.939120349999996,-81.03249109890248
112,A COMPARATIVE STUDY OF DUBAI GOVERNMENT EXCELLENCEPROGRAM’S AWARD MODELS: THE EFQM AND 4TH. GENERATIONOF EXCELLENCE SYSTEM BASED MODEL,@sharjah.ac.ae,"Excellence Awards, Excellence Models, 4 th Generation of Excellence, Government Excellence, Dubai Government, ","Business excellence models have been widely used by national excellence awards. More than a decade ago, the Dubai  Government Excellence Program (DGEP) adopted the European Fou ndation for Quality Management (EFQM)  business excellence model. Leadership -supported adoption of the model, together with the award program and  initiatives, have made large  contributions to the excellence movement in the Dubai Government Sector . In 2016,  DGEP adopted the 4th Generation of the Excellence System (4GES) model, which was launched at the Federal level  one year earlier. This study aims to gain insight into the two models, comparing them based on several factors related  to their core values and objectives, model structure criteria and weights, and assessment methodology and tools.  Results of the comparison are discussed, study conclusions are stated, and recommendations were focused on how to  improve the implementation of the 4GES model to achieve its objectives.    Keywords  Excellence Awards, Excellence Models, 4 th Generation of Excellence, Government Excellence, Dubai Government,  EFQM Model.    Introduction  Quality and excellence awards are widely used by national governments and regional and international institutions to  spread a culture of quality and encourage organizations to achieve sustainable excellent performance in various  sectors. Dubai Government Excellence Program (DGEP) is a pioneering program established by His Highness Sheikh  Mohammed Bin Rashid Al Maktoum , Vice President and Prime Minister of the United Arab Emirates and Ruler of  Dubai in 1997. Since 2002, DGEP has adopted the European Foundation for Quality Manageme nt (EFQM) business  excellence model and adapted that model with necessary permissions to suit the updated requirements and achieve the  objectives of Dubai Government. In April 2016, DGEP adopted the 4th. Generation of Government Excellence System  (4GES), which was earlier launched by The Prime Minister of the United Arab Emirates (UAE) Office in 2015 and  adopted at the UAE Federal Level.  This study is one of the first studies to explore this newly launched, and recently adopted 4GES model. The  objective of this study is to compare this 4GES model with the previously adopted, and widely used EFQM business  excellence model as adopted by the DGEP award . In this comparison, the similarities and differences as well as the  advantages and disadvanta ges from multiple stakeholders’ perspectives shall be identified , furthermore,  recommendations on how to improve the effective achievement of DGEP award objectives are concluded. This study  is important to Dubai government entities, and any other organization currently adopting the EFQM excellence model  and wish to explore the possibility of adopting the 4GES model, since it provides those organizations with a cross  reference between the frameworks, concepts, criteria and assessment methodologies for both models. Thereby, helping  those organizations in developing their roadmap for migrating to the new 4GES model, identifying ways to overcome  challenges associated with the effective implementation of the 4GES model. ",sharjah.ac.ae,University of Sharjah,United Arab Emirates,25.26519105,55.91143243267334
