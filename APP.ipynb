{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "OUpTMlzJ8Kwy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg1wBA2-jyoc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "import math\n",
        "import networkx as nx\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "import requests\n",
        "import warnings\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to get the data"
      ],
      "metadata": {
        "id": "o4VP8sNy8M8P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKN4yIWjPxJ4"
      },
      "outputs": [],
      "source": [
        "def get_df(file_year):\n",
        "  base_url = \"https://raw.githubusercontent.com/edavgaun/ASEM-Analysis-App/refs/heads/main/Data/CP_{}.csv\"\n",
        "  file_name=base_url.format(file_year)\n",
        "  df = pd.read_csv(file_name, index_col=\"Unnamed: 0\")\n",
        "  if file_year!=2015:\n",
        "    df[\"Paper\"] = df[\"Title\"].str.lower()+ \", \" \\\n",
        "                   + df[\"KeyWords\"].str.lower() + \", \" \\\n",
        "                   + df[\"Abstract\"].str.lower()+\", \"\n",
        "  else:\n",
        "    df[\"Paper\"] = df[\"Title\"].str.lower()+ \", \" \\\n",
        "                + df[\"Abstract\"].str.lower()+\", \"\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBUbrn34qUig"
      },
      "outputs": [],
      "source": [
        "def get_corpus(df, year):\n",
        "  corpus=\", \".join([t.lower() if type(t)!=float else \"\" for t in df.Title.values])\n",
        "  if year!=2015:\n",
        "    corpus+=\", \".join([t.lower() if type(t)!=float else \"\" for t in df.KeyWords.values])\n",
        "  corpus+=\", \".join([t.lower() if type(t)!=float else \"\" for t in df.Abstract.values])\n",
        "  corpus=\", \".join([c.replace(\" \", \"-\").replace(\";\", \",\").replace(\".\", \",\").replace(\"-,\", \"\") for c in corpus.split(\", \")])\n",
        "  corpus=corpus.replace(\"4,0\", \"4.0\").replace(\"5,0\", \"5.0\")\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNapjaNC3ygh"
      },
      "outputs": [],
      "source": [
        "def get_tokens(corpus, nlp=nlp):\n",
        "  concepts=[t.replace(\"--\", \"-\").replace(\"-\", \" \") for t in set(corpus.split(\", \")) if len(t)>=2]\n",
        "  doc = nlp(\", \".join(concepts))\n",
        "  return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dioHYLaUqUgU"
      },
      "outputs": [],
      "source": [
        "def get_bow(tokens):\n",
        "  bow_kw={}\n",
        "  for token in tokens:\n",
        "    if (not token.is_stop) and (not token.is_punct) and (not token.is_digit) \\\n",
        "        and (len(token)>=2):\n",
        "      try:\n",
        "        bow_kw[token.lemma_]+=1\n",
        "      except:\n",
        "        bow_kw[token.lemma_]=1\n",
        "  return bow_kw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvDyMtFlg7rd"
      },
      "outputs": [],
      "source": [
        "def get_bow_df(bow):\n",
        "  df_kw=pd.DataFrame({\"Word\":bow.keys(), \"frq\":bow.values()}).sort_values(\"frq\", ascending=False\n",
        "                                                                                ).reset_index(drop=True)\n",
        "  return df_kw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSztX5wwYHzl"
      },
      "source": [
        "$$\n",
        "\\large\n",
        "C\\binom{n}{r} = \\frac{n!}{r!(n-r)!}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mZgAhw65SzF"
      },
      "outputs": [],
      "source": [
        "def get_dict():\n",
        "  url=\"https://raw.githubusercontent.com/edavgaun/ASEM-Analysis-App/main/Data/own_stopwords.txt\"\n",
        "  response = requests.get(url)\n",
        "  content = response.text\n",
        "  content = content.strip().replace(\"\\n\", \", \").split(\", \")\n",
        "  content.sort()\n",
        "  return content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpSBV_yKugTr"
      },
      "outputs": [],
      "source": [
        "def get_topN_word_bow_df(num_word, bow_df, own_stopwords=own_stopwords):\n",
        "  return bow_df[~bow_df.Word.isin(own_stopwords)].head(num_word).Word.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iveOVl6s-yJ"
      },
      "outputs": [],
      "source": [
        "def get_word_frq(bow_df, Top_KW):\n",
        "  return bow_df[bow_df.Word.isin(Top_KW)].set_index(\"Word\").to_dict()[\"frq\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "903KSkyv3E6m"
      },
      "outputs": [],
      "source": [
        "# Get top N words\n",
        "def get_combinations(df, bow_df, KW_values):\n",
        "  # Initialize co-occurrence counter\n",
        "  pair_counter = Counter()\n",
        "\n",
        "  # Process each paper efficiently\n",
        "  for paper in df[\"Paper\"].dropna():  # Remove NaNs\n",
        "      words_in_paper = set(paper.split())  # Tokenize paper into a set of words\n",
        "      for w1, w2 in combinations(KW_values, 2):  # Generate word pairs dynamically\n",
        "          if w1 in words_in_paper and w2 in words_in_paper:\n",
        "              pair_counter[(w1, w2)] += 1\n",
        "\n",
        "  # Convert the Counter dictionary to a DataFrame\n",
        "  df_comb = pd.DataFrame(pair_counter.items(), columns=[\"Word_Pair\", \"Count\"])\n",
        "  df_comb[[\"Word1\", \"Word2\"]] = pd.DataFrame(df_comb[\"Word_Pair\"].tolist(), index=df_comb.index)\n",
        "  df_comb.drop(columns=[\"Word_Pair\"], inplace=True)\n",
        "\n",
        "  return df_comb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "G55vx4GuCHxg"
      },
      "outputs": [],
      "source": [
        "start=2015\n",
        "dfs, corpuses, tokenses, bows, bow_dfs = {}, {}, {}, {}, {}\n",
        "for n in range(10):\n",
        "  try:\n",
        "    dfs[start+n]=get_df(start+n)\n",
        "    corpuses[start+n]=get_corpus(dfs[start+n], start+n)\n",
        "    tokenses[start+n]=get_tokens(corpuses[start+n])\n",
        "    bows[start+n]=get_bow(tokenses[start+n])\n",
        "    bow_dfs[start+n]=get_bow_df(bows[start+n])\n",
        "  except:\n",
        "    continue"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Graph"
      ],
      "metadata": {
        "id": "A23LU_xi8INS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXrsmzy0qXjp"
      },
      "outputs": [],
      "source": [
        "def draw_Network(data_year, num_word=10, random_loc=0):\n",
        "  df=dfs[data_year]\n",
        "  corpus=corpuses[data_year]\n",
        "  tokens=tokenses[data_year]\n",
        "  bow=bows[data_year]\n",
        "  bow_df=bow_dfs[data_year]\n",
        "  KW=get_topN_word_bow_df(num_word, bow_df)\n",
        "  word_frequencies = get_word_frq(bow_df, KW)\n",
        "  df_comb = get_combinations(df, bow_df, KW)\n",
        "\n",
        "  # Efficient word co-occurrence counting using Counter\n",
        "  pair_counter = Counter()\n",
        "\n",
        "  # Create the network graph\n",
        "  G = nx.Graph()\n",
        "\n",
        "  # Add edges (word pairs) with weight as count\n",
        "  for _, row in df_comb.iterrows():\n",
        "      if row[\"Count\"] > 0:\n",
        "          G.add_edge(row[\"Word1\"], row[\"Word2\"], weight=row[\"Count\"] / num_word)\n",
        "\n",
        "  # Compute node degrees\n",
        "  node_degrees = dict(G.degree())\n",
        "  nx.set_node_attributes(G, node_degrees, \"degree\")\n",
        "\n",
        "  # Normalize node degrees for color mapping\n",
        "  max_degree = max(node_degrees.values()) if node_degrees else 1\n",
        "  node_colors = [node_degrees[node] / max_degree for node in G.nodes]\n",
        "\n",
        "  # Choose a colormap\n",
        "  cmap = cm.plasma\n",
        "  norm = mcolors.Normalize(vmin=0, vmax=50)\n",
        "\n",
        "  # Generate positions using spring layout\n",
        "  pos = nx.spring_layout(G, seed=int(random_loc), k=0.7)\n",
        "\n",
        "  # Scale node sizes based on frequency\n",
        "  node_sizes = [word_frequencies.get(word, 1) * 5 for word in G.nodes()]  # Default size if missing\n",
        "\n",
        "  # Create figure\n",
        "  fig, ax = plt.subplots(figsize=(12, 9))\n",
        "\n",
        "  # Draw edges\n",
        "  edge_weights = [data[\"weight\"] * 7 for _, _, data in G.edges(data=True)]\n",
        "  nx.draw_networkx_edges(G, pos, alpha=0.6, width=edge_weights, edge_color=\"gray\")\n",
        "\n",
        "  # Draw nodes with color mapping (no 'norm' in draw_networkx_nodes)\n",
        "  nodes = nx.draw_networkx_nodes(\n",
        "      G, pos, node_size=node_sizes, node_color=node_colors, cmap=cmap, edgecolors=\"black\", alpha=0.9\n",
        "  )\n",
        "\n",
        "  # Draw labels\n",
        "  nx.draw_networkx_labels(G, pos, font_size=10, font_weight=\"bold\", verticalalignment=\"top\")\n",
        "\n",
        "  # Add colorbar\n",
        "  sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "  sm.set_array([])  # Empty array for colorbar to work\n",
        "  cbar = plt.colorbar(sm, ax=ax, fraction=0.02, pad=0.04,)\n",
        "  cbar.set_label(\"Degree Centrality\", fontsize=12)\n",
        "\n",
        "  # Final adjustments\n",
        "  plt.title(\"Word Co-occurrence Network, {} (Semantic Clustering)\".format(data_year), fontsize=14)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2nY-YK4vl3r"
      },
      "outputs": [],
      "source": [
        "interact(draw_Network, data_year=(2015,2025,1),\n",
        "                       num_word=(3,50,1),\n",
        "                       random_loc=\"0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Cloud"
      ],
      "metadata": {
        "id": "uEjqS-b08E1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuTGs_U41zM9"
      },
      "outputs": [],
      "source": [
        "def draw_word_cloud(data_year, num_word=10):\n",
        "  import imageio.v2 as imageio\n",
        "  from io import BytesIO\n",
        "\n",
        "  logo_url = \"https://raw.githubusercontent.com/edavgaun/ASEM-Analysis-App/main/assets/asem_logo.png\"\n",
        "  df=get_df(data_year)\n",
        "  corpus=get_corpus(df, data_year)\n",
        "  tokens=get_tokens(corpus)\n",
        "  bow=get_bow(tokens)\n",
        "  bow_df=get_bow_df(bow)\n",
        "  own_stopwords=get_dict()\n",
        "  word_freq = dict(zip(bow_df[bow_df.Word.isin(own_stopwords)]['Word'],\n",
        "                     bow_df[bow_df.Word.isin(own_stopwords)]['frq']))\n",
        "\n",
        "  response = requests.get(logo_url)\n",
        "  mask = imageio.imread(BytesIO(response.content))\n",
        "  mask = np.where(mask > 128, 255, 0)  # Apply a threshold to get a binary mask\n",
        "\n",
        "  wordcloud = WordCloud(width=1000, height=700, mask=mask,\n",
        "                        background_color='white',contour_width=0.5, contour_color='Blue'\n",
        "                        ).generate_from_frequencies(word_freq)\n",
        "\n",
        "  # Plot the word cloud\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis('off')  # Turn off axis labels\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Radar Chart"
      ],
      "metadata": {
        "id": "xw88KKGMAKmk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hHDlzKUPN66"
      },
      "outputs": [],
      "source": [
        "def radar_chart(year, word, topN_Words, ax, color):\n",
        "  word=word.lower()\n",
        "  df=dfs[year]\n",
        "  bow_df=bow_dfs[year]\n",
        "  KW=get_topN_word_bow_df(topN_Words, bow_df)\n",
        "  word_frequencies = get_word_frq(bow_df, KW)\n",
        "  df_comb=get_combinations(df, bow_df, KW)\n",
        "  rank=bow_df[bow_df[\"Word\"]==word].index.values[0]\n",
        "  df_comb_word=df_comb[(df_comb.Word1==word) | (df_comb.Word2==word)]\n",
        "  arr=df_comb_word.iloc[:,1:].values\n",
        "  df_comb_word.loc[:, \"label\"]=arr[arr != word]\n",
        "  df_comb_word=df_comb_word.sort_values(\"label\").reset_index()\n",
        "  labels = df_comb_word.label.values.tolist()\n",
        "  values = df_comb_word.Count.values\n",
        "  norm = np.linalg.norm(values)\n",
        "  norm_values=list(values/norm)\n",
        "\n",
        "  # Convert to radians for the radar chart\n",
        "  num_vars = len(labels)\n",
        "  angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
        "\n",
        "  # Close the radar chart (connect last point to first)\n",
        "  norm_values += norm_values[:1]\n",
        "  angles += angles[:1]\n",
        "\n",
        "  # Plot the data\n",
        "  ax.fill(angles, norm_values, color=color, alpha=0.25)  # Fill area\n",
        "  ax.plot(angles, norm_values, color=color, linewidth=2)  # Line plot\n",
        "\n",
        "  # Add category labels\n",
        "  ax.set_xticks(angles[:-1])\n",
        "  ax.set_xticklabels(labels, fontsize=10)\n",
        "  ax.set_yticks([0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "  ax.set_yticklabels([0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "\n",
        "  # Display the chart\n",
        "  ax.text(0,0,\"{}\\n{}\\nrank{}\".format(word.upper(), year,rank+1), ha='center', va=\"center\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aIDDJy-42SG"
      },
      "outputs": [],
      "source": [
        "def compare_radar(word, topN_Words, year1, year2):\n",
        "\n",
        "  fig, ax = plt.subplots(1,2, figsize=(14, 6), subplot_kw=dict(polar=True))\n",
        "  try:\n",
        "    radar_chart(year1, word, topN_Words, ax[0], \"red\")\n",
        "  except:\n",
        "    ax[0].text(0,0,\"Missing data\\nfor this topic\\nthis year\", ha=\"center\", va=\"center\")\n",
        "  try:\n",
        "    radar_chart(year2, word, topN_Words, ax[1], \"blue\")\n",
        "  except:\n",
        "    ax[1].text(0,0,\"Missing data\\nfor this topic\\nthis year\", ha=\"center\", va=\"center\")\n",
        "  plt.suptitle('Relationship between \"{}\" and Other Topics'.format(\n",
        "                                                        word.title() ),\n",
        "                                                        fontsize=20,\n",
        "                                                        y=1.025)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvc3HWqu5F9p"
      },
      "outputs": [],
      "source": [
        "interact(compare_radar, word=\"\", topN_Words=(50,100,10), year1=[2015+n for n in range(10)],\n",
        "                                                         year2=[2015+n for n in range(10)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting wide and long data frames"
      ],
      "metadata": {
        "id": "KrYWt7ofCpYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wide_df():\n",
        "  url = \"https://raw.githubusercontent.com/edavgaun/ASEM-Analysis-App/refs/heads/main/Data/data_full.csv\"\n",
        "  df = pd.read_csv(url)\n",
        "  return df"
      ],
      "metadata": {
        "id": "G6JJz-LExk8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_long_df():\n",
        "  url = \"https://raw.githubusercontent.com/edavgaun/ASEM-Analysis-App/refs/heads/main/Data/df_long.csv\"\n",
        "  df = pd.read_csv(url, index_col=\"Unnamed: 0\")\n",
        "  return df"
      ],
      "metadata": {
        "id": "h8z12YRHDUNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bubble chart"
      ],
      "metadata": {
        "id": "9w1FoYXGD1Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Bubble_chart(*top_words):\n",
        "  df_long=get_long_df()\n",
        "  data_full=get_wide_df()\n",
        "  try:\n",
        "    top_words = list(top_words)\n",
        "    top_words.sort()\n",
        "\n",
        "    df_plot = df_long[df_long.Word.isin(top_words)]\n",
        "\n",
        "    fig, axs = plt.subplots(figsize=(10, max(2, len(top_words) * 0.9)))\n",
        "\n",
        "    sns.scatterplot(\n",
        "        data=df_plot,\n",
        "        x='year', y='Word', size='frq', hue='frq',\n",
        "        palette='Blues', sizes=(100, 3000),\n",
        "        edgecolor='k', ax=axs,\n",
        "        legend=False\n",
        "    )\n",
        "\n",
        "    # Add bubble value annotations\n",
        "    for index, row in df_plot.iterrows():\n",
        "        txt_color = \"black\"\n",
        "        txt_weight = None\n",
        "        if row['frq'] > int(df_plot.frq.max()*3/5):\n",
        "            txt_color = \"white\"\n",
        "            txt_weight = \"bold\"\n",
        "\n",
        "        axs.text(row['year'], row['Word'], str(row['frq']),\n",
        "                 ha='center', va='center', fontsize=8, alpha=0.7,\n",
        "                 color=txt_color, fontweight=txt_weight)\n",
        "\n",
        "    axs.set_ylim(-0.6, len(top_words) - 0.4)\n",
        "    axs.grid(axis='both', linestyle='--', alpha=0.4)\n",
        "    axs.spines['top'].set_visible(False)\n",
        "    axs.spines['right'].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "MWxb6UxqNtoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WS():\n",
        "  data_full=get_wide_df()\n",
        "  word_selector = widgets.SelectMultiple(\n",
        "      options=data_full.Word,\n",
        "      description='Words to choose:',\n",
        "      rows=5,  # number of rows shown\n",
        "      style={'description_width': 'initial'},\n",
        "      layout=widgets.Layout(width='50%')\n",
        "  )\n",
        "  return word_selector"
      ],
      "metadata": {
        "id": "Y9kHg1uRY8IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@interact(top_words=WS())\n",
        "def update_bubble_chart(top_words):\n",
        "    Bubble_chart(*top_words)"
      ],
      "metadata": {
        "id": "TS_AvYTmblEo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scatterplot"
      ],
      "metadata": {
        "id": "enq-m-z_Eygk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scatterplot3D():\n",
        "  data_full=get_wide_df().set_index(\"Word\")\n",
        "  df_long=get_long_df()\n",
        "  X = data_full.values\n",
        "  X_embedded = TSNE(n_components=3, learning_rate='auto',\n",
        "                    init='random', perplexity=3).fit_transform(X)\n",
        "  df = px.data.iris()\n",
        "  fig = px.scatter_3d(x=X_embedded[:,0], y=X_embedded[:,1], z=X_embedded[:,2],\n",
        "                color=data_full.Cluster, size=df_long.groupby(\"Word\")[\"frq\"].sum())\n",
        "  fig.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8Tn6EgPuiNFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dvXuJV6pFZna"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}